/glob/intel-python/python3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
/glob/intel-python/python3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
/glob/intel-python/python3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.
  DeprecationWarning)
Setting up problem parameters... DONE
Gathering data... Numbmer of datapoints: 5777
Building graph support... DONE
Running Neural Networks: BEGINNING
 
Training model: c_cheb_a
 
  architecture/L = 2
  architecture/N = [32, 16, 8]
CNNGS Architecture: c_cheb_a (clustering)
  input: M_0 = N = 32
  l_1: gsconv_1
    input dimension : M_0 = F_0 N_0 =  1 * 32 = 32
    output dimension: M_1 = F_1 N_1 = 14 * 16 = 224
    parameters: K_1 F_1 F_0 = 7 * 14 * 1 = 98
  l_2: gsconv_2
    input dimension : M_1 = F_1 N_1 = 14 * 16 = 224
    output dimension: M_2 = F_2 N_2 = 28 *  8 = 224
    parameters: K_2 F_2 F_1 = 14 * 28 * 14 = 5488
  l_3: softmax
    input dimension : M_2 = 224
    output dimension: M_3 = 2
    parameters: M_3 M_2 = 2 * 224 = 448
  Total parameters = 6034
 
2018-06-29 00:28:02.343404: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
[c_cheb_a] step 1000 / 51990 (epoch 19.23 / 1000):
  learning_rate = 9.81e-04, loss_average = 6.32e+03
/glob/intel-python/python3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 2.07e+04
  time: 203s (wall 27s)
[c_cheb_a] step 2000 / 51990 (epoch 38.47 / 1000):
  learning_rate = 9.63e-04, loss_average = 9.93e+02
  validation accuracy: 94.98 (549 / 578), f1 (binary): 38.30, loss: 1.91e+04
  time: 407s (wall 54s)
[c_cheb_a] step 3000 / 51990 (epoch 57.70 / 1000):
  learning_rate = 9.45e-04, loss_average = 1.17e+03
  validation accuracy: 96.54 (558 / 578), f1 (binary): 50.00, loss: 1.05e+04
  time: 611s (wall 80s)
[c_cheb_a] step 4000 / 51990 (epoch 76.94 / 1000):
  learning_rate = 9.27e-04, loss_average = 6.37e+02
  validation accuracy: 96.02 (555 / 578), f1 (binary): 61.02, loss: 1.16e+04
  time: 815s (wall 107s)
[c_cheb_a] step 5000 / 51990 (epoch 96.17 / 1000):
  learning_rate = 9.08e-04, loss_average = 2.17e+02
  validation accuracy: 96.02 (555 / 578), f1 (binary): 46.51, loss: 1.31e+04
  time: 1017s (wall 134s)
[c_cheb_a] step 6000 / 51990 (epoch 115.41 / 1000):
  learning_rate = 8.91e-04, loss_average = 2.39e+02
  validation accuracy: 90.48 (523 / 578), f1 (binary): 38.20, loss: 1.18e+04
  time: 1220s (wall 160s)
[c_cheb_a] step 7000 / 51990 (epoch 134.64 / 1000):
  learning_rate = 8.75e-04, loss_average = 4.81e+02
  validation accuracy: 96.19 (556 / 578), f1 (binary): 0.00, loss: 1.06e+04
  time: 1423s (wall 187s)
[c_cheb_a] step 8000 / 51990 (epoch 153.88 / 1000):
  learning_rate = 8.58e-04, loss_average = 1.09e+02
  validation accuracy: 97.23 (562 / 578), f1 (binary): 61.90, loss: 8.15e+03
  time: 1626s (wall 213s)
[c_cheb_a] step 9000 / 51990 (epoch 173.11 / 1000):
  learning_rate = 8.41e-04, loss_average = 1.28e+02
  validation accuracy: 97.06 (561 / 578), f1 (binary): 32.00, loss: 7.53e+03
  time: 1830s (wall 240s)
[c_cheb_a] step 10000 / 51990 (epoch 192.34 / 1000):
  learning_rate = 8.25e-04, loss_average = 2.14e+02
  validation accuracy: 96.54 (558 / 578), f1 (binary): 9.09, loss: 4.74e+03
  time: 2033s (wall 267s)
[c_cheb_a] step 11000 / 51990 (epoch 211.58 / 1000):
  learning_rate = 8.10e-04, loss_average = 4.85e+01
  validation accuracy: 97.06 (561 / 578), f1 (binary): 48.48, loss: 5.46e+03
  time: 2236s (wall 294s)
[c_cheb_a] step 12000 / 51990 (epoch 230.81 / 1000):
  learning_rate = 7.94e-04, loss_average = 2.52e+01
  validation accuracy: 96.54 (558 / 578), f1 (binary): 58.33, loss: 4.71e+03
  time: 2440s (wall 320s)
[c_cheb_a] step 13000 / 51990 (epoch 250.05 / 1000):
  learning_rate = 7.79e-04, loss_average = 2.30e+01
  validation accuracy: 97.06 (561 / 578), f1 (binary): 54.05, loss: 6.43e+03
  time: 2644s (wall 347s)
[c_cheb_a] step 14000 / 51990 (epoch 269.28 / 1000):
  learning_rate = 7.64e-04, loss_average = 1.57e+02
  validation accuracy: 94.64 (547 / 578), f1 (binary): 11.43, loss: 1.36e+04
  time: 2848s (wall 374s)
[c_cheb_a] step 15000 / 51990 (epoch 288.52 / 1000):
  learning_rate = 7.50e-04, loss_average = 1.17e+01
  validation accuracy: 96.71 (559 / 578), f1 (binary): 55.81, loss: 9.40e+03
  time: 3050s (wall 400s)
[c_cheb_a] step 16000 / 51990 (epoch 307.75 / 1000):
  learning_rate = 7.36e-04, loss_average = 1.41e+01
  validation accuracy: 97.06 (561 / 578), f1 (binary): 45.16, loss: 6.08e+03
  time: 3253s (wall 427s)
[c_cheb_a] step 17000 / 51990 (epoch 326.99 / 1000):
  learning_rate = 7.22e-04, loss_average = 1.18e+01
  validation accuracy: 96.89 (560 / 578), f1 (binary): 59.09, loss: 2.14e+04
  time: 3457s (wall 453s)
[c_cheb_a] step 18000 / 51990 (epoch 346.22 / 1000):
  learning_rate = 7.07e-04, loss_average = 1.92e+01
  validation accuracy: 93.60 (541 / 578), f1 (binary): 41.27, loss: 1.88e+04
  time: 3661s (wall 480s)
[c_cheb_a] step 19000 / 51990 (epoch 365.45 / 1000):
  learning_rate = 6.94e-04, loss_average = 5.45e+00
  validation accuracy: 97.23 (562 / 578), f1 (binary): 60.00, loss: 1.62e+04
  time: 3865s (wall 507s)
[c_cheb_a] step 20000 / 51990 (epoch 384.69 / 1000):
  learning_rate = 6.81e-04, loss_average = 6.79e+00
  validation accuracy: 97.23 (562 / 578), f1 (binary): 46.67, loss: 1.57e+04
  time: 4069s (wall 533s)
[c_cheb_a] step 21000 / 51990 (epoch 403.92 / 1000):
  learning_rate = 6.68e-04, loss_average = 3.78e+00
  validation accuracy: 96.37 (557 / 578), f1 (binary): 55.32, loss: 1.41e+04
  time: 4273s (wall 560s)
[c_cheb_a] step 22000 / 51990 (epoch 423.16 / 1000):
  learning_rate = 6.55e-04, loss_average = 3.25e+00
  validation accuracy: 97.06 (561 / 578), f1 (binary): 58.54, loss: 1.29e+04
  time: 4477s (wall 587s)
[c_cheb_a] step 23000 / 51990 (epoch 442.39 / 1000):
  learning_rate = 6.43e-04, loss_average = 2.37e+00
  validation accuracy: 97.06 (561 / 578), f1 (binary): 48.48, loss: 7.53e+03
  time: 4681s (wall 614s)
[c_cheb_a] step 24000 / 51990 (epoch 461.63 / 1000):
  learning_rate = 6.31e-04, loss_average = 3.15e+00
  validation accuracy: 97.23 (562 / 578), f1 (binary): 50.00, loss: 6.57e+03
  time: 4885s (wall 640s)
[c_cheb_a] step 25000 / 51990 (epoch 480.86 / 1000):
  learning_rate = 6.19e-04, loss_average = 4.16e+00
  validation accuracy: 96.54 (558 / 578), f1 (binary): 16.67, loss: 1.82e+03
  time: 5089s (wall 667s)
[c_cheb_a] step 26000 / 51990 (epoch 500.10 / 1000):
  learning_rate = 6.06e-04, loss_average = 1.73e+00
  validation accuracy: 96.54 (558 / 578), f1 (binary): 41.18, loss: 1.26e+03
  time: 5293s (wall 694s)
[c_cheb_a] step 27000 / 51990 (epoch 519.33 / 1000):
  learning_rate = 5.95e-04, loss_average = 1.30e+00
  validation accuracy: 96.54 (558 / 578), f1 (binary): 50.00, loss: 9.33e+02
  time: 5497s (wall 720s)
[c_cheb_a] step 28000 / 51990 (epoch 538.57 / 1000):
  learning_rate = 5.84e-04, loss_average = 7.32e+01
  validation accuracy: 77.68 (449 / 578), f1 (binary): 23.67, loss: 2.16e+03
  time: 5701s (wall 747s)
[c_cheb_a] step 29000 / 51990 (epoch 557.80 / 1000):
  learning_rate = 5.73e-04, loss_average = 1.46e+00
  validation accuracy: 97.75 (565 / 578), f1 (binary): 60.61, loss: 8.05e+02
  time: 5905s (wall 774s)
[c_cheb_a] step 30000 / 51990 (epoch 577.03 / 1000):
  learning_rate = 5.61e-04, loss_average = 1.93e+00
  validation accuracy: 96.71 (559 / 578), f1 (binary): 29.63, loss: 6.04e+02
  time: 6109s (wall 801s)
[c_cheb_a] step 31000 / 51990 (epoch 596.27 / 1000):
  learning_rate = 5.51e-04, loss_average = 2.02e+00
  validation accuracy: 92.56 (535 / 578), f1 (binary): 31.75, loss: 8.33e+02
  time: 6313s (wall 827s)
[c_cheb_a] step 32000 / 51990 (epoch 615.50 / 1000):
  learning_rate = 5.40e-04, loss_average = 1.28e+00
  validation accuracy: 97.75 (565 / 578), f1 (binary): 58.06, loss: 1.31e+03
  time: 6517s (wall 854s)
[c_cheb_a] step 33000 / 51990 (epoch 634.74 / 1000):
  learning_rate = 5.30e-04, loss_average = 1.39e+00
  validation accuracy: 96.02 (555 / 578), f1 (binary): 46.51, loss: 1.61e+03
  time: 6721s (wall 881s)
[c_cheb_a] step 34000 / 51990 (epoch 653.97 / 1000):
  learning_rate = 5.20e-04, loss_average = 8.49e-01
  validation accuracy: 97.23 (562 / 578), f1 (binary): 55.56, loss: 1.47e+03
  time: 6926s (wall 907s)
[c_cheb_a] step 35000 / 51990 (epoch 673.21 / 1000):
  learning_rate = 5.10e-04, loss_average = 4.61e-01
  validation accuracy: 97.06 (561 / 578), f1 (binary): 56.41, loss: 1.12e+03
  time: 7130s (wall 934s)
[c_cheb_a] step 36000 / 51990 (epoch 692.44 / 1000):
  learning_rate = 5.00e-04, loss_average = 1.45e+00
  validation accuracy: 96.71 (559 / 578), f1 (binary): 34.48, loss: 6.95e+02
  time: 7333s (wall 961s)
[c_cheb_a] step 37000 / 51990 (epoch 711.68 / 1000):
  learning_rate = 4.91e-04, loss_average = 6.16e-01
  validation accuracy: 97.23 (562 / 578), f1 (binary): 55.56, loss: 6.92e+02
  time: 7538s (wall 987s)
[c_cheb_a] step 38000 / 51990 (epoch 730.91 / 1000):
  learning_rate = 4.82e-04, loss_average = 4.77e-01
  validation accuracy: 96.71 (559 / 578), f1 (binary): 29.63, loss: 6.04e+02
  time: 7743s (wall 1014s)
[c_cheb_a] step 39000 / 51990 (epoch 750.14 / 1000):
  learning_rate = 4.72e-04, loss_average = 3.48e-01
  validation accuracy: 97.23 (562 / 578), f1 (binary): 61.90, loss: 3.73e+02
  time: 7947s (wall 1041s)
[c_cheb_a] step 40000 / 51990 (epoch 769.38 / 1000):
  learning_rate = 4.63e-04, loss_average = 5.31e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 48.78, loss: 1.70e+03
  time: 8152s (wall 1068s)
[c_cheb_a] step 41000 / 51990 (epoch 788.61 / 1000):
  learning_rate = 4.55e-04, loss_average = 4.11e-01
  validation accuracy: 96.89 (560 / 578), f1 (binary): 30.77, loss: 1.56e+03
  time: 8356s (wall 1094s)
[c_cheb_a] step 42000 / 51990 (epoch 807.85 / 1000):
  learning_rate = 4.46e-04, loss_average = 6.85e-01
  validation accuracy: 93.94 (543 / 578), f1 (binary): 42.62, loss: 1.48e+03
  time: 8560s (wall 1121s)
[c_cheb_a] step 43000 / 51990 (epoch 827.08 / 1000):
  learning_rate = 4.37e-04, loss_average = 4.53e-01
  validation accuracy: 97.23 (562 / 578), f1 (binary): 55.56, loss: 1.14e+03
  time: 8764s (wall 1148s)
[c_cheb_a] step 44000 / 51990 (epoch 846.32 / 1000):
  learning_rate = 4.29e-04, loss_average = 3.95e-01
  validation accuracy: 96.71 (559 / 578), f1 (binary): 55.81, loss: 8.77e+03
  time: 8969s (wall 1174s)
[c_cheb_a] step 45000 / 51990 (epoch 865.55 / 1000):
  learning_rate = 4.21e-04, loss_average = 2.60e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 48.89, loss: 8.16e+03
  time: 9174s (wall 1201s)
[c_cheb_a] step 46000 / 51990 (epoch 884.79 / 1000):
  learning_rate = 4.13e-04, loss_average = 2.02e-01
  validation accuracy: 97.92 (566 / 578), f1 (binary): 64.71, loss: 7.76e+03
  time: 9378s (wall 1228s)
[c_cheb_a] step 47000 / 51990 (epoch 904.02 / 1000):
  learning_rate = 4.05e-04, loss_average = 2.65e-01
  validation accuracy: 94.46 (546 / 578), f1 (binary): 42.86, loss: 6.67e+03
  time: 9582s (wall 1254s)
[c_cheb_a] step 48000 / 51990 (epoch 923.25 / 1000):
  learning_rate = 3.97e-04, loss_average = 3.33e-01
  validation accuracy: 97.23 (562 / 578), f1 (binary): 38.46, loss: 5.81e+03
  time: 9787s (wall 1281s)
[c_cheb_a] step 49000 / 51990 (epoch 942.49 / 1000):
  learning_rate = 3.90e-04, loss_average = 1.49e-01
  validation accuracy: 97.58 (564 / 578), f1 (binary): 50.00, loss: 5.10e+03
  time: 9991s (wall 1307s)
[c_cheb_a] step 50000 / 51990 (epoch 961.72 / 1000):
  learning_rate = 3.82e-04, loss_average = 8.82e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 22.22, loss: 1.86e+03
  time: 10195s (wall 1334s)
[c_cheb_a] step 51000 / 51990 (epoch 980.96 / 1000):
  learning_rate = 3.75e-04, loss_average = 2.37e-01
  validation accuracy: 96.19 (556 / 578), f1 (binary): 31.25, loss: 1.95e+03
  time: 10398s (wall 1361s)
[c_cheb_a] step 51990 / 51990 (epoch 1000.00 / 1000):
  learning_rate = 3.68e-04, loss_average = 1.40e-01
  validation accuracy: 95.85 (554 / 578), f1 (binary): 42.86, loss: 1.48e+03
  time: 10599s (wall 1387s)
validation accuracy: peak = 97.92, mean = 96.56
train accuracy: 97.71 (5080 / 5199), f1 (binary): 70.76, loss: 8.57e-02
time: 6s (wall 1s)
test  accuracy: 95.85 (554 / 578), f1 (binary): 42.86, loss: 1.48e+03
time: 1s (wall 0s)
 
Training model: np_3
 
  architecture/L = 2
  architecture/N = [25, 25, 25]
CNNGS Architecture: np_3 (no-pooling)
  input: M_0 = N = 25
  l_1: gsconv_1
    input dimension : M_0 = F_0 N_0 =  1 * 25 = 25
    output dimension: M_1 = F_1 N_1 = 14 * 25 = 350
    parameters: K_1 F_1 F_0 = 7 * 14 * 1 = 98
  l_2: gsconv_2
    input dimension : M_1 = F_1 N_1 = 14 * 25 = 350
    output dimension: M_2 = F_2 N_2 = 28 * 25 = 700
    parameters: K_2 F_2 F_1 = 14 * 28 * 14 = 5488
  l_3: softmax
    input dimension : M_2 = 700
    output dimension: M_3 = 2
    parameters: M_3 M_2 = 2 * 700 = 1400
  Total parameters = 6986
 
[np_3] step 1000 / 51990 (epoch 19.23 / 1000):
  learning_rate = 9.81e-04, loss_average = 1.81e+02
  validation accuracy: 97.40 (563 / 578), f1 (binary): 44.44, loss: 1.47e+02
  time: 304s (wall 24s)
[np_3] step 2000 / 51990 (epoch 38.47 / 1000):
  learning_rate = 9.63e-04, loss_average = 7.50e+01
  validation accuracy: 96.54 (558 / 578), f1 (binary): 54.55, loss: 6.28e+01
  time: 607s (wall 47s)
[np_3] step 3000 / 51990 (epoch 57.70 / 1000):
  learning_rate = 9.45e-04, loss_average = 5.42e+01
  validation accuracy: 97.92 (566 / 578), f1 (binary): 66.67, loss: 5.37e+01
  time: 910s (wall 70s)
[np_3] step 4000 / 51990 (epoch 76.94 / 1000):
  learning_rate = 9.27e-04, loss_average = 3.03e+01
  validation accuracy: 96.71 (559 / 578), f1 (binary): 17.39, loss: 6.83e+01
  time: 1212s (wall 94s)
[np_3] step 5000 / 51990 (epoch 96.17 / 1000):
  learning_rate = 9.08e-04, loss_average = 2.61e+01
  validation accuracy: 97.40 (563 / 578), f1 (binary): 59.46, loss: 4.97e+01
  time: 1515s (wall 117s)
[np_3] step 6000 / 51990 (epoch 115.41 / 1000):
  learning_rate = 8.91e-04, loss_average = 1.51e+01
  validation accuracy: 94.46 (546 / 578), f1 (binary): 50.00, loss: 5.33e+01
  time: 1818s (wall 140s)
[np_3] step 7000 / 51990 (epoch 134.64 / 1000):
  learning_rate = 8.75e-04, loss_average = 5.57e+00
  validation accuracy: 96.02 (555 / 578), f1 (binary): 51.06, loss: 5.60e+01
  time: 2122s (wall 164s)
[np_3] step 8000 / 51990 (epoch 153.88 / 1000):
  learning_rate = 8.58e-04, loss_average = 3.71e+00
  validation accuracy: 96.71 (559 / 578), f1 (binary): 61.22, loss: 4.08e+01
  time: 2425s (wall 187s)
[np_3] step 9000 / 51990 (epoch 173.11 / 1000):
  learning_rate = 8.41e-04, loss_average = 3.51e+00
  validation accuracy: 97.06 (561 / 578), f1 (binary): 56.41, loss: 4.37e+01
  time: 2729s (wall 210s)
[np_3] step 10000 / 51990 (epoch 192.34 / 1000):
  learning_rate = 8.25e-04, loss_average = 1.31e+01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 3.91e+01
  time: 3030s (wall 233s)
[np_3] step 11000 / 51990 (epoch 211.58 / 1000):
  learning_rate = 8.10e-04, loss_average = 5.95e-01
  validation accuracy: 98.10 (567 / 578), f1 (binary): 68.57, loss: 2.91e+01
  time: 3332s (wall 257s)
[np_3] step 12000 / 51990 (epoch 230.81 / 1000):
  learning_rate = 7.94e-04, loss_average = 1.38e+00
  validation accuracy: 93.60 (541 / 578), f1 (binary): 47.89, loss: 2.97e+01
  time: 3634s (wall 280s)
[np_3] step 13000 / 51990 (epoch 250.05 / 1000):
  learning_rate = 7.79e-04, loss_average = 9.09e-01
  validation accuracy: 96.89 (560 / 578), f1 (binary): 25.00, loss: 3.17e+01
  time: 3935s (wall 303s)
[np_3] step 14000 / 51990 (epoch 269.28 / 1000):
  learning_rate = 7.64e-04, loss_average = 5.43e-01
  validation accuracy: 97.23 (562 / 578), f1 (binary): 57.89, loss: 3.43e+01
  time: 4238s (wall 326s)
[np_3] step 15000 / 51990 (epoch 288.52 / 1000):
  learning_rate = 7.50e-04, loss_average = 3.01e-01
  validation accuracy: 97.40 (563 / 578), f1 (binary): 63.41, loss: 3.35e+01
  time: 4540s (wall 350s)
[np_3] step 16000 / 51990 (epoch 307.75 / 1000):
  learning_rate = 7.36e-04, loss_average = 7.34e-01
  validation accuracy: 91.87 (531 / 578), f1 (binary): 44.71, loss: 3.75e+01
  time: 4843s (wall 373s)
[np_3] step 17000 / 51990 (epoch 326.99 / 1000):
  learning_rate = 7.22e-04, loss_average = 3.53e-01
  validation accuracy: 98.79 (571 / 578), f1 (binary): 81.08, loss: 3.57e+01
  time: 5145s (wall 396s)
[np_3] step 18000 / 51990 (epoch 346.22 / 1000):
  learning_rate = 7.07e-04, loss_average = 5.74e-01
  validation accuracy: 91.87 (531 / 578), f1 (binary): 38.96, loss: 3.69e+01
  time: 5447s (wall 419s)
[np_3] step 19000 / 51990 (epoch 365.45 / 1000):
  learning_rate = 6.94e-04, loss_average = 8.81e-01
  validation accuracy: 97.06 (561 / 578), f1 (binary): 37.04, loss: 2.85e+01
  time: 5749s (wall 443s)
[np_3] step 20000 / 51990 (epoch 384.69 / 1000):
  learning_rate = 6.81e-04, loss_average = 2.08e-01
  validation accuracy: 96.54 (558 / 578), f1 (binary): 62.96, loss: 3.35e+01
  time: 6050s (wall 466s)
[np_3] step 21000 / 51990 (epoch 403.92 / 1000):
  learning_rate = 6.68e-04, loss_average = 1.23e-01
  validation accuracy: 95.50 (552 / 578), f1 (binary): 53.57, loss: 3.27e+01
  time: 6352s (wall 489s)
[np_3] step 22000 / 51990 (epoch 423.16 / 1000):
  learning_rate = 6.55e-04, loss_average = 1.25e-01
  validation accuracy: 97.92 (566 / 578), f1 (binary): 60.00, loss: 3.19e+01
  time: 6653s (wall 512s)
[np_3] step 23000 / 51990 (epoch 442.39 / 1000):
  learning_rate = 6.43e-04, loss_average = 4.37e-01
  validation accuracy: 96.71 (559 / 578), f1 (binary): 55.81, loss: 3.46e+01
  time: 6955s (wall 536s)
[np_3] step 24000 / 51990 (epoch 461.63 / 1000):
  learning_rate = 6.31e-04, loss_average = 1.86e-01
  validation accuracy: 96.71 (559 / 578), f1 (binary): 62.75, loss: 2.98e+01
  time: 7256s (wall 559s)
[np_3] step 25000 / 51990 (epoch 480.86 / 1000):
  learning_rate = 6.19e-04, loss_average = 7.74e-01
  validation accuracy: 93.08 (538 / 578), f1 (binary): 47.37, loss: 2.96e+01
  time: 7558s (wall 582s)
[np_3] step 26000 / 51990 (epoch 500.10 / 1000):
  learning_rate = 6.06e-04, loss_average = 2.32e-01
  validation accuracy: 96.54 (558 / 578), f1 (binary): 62.96, loss: 2.86e+01
  time: 7860s (wall 605s)
[np_3] step 27000 / 51990 (epoch 519.33 / 1000):
  learning_rate = 5.95e-04, loss_average = 8.01e-02
  validation accuracy: 97.23 (562 / 578), f1 (binary): 70.37, loss: 2.76e+01
  time: 8163s (wall 629s)
[np_3] step 28000 / 51990 (epoch 538.57 / 1000):
  learning_rate = 5.84e-04, loss_average = 9.41e-02
  validation accuracy: 97.40 (563 / 578), f1 (binary): 70.59, loss: 2.71e+01
  time: 8465s (wall 652s)
[np_3] step 29000 / 51990 (epoch 557.80 / 1000):
  learning_rate = 5.73e-04, loss_average = 1.42e-01
  validation accuracy: 98.27 (568 / 578), f1 (binary): 78.26, loss: 2.61e+01
  time: 8766s (wall 675s)
[np_3] step 30000 / 51990 (epoch 577.03 / 1000):
  learning_rate = 5.61e-04, loss_average = 3.26e-01
  validation accuracy: 97.92 (566 / 578), f1 (binary): 64.71, loss: 2.56e+01
  time: 9068s (wall 698s)
[np_3] step 31000 / 51990 (epoch 596.27 / 1000):
  learning_rate = 5.51e-04, loss_average = 2.88e-01
  validation accuracy: 97.92 (566 / 578), f1 (binary): 64.71, loss: 2.57e+01
  time: 9371s (wall 722s)
[np_3] step 32000 / 51990 (epoch 615.50 / 1000):
  learning_rate = 5.40e-04, loss_average = 2.30e+00
  validation accuracy: 91.52 (529 / 578), f1 (binary): 34.67, loss: 2.81e+01
  time: 9672s (wall 745s)
[np_3] step 33000 / 51990 (epoch 634.74 / 1000):
  learning_rate = 5.30e-04, loss_average = 7.16e-02
  validation accuracy: 98.96 (572 / 578), f1 (binary): 86.36, loss: 2.22e+01
  time: 9974s (wall 768s)
[np_3] step 34000 / 51990 (epoch 653.97 / 1000):
  learning_rate = 5.20e-04, loss_average = 7.63e-02
  validation accuracy: 97.40 (563 / 578), f1 (binary): 69.39, loss: 2.11e+01
  time: 10276s (wall 791s)
[np_3] step 35000 / 51990 (epoch 673.21 / 1000):
  learning_rate = 5.10e-04, loss_average = 1.36e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 63.16, loss: 2.13e+01
  time: 10578s (wall 815s)
[np_3] step 36000 / 51990 (epoch 692.44 / 1000):
  learning_rate = 5.00e-04, loss_average = 3.54e-02
  validation accuracy: 97.92 (566 / 578), f1 (binary): 70.00, loss: 2.18e+01
  time: 10880s (wall 838s)
[np_3] step 37000 / 51990 (epoch 711.68 / 1000):
  learning_rate = 4.91e-04, loss_average = 6.13e-02
  validation accuracy: 96.37 (557 / 578), f1 (binary): 63.16, loss: 2.20e+01
  time: 11182s (wall 861s)
[np_3] step 38000 / 51990 (epoch 730.91 / 1000):
  learning_rate = 4.82e-04, loss_average = 5.67e-02
  validation accuracy: 98.44 (569 / 578), f1 (binary): 76.92, loss: 2.30e+01
  time: 11484s (wall 884s)
[np_3] step 39000 / 51990 (epoch 750.14 / 1000):
  learning_rate = 4.72e-04, loss_average = 3.83e-02
  validation accuracy: 97.75 (565 / 578), f1 (binary): 68.29, loss: 2.36e+01
  time: 11787s (wall 908s)
[np_3] step 40000 / 51990 (epoch 769.38 / 1000):
  learning_rate = 4.63e-04, loss_average = 4.20e-02
  validation accuracy: 97.92 (566 / 578), f1 (binary): 71.43, loss: 2.33e+01
  time: 12089s (wall 931s)
[np_3] step 41000 / 51990 (epoch 788.61 / 1000):
  learning_rate = 4.55e-04, loss_average = 8.85e-02
  validation accuracy: 98.10 (567 / 578), f1 (binary): 74.42, loss: 2.31e+01
  time: 12391s (wall 954s)
[np_3] step 42000 / 51990 (epoch 807.85 / 1000):
  learning_rate = 4.46e-04, loss_average = 2.38e-02
  validation accuracy: 98.27 (568 / 578), f1 (binary): 70.59, loss: 2.36e+01
  time: 12692s (wall 977s)
[np_3] step 43000 / 51990 (epoch 827.08 / 1000):
  learning_rate = 4.37e-04, loss_average = 2.15e-02
  validation accuracy: 98.10 (567 / 578), f1 (binary): 75.56, loss: 2.38e+01
  time: 12994s (wall 1001s)
[np_3] step 44000 / 51990 (epoch 846.32 / 1000):
  learning_rate = 4.29e-04, loss_average = 2.52e-02
  validation accuracy: 97.92 (566 / 578), f1 (binary): 72.73, loss: 2.34e+01
  time: 13296s (wall 1024s)
[np_3] step 45000 / 51990 (epoch 865.55 / 1000):
  learning_rate = 4.21e-04, loss_average = 1.79e-01
  validation accuracy: 98.10 (567 / 578), f1 (binary): 66.67, loss: 2.33e+01
  time: 13596s (wall 1047s)
[np_3] step 46000 / 51990 (epoch 884.79 / 1000):
  learning_rate = 4.13e-04, loss_average = 3.23e-02
  validation accuracy: 97.23 (562 / 578), f1 (binary): 61.90, loss: 2.29e+01
  time: 13897s (wall 1070s)
[np_3] step 47000 / 51990 (epoch 904.02 / 1000):
  learning_rate = 4.05e-04, loss_average = 2.46e-02
  validation accuracy: 98.62 (570 / 578), f1 (binary): 78.95, loss: 2.28e+01
  time: 14198s (wall 1093s)
[np_3] step 48000 / 51990 (epoch 923.25 / 1000):
  learning_rate = 3.97e-04, loss_average = 2.11e-01
  validation accuracy: 98.10 (567 / 578), f1 (binary): 66.67, loss: 2.39e+01
  time: 14498s (wall 1117s)
[np_3] step 49000 / 51990 (epoch 942.49 / 1000):
  learning_rate = 3.90e-04, loss_average = 3.96e-02
  validation accuracy: 98.10 (567 / 578), f1 (binary): 76.60, loss: 2.38e+01
  time: 14800s (wall 1140s)
[np_3] step 50000 / 51990 (epoch 961.72 / 1000):
  learning_rate = 3.82e-04, loss_average = 1.15e-02
  validation accuracy: 97.92 (566 / 578), f1 (binary): 71.43, loss: 2.34e+01
  time: 15102s (wall 1163s)
[np_3] step 51000 / 51990 (epoch 980.96 / 1000):
  learning_rate = 3.75e-04, loss_average = 1.05e-01
  validation accuracy: 98.62 (570 / 578), f1 (binary): 80.95, loss: 2.33e+01
  time: 15403s (wall 1186s)
[np_3] step 51990 / 51990 (epoch 1000.00 / 1000):
  learning_rate = 3.68e-04, loss_average = 9.62e-03
  validation accuracy: 98.27 (568 / 578), f1 (binary): 76.19, loss: 2.26e+01
  time: 15702s (wall 1209s)
validation accuracy: peak = 98.96, mean = 98.10
train accuracy: 99.81 (5189 / 5199), f1 (binary): 97.46, loss: 6.97e-03
time: 10s (wall 1s)
test  accuracy: 98.27 (568 / 578), f1 (binary): 76.19, loss: 2.26e+01
time: 1s (wall 0s)
 
Training model: selection_pooling
 
  architecture/L = 2
  architecture/N = [25, 25, 15]
CNNGS Architecture: selection_pooling (selection)
  input: M_0 = N = 25
  l_1: gsconv_1
    input dimension : M_0 = F_0 N_0 =  1 * 25 = 25
    output dimension: M_1 = F_1 N_1 = 16 * 25 = 400
    parameters: K_1 F_1 F_0 = 16 * 16 * 1 = 256
  l_2: gsconv_2
    input dimension : M_1 = F_1 N_1 = 16 * 25 = 400
    output dimension: M_2 = F_2 N_2 = 16 * 15 = 240
    parameters: K_2 F_2 F_1 = 16 * 16 * 16 = 4096
  l_3: softmax
    input dimension : M_2 = 240
    output dimension: M_3 = 2
    parameters: M_3 M_2 = 2 * 240 = 480
  Total parameters = 4832
 
[selection_pooling] step 1000 / 51990 (epoch 19.23 / 1000):
  learning_rate = 9.81e-04, loss_average = 2.82e+02
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 6.29e+02
  time: 528s (wall 39s)
[selection_pooling] step 2000 / 51990 (epoch 38.47 / 1000):
  learning_rate = 9.63e-04, loss_average = 9.24e+00
  validation accuracy: 92.91 (537 / 578), f1 (binary): 8.89, loss: 1.53e+01
  time: 1055s (wall 78s)
[selection_pooling] step 3000 / 51990 (epoch 57.70 / 1000):
  learning_rate = 9.45e-04, loss_average = 6.72e+00
  validation accuracy: 93.08 (538 / 578), f1 (binary): 4.76, loss: 8.37e+00
  time: 1584s (wall 117s)
[selection_pooling] step 4000 / 51990 (epoch 76.94 / 1000):
  learning_rate = 9.27e-04, loss_average = 8.64e+00
  validation accuracy: 94.64 (547 / 578), f1 (binary): 6.06, loss: 7.11e+00
  time: 2113s (wall 155s)
[selection_pooling] step 5000 / 51990 (epoch 96.17 / 1000):
  learning_rate = 9.08e-04, loss_average = 7.69e+00
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 3.71e+01
  time: 2640s (wall 194s)
[selection_pooling] step 6000 / 51990 (epoch 115.41 / 1000):
  learning_rate = 8.91e-04, loss_average = 3.78e+00
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 7.88e+00
  time: 3167s (wall 233s)
[selection_pooling] step 7000 / 51990 (epoch 134.64 / 1000):
  learning_rate = 8.75e-04, loss_average = 5.50e+01
  validation accuracy: 93.77 (542 / 578), f1 (binary): 0.00, loss: 3.45e+02
  time: 3691s (wall 272s)
[selection_pooling] step 8000 / 51990 (epoch 153.88 / 1000):
  learning_rate = 8.58e-04, loss_average = 1.15e+01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 3.85e+01
  time: 4215s (wall 310s)
[selection_pooling] step 9000 / 51990 (epoch 173.11 / 1000):
  learning_rate = 8.41e-04, loss_average = 2.18e+00
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 1.50e+01
  time: 4739s (wall 349s)
[selection_pooling] step 10000 / 51990 (epoch 192.34 / 1000):
  learning_rate = 8.25e-04, loss_average = 2.32e+00
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 2.47e+01
  time: 5264s (wall 388s)
[selection_pooling] step 11000 / 51990 (epoch 211.58 / 1000):
  learning_rate = 8.10e-04, loss_average = 1.25e+00
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 2.60e+00
  time: 5791s (wall 426s)
[selection_pooling] step 12000 / 51990 (epoch 230.81 / 1000):
  learning_rate = 7.94e-04, loss_average = 3.79e+00
  validation accuracy: 95.67 (553 / 578), f1 (binary): 13.79, loss: 2.54e+01
  time: 6317s (wall 465s)
[selection_pooling] step 13000 / 51990 (epoch 250.05 / 1000):
  learning_rate = 7.79e-04, loss_average = 4.69e-01
  validation accuracy: 82.18 (475 / 578), f1 (binary): 17.60, loss: 5.77e-01
  time: 6843s (wall 504s)
[selection_pooling] step 14000 / 51990 (epoch 269.28 / 1000):
  learning_rate = 7.64e-04, loss_average = 1.66e-01
  validation accuracy: 96.19 (556 / 578), f1 (binary): 15.38, loss: 1.62e-01
  time: 7368s (wall 543s)
[selection_pooling] step 15000 / 51990 (epoch 288.52 / 1000):
  learning_rate = 7.50e-04, loss_average = 1.37e-01
  validation accuracy: 95.67 (553 / 578), f1 (binary): 13.79, loss: 2.05e-01
  time: 7892s (wall 581s)
[selection_pooling] step 16000 / 51990 (epoch 307.75 / 1000):
  learning_rate = 7.36e-04, loss_average = 1.47e-01
  validation accuracy: 96.54 (558 / 578), f1 (binary): 16.67, loss: 1.51e-01
  time: 8416s (wall 620s)
[selection_pooling] step 17000 / 51990 (epoch 326.99 / 1000):
  learning_rate = 7.22e-04, loss_average = 9.49e+00
  validation accuracy: 93.43 (540 / 578), f1 (binary): 0.00, loss: 1.55e+02
  time: 8942s (wall 659s)
[selection_pooling] step 18000 / 51990 (epoch 346.22 / 1000):
  learning_rate = 7.07e-04, loss_average = 7.80e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 1.74e+02
  time: 9467s (wall 698s)
[selection_pooling] step 19000 / 51990 (epoch 365.45 / 1000):
  learning_rate = 6.94e-04, loss_average = 2.64e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 1.37e+02
  time: 9991s (wall 736s)
[selection_pooling] step 20000 / 51990 (epoch 384.69 / 1000):
  learning_rate = 6.81e-04, loss_average = 1.91e-01
  validation accuracy: 95.50 (552 / 578), f1 (binary): 7.14, loss: 1.43e+02
  time: 10515s (wall 775s)
[selection_pooling] step 21000 / 51990 (epoch 403.92 / 1000):
  learning_rate = 6.68e-04, loss_average = 1.56e-01
  validation accuracy: 96.54 (558 / 578), f1 (binary): 9.09, loss: 1.34e+02
  time: 11039s (wall 814s)
[selection_pooling] step 22000 / 51990 (epoch 423.16 / 1000):
  learning_rate = 6.55e-04, loss_average = 4.91e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.47e+02
  time: 11565s (wall 852s)
[selection_pooling] step 23000 / 51990 (epoch 442.39 / 1000):
  learning_rate = 6.43e-04, loss_average = 1.68e-01
  validation accuracy: 92.73 (536 / 578), f1 (binary): 25.00, loss: 3.88e+02
  time: 12090s (wall 891s)
[selection_pooling] step 24000 / 51990 (epoch 461.63 / 1000):
  learning_rate = 6.31e-04, loss_average = 1.40e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 3.35e+02
  time: 12613s (wall 930s)
[selection_pooling] step 25000 / 51990 (epoch 480.86 / 1000):
  learning_rate = 6.19e-04, loss_average = 4.84e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 6.52e+02
  time: 13138s (wall 968s)
[selection_pooling] step 26000 / 51990 (epoch 500.10 / 1000):
  learning_rate = 6.06e-04, loss_average = 2.37e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 4.59e+02
  time: 13662s (wall 1007s)
[selection_pooling] step 27000 / 51990 (epoch 519.33 / 1000):
  learning_rate = 5.95e-04, loss_average = 1.54e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 4.48e+02
  time: 14186s (wall 1046s)
[selection_pooling] step 28000 / 51990 (epoch 538.57 / 1000):
  learning_rate = 5.84e-04, loss_average = 1.45e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 4.39e+02
  time: 14712s (wall 1086s)
[selection_pooling] step 29000 / 51990 (epoch 557.80 / 1000):
  learning_rate = 5.73e-04, loss_average = 1.47e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 4.35e+02
  time: 15235s (wall 1125s)
[selection_pooling] step 30000 / 51990 (epoch 577.03 / 1000):
  learning_rate = 5.61e-04, loss_average = 1.51e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 4.35e+02
  time: 15758s (wall 1164s)
[selection_pooling] step 31000 / 51990 (epoch 596.27 / 1000):
  learning_rate = 5.51e-04, loss_average = 1.28e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 4.35e+02
  time: 16282s (wall 1202s)
[selection_pooling] step 32000 / 51990 (epoch 615.50 / 1000):
  learning_rate = 5.40e-04, loss_average = 1.61e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 4.30e+02
  time: 16805s (wall 1241s)
[selection_pooling] step 33000 / 51990 (epoch 634.74 / 1000):
  learning_rate = 5.30e-04, loss_average = 1.43e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 4.30e+02
  time: 17327s (wall 1280s)
[selection_pooling] step 34000 / 51990 (epoch 653.97 / 1000):
  learning_rate = 5.20e-04, loss_average = 1.50e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 4.30e+02
  time: 17849s (wall 1318s)
[selection_pooling] step 35000 / 51990 (epoch 673.21 / 1000):
  learning_rate = 5.10e-04, loss_average = 1.60e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 4.30e+02
  time: 18371s (wall 1357s)
[selection_pooling] step 36000 / 51990 (epoch 692.44 / 1000):
  learning_rate = 5.00e-04, loss_average = 1.49e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 4.30e+02
  time: 18893s (wall 1396s)
[selection_pooling] step 37000 / 51990 (epoch 711.68 / 1000):
  learning_rate = 4.91e-04, loss_average = 1.54e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 4.30e+02
  time: 19416s (wall 1434s)
[selection_pooling] step 38000 / 51990 (epoch 730.91 / 1000):
  learning_rate = 4.82e-04, loss_average = 1.42e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 4.30e+02
  time: 19938s (wall 1473s)
[selection_pooling] step 39000 / 51990 (epoch 750.14 / 1000):
  learning_rate = 4.72e-04, loss_average = 1.72e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 4.30e+02
  time: 20460s (wall 1511s)
[selection_pooling] step 40000 / 51990 (epoch 769.38 / 1000):
  learning_rate = 4.63e-04, loss_average = 1.39e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 4.30e+02
  time: 20983s (wall 1550s)
[selection_pooling] step 41000 / 51990 (epoch 788.61 / 1000):
  learning_rate = 4.55e-04, loss_average = 1.67e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 6.42e+02
  time: 21507s (wall 1589s)
[selection_pooling] step 42000 / 51990 (epoch 807.85 / 1000):
  learning_rate = 4.46e-04, loss_average = 1.46e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 6.42e+02
  time: 22030s (wall 1627s)
[selection_pooling] step 43000 / 51990 (epoch 827.08 / 1000):
  learning_rate = 4.37e-04, loss_average = 1.49e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 6.42e+02
  time: 22553s (wall 1666s)
[selection_pooling] step 44000 / 51990 (epoch 846.32 / 1000):
  learning_rate = 4.29e-04, loss_average = 1.37e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 6.42e+02
  time: 23078s (wall 1705s)
[selection_pooling] step 45000 / 51990 (epoch 865.55 / 1000):
  learning_rate = 4.21e-04, loss_average = 1.38e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 6.42e+02
  time: 23599s (wall 1743s)
[selection_pooling] step 46000 / 51990 (epoch 884.79 / 1000):
  learning_rate = 4.13e-04, loss_average = 1.53e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 6.42e+02
  time: 24119s (wall 1782s)
[selection_pooling] step 47000 / 51990 (epoch 904.02 / 1000):
  learning_rate = 4.05e-04, loss_average = 1.57e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 6.42e+02
  time: 24639s (wall 1820s)
[selection_pooling] step 48000 / 51990 (epoch 923.25 / 1000):
  learning_rate = 3.97e-04, loss_average = 1.46e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 6.42e+02
  time: 25163s (wall 1859s)
[selection_pooling] step 49000 / 51990 (epoch 942.49 / 1000):
  learning_rate = 3.90e-04, loss_average = 1.61e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 6.42e+02
  time: 25687s (wall 1897s)
[selection_pooling] step 50000 / 51990 (epoch 961.72 / 1000):
  learning_rate = 3.82e-04, loss_average = 1.59e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 6.42e+02
  time: 26209s (wall 1936s)
[selection_pooling] step 51000 / 51990 (epoch 980.96 / 1000):
  learning_rate = 3.75e-04, loss_average = 1.47e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 6.42e+02
  time: 26730s (wall 1974s)
[selection_pooling] step 51990 / 51990 (epoch 1000.00 / 1000):
  learning_rate = 3.68e-04, loss_average = 1.62e+00
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 3.67e+02
  time: 27247s (wall 2012s)
validation accuracy: peak = 96.54, mean = 96.37
train accuracy: 96.21 (5002 / 5199), f1 (binary): 0.00, loss: 6.75e+00
time: 16s (wall 1s)
test  accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 3.67e+02
time: 2s (wall 0s)
 
Training model: aggregation_pooling
 
  architecture/L = 2
  architecture/N = [25, 12, 6]
CNNGS Architecture: aggregation_pooling (aggregation)
  input: M_0 = N = 25
  l_1: gsconv_1
    input dimension : M_0 = F_0 N_0 =  1 * 25 = 25
    output dimension: M_1 = F_1 N_1 = 16 * 12 = 192
    parameters: K_1 F_1 F_0 = 16 * 16 * 1 = 256
  l_2: gsconv_2
    input dimension : M_1 = F_1 N_1 = 16 * 12 = 192
    output dimension: M_2 = F_2 N_2 = 16 *  6 = 96
    parameters: K_2 F_2 F_1 = 16 * 16 * 16 = 4096
  l_3: softmax
    input dimension : M_2 = 96
    output dimension: M_3 = 2
    parameters: M_3 M_2 = 2 * 96 = 192
  Total parameters = 4544
 
[aggregation_pooling] step 1000 / 51990 (epoch 19.23 / 1000):
  learning_rate = 9.81e-04, loss_average = 1.59e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 0.00, loss: 3.95e-01
  time: 100s (wall 15s)
[aggregation_pooling] step 2000 / 51990 (epoch 38.47 / 1000):
  learning_rate = 9.63e-04, loss_average = 1.18e-01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 8.70, loss: 4.03e-01
  time: 201s (wall 29s)
[aggregation_pooling] step 3000 / 51990 (epoch 57.70 / 1000):
  learning_rate = 9.45e-04, loss_average = 1.06e-01
  validation accuracy: 96.19 (556 / 578), f1 (binary): 8.33, loss: 4.94e-01
  time: 300s (wall 43s)
[aggregation_pooling] step 4000 / 51990 (epoch 76.94 / 1000):
  learning_rate = 9.27e-04, loss_average = 1.07e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 8.00, loss: 5.28e-01
  time: 400s (wall 57s)
[aggregation_pooling] step 5000 / 51990 (epoch 96.17 / 1000):
  learning_rate = 9.08e-04, loss_average = 1.08e-01
  validation accuracy: 96.19 (556 / 578), f1 (binary): 8.33, loss: 5.80e-01
  time: 501s (wall 71s)
[aggregation_pooling] step 6000 / 51990 (epoch 115.41 / 1000):
  learning_rate = 8.91e-04, loss_average = 1.05e-01
  validation accuracy: 95.67 (553 / 578), f1 (binary): 19.35, loss: 4.36e-01
  time: 600s (wall 85s)
[aggregation_pooling] step 7000 / 51990 (epoch 134.64 / 1000):
  learning_rate = 8.75e-04, loss_average = 1.01e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 14.81, loss: 5.67e-01
  time: 700s (wall 100s)
[aggregation_pooling] step 8000 / 51990 (epoch 153.88 / 1000):
  learning_rate = 8.58e-04, loss_average = 8.06e-02
  validation accuracy: 96.54 (558 / 578), f1 (binary): 16.67, loss: 6.43e-01
  time: 799s (wall 114s)
[aggregation_pooling] step 9000 / 51990 (epoch 173.11 / 1000):
  learning_rate = 8.41e-04, loss_average = 6.93e-02
  validation accuracy: 96.89 (560 / 578), f1 (binary): 30.77, loss: 8.43e-01
  time: 899s (wall 128s)
[aggregation_pooling] step 10000 / 51990 (epoch 192.34 / 1000):
  learning_rate = 8.25e-04, loss_average = 6.02e-02
  validation accuracy: 96.71 (559 / 578), f1 (binary): 42.42, loss: 8.13e-01
  time: 998s (wall 142s)
[aggregation_pooling] step 11000 / 51990 (epoch 211.58 / 1000):
  learning_rate = 8.10e-04, loss_average = 6.51e-02
  validation accuracy: 96.37 (557 / 578), f1 (binary): 16.00, loss: 9.39e-01
  time: 1098s (wall 156s)
[aggregation_pooling] step 12000 / 51990 (epoch 230.81 / 1000):
  learning_rate = 7.94e-04, loss_average = 6.21e-02
  validation accuracy: 96.71 (559 / 578), f1 (binary): 17.39, loss: 9.67e-01
  time: 1197s (wall 170s)
[aggregation_pooling] step 13000 / 51990 (epoch 250.05 / 1000):
  learning_rate = 7.79e-04, loss_average = 6.02e-02
  validation accuracy: 96.54 (558 / 578), f1 (binary): 28.57, loss: 7.95e-01
  time: 1297s (wall 185s)
[aggregation_pooling] step 14000 / 51990 (epoch 269.28 / 1000):
  learning_rate = 7.64e-04, loss_average = 7.24e-02
  validation accuracy: 96.71 (559 / 578), f1 (binary): 38.71, loss: 7.60e-01
  time: 1397s (wall 199s)
[aggregation_pooling] step 15000 / 51990 (epoch 288.52 / 1000):
  learning_rate = 7.50e-04, loss_average = 6.18e-02
  validation accuracy: 96.54 (558 / 578), f1 (binary): 16.67, loss: 1.21e+00
  time: 1497s (wall 213s)
[aggregation_pooling] step 16000 / 51990 (epoch 307.75 / 1000):
  learning_rate = 7.36e-04, loss_average = 5.48e-02
  validation accuracy: 96.89 (560 / 578), f1 (binary): 30.77, loss: 1.19e+00
  time: 1597s (wall 227s)
[aggregation_pooling] step 17000 / 51990 (epoch 326.99 / 1000):
  learning_rate = 7.22e-04, loss_average = 8.80e-02
  validation accuracy: 96.71 (559 / 578), f1 (binary): 24.00, loss: 1.20e+00
  time: 1698s (wall 242s)
[aggregation_pooling] step 18000 / 51990 (epoch 346.22 / 1000):
  learning_rate = 7.07e-04, loss_average = 4.77e-02
  validation accuracy: 96.54 (558 / 578), f1 (binary): 33.33, loss: 1.11e+00
  time: 1797s (wall 256s)
[aggregation_pooling] step 19000 / 51990 (epoch 365.45 / 1000):
  learning_rate = 6.94e-04, loss_average = 4.38e-02
  validation accuracy: 96.89 (560 / 578), f1 (binary): 35.71, loss: 9.50e-01
  time: 1897s (wall 270s)
[aggregation_pooling] step 20000 / 51990 (epoch 384.69 / 1000):
  learning_rate = 6.81e-04, loss_average = 3.84e-02
  validation accuracy: 97.40 (563 / 578), f1 (binary): 44.44, loss: 8.80e-01
  time: 1996s (wall 284s)
[aggregation_pooling] step 21000 / 51990 (epoch 403.92 / 1000):
  learning_rate = 6.68e-04, loss_average = 3.10e-02
  validation accuracy: 97.23 (562 / 578), f1 (binary): 46.67, loss: 6.62e-01
  time: 2096s (wall 298s)
[aggregation_pooling] step 22000 / 51990 (epoch 423.16 / 1000):
  learning_rate = 6.55e-04, loss_average = 2.90e-02
  validation accuracy: 97.06 (561 / 578), f1 (binary): 37.04, loss: 5.73e-01
  time: 2195s (wall 313s)
[aggregation_pooling] step 23000 / 51990 (epoch 442.39 / 1000):
  learning_rate = 6.43e-04, loss_average = 2.46e-02
  validation accuracy: 96.71 (559 / 578), f1 (binary): 38.71, loss: 4.60e-01
  time: 2293s (wall 327s)
[aggregation_pooling] step 24000 / 51990 (epoch 461.63 / 1000):
  learning_rate = 6.31e-04, loss_average = 2.13e-02
  validation accuracy: 96.71 (559 / 578), f1 (binary): 34.48, loss: 3.05e-01
  time: 2392s (wall 341s)
[aggregation_pooling] step 25000 / 51990 (epoch 480.86 / 1000):
  learning_rate = 6.19e-04, loss_average = 2.05e-02
  validation accuracy: 96.71 (559 / 578), f1 (binary): 34.48, loss: 3.86e-01
  time: 2491s (wall 355s)
[aggregation_pooling] step 26000 / 51990 (epoch 500.10 / 1000):
  learning_rate = 6.06e-04, loss_average = 1.90e-02
  validation accuracy: 96.71 (559 / 578), f1 (binary): 34.48, loss: 5.15e-01
  time: 2590s (wall 369s)
[aggregation_pooling] step 27000 / 51990 (epoch 519.33 / 1000):
  learning_rate = 5.95e-04, loss_average = 1.90e-02
  validation accuracy: 97.23 (562 / 578), f1 (binary): 46.67, loss: 4.64e-01
  time: 2690s (wall 384s)
[aggregation_pooling] step 28000 / 51990 (epoch 538.57 / 1000):
  learning_rate = 5.84e-04, loss_average = 1.30e-02
  validation accuracy: 96.37 (557 / 578), f1 (binary): 40.00, loss: 6.44e-01
  time: 2789s (wall 398s)
[aggregation_pooling] step 29000 / 51990 (epoch 557.80 / 1000):
  learning_rate = 5.73e-04, loss_average = 1.94e-02
  validation accuracy: 95.67 (553 / 578), f1 (binary): 24.24, loss: 5.02e-01
  time: 2888s (wall 412s)
[aggregation_pooling] step 30000 / 51990 (epoch 577.03 / 1000):
  learning_rate = 5.61e-04, loss_average = 2.65e-02
  validation accuracy: 95.85 (554 / 578), f1 (binary): 33.33, loss: 2.92e-01
  time: 2987s (wall 426s)
[aggregation_pooling] step 31000 / 51990 (epoch 596.27 / 1000):
  learning_rate = 5.51e-04, loss_average = 7.07e-03
  validation accuracy: 97.40 (563 / 578), f1 (binary): 48.28, loss: 3.78e-01
  time: 3086s (wall 440s)
[aggregation_pooling] step 32000 / 51990 (epoch 615.50 / 1000):
  learning_rate = 5.40e-04, loss_average = 4.62e-03
  validation accuracy: 96.54 (558 / 578), f1 (binary): 37.50, loss: 4.14e-01
  time: 3185s (wall 455s)
[aggregation_pooling] step 33000 / 51990 (epoch 634.74 / 1000):
  learning_rate = 5.30e-04, loss_average = 6.18e-03
  validation accuracy: 97.23 (562 / 578), f1 (binary): 46.67, loss: 5.32e-01
  time: 3284s (wall 469s)
[aggregation_pooling] step 34000 / 51990 (epoch 653.97 / 1000):
  learning_rate = 5.20e-04, loss_average = 5.15e-03
  validation accuracy: 96.54 (558 / 578), f1 (binary): 44.44, loss: 4.75e-01
  time: 3383s (wall 483s)
[aggregation_pooling] step 35000 / 51990 (epoch 673.21 / 1000):
  learning_rate = 5.10e-04, loss_average = 8.70e-03
  validation accuracy: 96.19 (556 / 578), f1 (binary): 35.29, loss: 5.75e-01
  time: 3482s (wall 497s)
[aggregation_pooling] step 36000 / 51990 (epoch 692.44 / 1000):
  learning_rate = 5.00e-04, loss_average = 1.35e-02
  validation accuracy: 96.37 (557 / 578), f1 (binary): 43.24, loss: 5.27e-01
  time: 3582s (wall 512s)
[aggregation_pooling] step 37000 / 51990 (epoch 711.68 / 1000):
  learning_rate = 4.91e-04, loss_average = 2.44e-03
  validation accuracy: 96.89 (560 / 578), f1 (binary): 43.75, loss: 6.62e-01
  time: 3681s (wall 526s)
[aggregation_pooling] step 38000 / 51990 (epoch 730.91 / 1000):
  learning_rate = 4.82e-04, loss_average = 2.20e-03
  validation accuracy: 96.71 (559 / 578), f1 (binary): 42.42, loss: 6.07e-01
  time: 3780s (wall 540s)
[aggregation_pooling] step 39000 / 51990 (epoch 750.14 / 1000):
  learning_rate = 4.72e-04, loss_average = 4.33e-02
  validation accuracy: 96.37 (557 / 578), f1 (binary): 40.00, loss: 4.25e-01
  time: 3879s (wall 554s)
[aggregation_pooling] step 40000 / 51990 (epoch 769.38 / 1000):
  learning_rate = 4.63e-04, loss_average = 6.16e-03
  validation accuracy: 96.37 (557 / 578), f1 (binary): 40.00, loss: 4.48e-01
  time: 3978s (wall 568s)
[aggregation_pooling] step 41000 / 51990 (epoch 788.61 / 1000):
  learning_rate = 4.55e-04, loss_average = 2.37e-03
  validation accuracy: 96.71 (559 / 578), f1 (binary): 29.63, loss: 1.14e+00
  time: 4077s (wall 583s)
[aggregation_pooling] step 42000 / 51990 (epoch 807.85 / 1000):
  learning_rate = 4.46e-04, loss_average = 8.59e-04
  validation accuracy: 96.71 (559 / 578), f1 (binary): 42.42, loss: 1.20e+00
  time: 4176s (wall 597s)
[aggregation_pooling] step 43000 / 51990 (epoch 827.08 / 1000):
  learning_rate = 4.37e-04, loss_average = 2.68e-03
  validation accuracy: 96.19 (556 / 578), f1 (binary): 42.11, loss: 1.02e+00
  time: 4275s (wall 611s)
[aggregation_pooling] step 44000 / 51990 (epoch 846.32 / 1000):
  learning_rate = 4.29e-04, loss_average = 8.24e-04
  validation accuracy: 96.71 (559 / 578), f1 (binary): 42.42, loss: 1.09e+00
  time: 4374s (wall 625s)
[aggregation_pooling] step 45000 / 51990 (epoch 865.55 / 1000):
  learning_rate = 4.21e-04, loss_average = 3.45e-03
  validation accuracy: 96.71 (559 / 578), f1 (binary): 29.63, loss: 9.29e-01
  time: 4473s (wall 639s)
[aggregation_pooling] step 46000 / 51990 (epoch 884.79 / 1000):
  learning_rate = 4.13e-04, loss_average = 7.85e-04
  validation accuracy: 96.71 (559 / 578), f1 (binary): 34.48, loss: 1.04e+00
  time: 4573s (wall 654s)
[aggregation_pooling] step 47000 / 51990 (epoch 904.02 / 1000):
  learning_rate = 4.05e-04, loss_average = 1.37e-02
  validation accuracy: 96.54 (558 / 578), f1 (binary): 37.50, loss: 1.13e+00
  time: 4672s (wall 668s)
[aggregation_pooling] step 48000 / 51990 (epoch 923.25 / 1000):
  learning_rate = 3.97e-04, loss_average = 3.40e-03
  validation accuracy: 96.37 (557 / 578), f1 (binary): 32.26, loss: 9.23e-01
  time: 4772s (wall 682s)
[aggregation_pooling] step 49000 / 51990 (epoch 942.49 / 1000):
  learning_rate = 3.90e-04, loss_average = 7.83e-04
  validation accuracy: 96.54 (558 / 578), f1 (binary): 28.57, loss: 1.03e+00
  time: 4873s (wall 697s)
[aggregation_pooling] step 50000 / 51990 (epoch 961.72 / 1000):
  learning_rate = 3.82e-04, loss_average = 8.78e-04
  validation accuracy: 96.71 (559 / 578), f1 (binary): 38.71, loss: 1.04e+00
  time: 4972s (wall 711s)
[aggregation_pooling] step 51000 / 51990 (epoch 980.96 / 1000):
  learning_rate = 3.75e-04, loss_average = 6.61e-04
  validation accuracy: 96.71 (559 / 578), f1 (binary): 34.48, loss: 1.10e+00
  time: 5071s (wall 725s)
[aggregation_pooling] step 51990 / 51990 (epoch 1000.00 / 1000):
  learning_rate = 3.68e-04, loss_average = 2.69e-02
  validation accuracy: 96.37 (557 / 578), f1 (binary): 40.00, loss: 9.83e-01
  time: 5169s (wall 739s)
validation accuracy: peak = 97.40, mean = 96.56
train accuracy: 99.73 (5185 / 5199), f1 (binary): 96.52, loss: 8.74e-03
time: 3s (wall 1s)
test  accuracy: 96.37 (557 / 578), f1 (binary): 40.00, loss: 9.83e-01
time: 0s (wall 0s)
 
Training model: hybrid_pooling
 
  architecture/L = 2
  architecture/N = [25, 25, 10]
CNNGS Architecture: hybrid_pooling (hybrid)
  input: M_0 = N = 25
  l_1: gsconv_1
    input dimension : M_0 = F_0 N_0 =  1 * 25 = 25
    output dimension: M_1 = F_1 N_1 = 16 * 25 = 400
    parameters_1 detail:
      parameters_(1,1): K_(1,1) F_(1,1) F_(1,0) = 8 * 8 * 1 = 64
      parameters_(1,2): K_(1,2) F_(1,2) F_(1,1) = 8 * 16 * 8 = 1024
    parameters = parameters_1 N_1 = 1088 * 25 = 27200
  l_2: gsconv_2
    input dimension : M_1 = F_1 N_1 = 16 * 25 = 400
    output dimension: M_2 = F_2 N_2 = 16 * 10 = 160
    parameters_2 detail:
      parameters_(2,1): K_(2,1) F_(2,1) F_(2,0) = 8 * 8 * 16 = 1024
      parameters_(2,2): K_(2,2) F_(2,2) F_(2,1) = 8 * 16 * 8 = 1024
    parameters = parameters_2 N_2 = 2048 * 10 = 20480
  l_3: softmax
    input dimension : M_2 = 160
    output dimension: M_3 = 2
    parameters: M_3 M_2 = 2 * 160 = 320
  Total parameters = 48000
 
[hybrid_pooling] step 1000 / 51990 (epoch 19.23 / 1000):
  learning_rate = 9.81e-04, loss_average = 1.02e-01
  validation accuracy: 94.98 (549 / 578), f1 (binary): 45.28, loss: 6.23e-01
  time: 571s (wall 48s)
[hybrid_pooling] step 2000 / 51990 (epoch 38.47 / 1000):
  learning_rate = 9.63e-04, loss_average = 4.61e-02
  validation accuracy: 96.54 (558 / 578), f1 (binary): 52.38, loss: 8.87e-01
  time: 1146s (wall 96s)
[hybrid_pooling] step 3000 / 51990 (epoch 57.70 / 1000):
  learning_rate = 9.45e-04, loss_average = 6.48e-02
  validation accuracy: 97.23 (562 / 578), f1 (binary): 50.00, loss: 1.06e+00
  time: 1719s (wall 142s)
[hybrid_pooling] step 4000 / 51990 (epoch 76.94 / 1000):
  learning_rate = 9.27e-04, loss_average = 6.17e-02
  validation accuracy: 96.54 (558 / 578), f1 (binary): 50.00, loss: 1.56e+00
  time: 2291s (wall 189s)
[hybrid_pooling] step 5000 / 51990 (epoch 96.17 / 1000):
  learning_rate = 9.08e-04, loss_average = 2.32e-02
  validation accuracy: 96.54 (558 / 578), f1 (binary): 52.38, loss: 2.15e+00
  time: 2863s (wall 235s)
[hybrid_pooling] step 6000 / 51990 (epoch 115.41 / 1000):
  learning_rate = 8.91e-04, loss_average = 1.44e-02
  validation accuracy: 96.54 (558 / 578), f1 (binary): 56.52, loss: 3.28e+00
  time: 3436s (wall 282s)
[hybrid_pooling] step 7000 / 51990 (epoch 134.64 / 1000):
  learning_rate = 8.75e-04, loss_average = 7.61e-03
  validation accuracy: 97.06 (561 / 578), f1 (binary): 58.54, loss: 3.39e+00
  time: 4010s (wall 328s)
[hybrid_pooling] step 8000 / 51990 (epoch 153.88 / 1000):
  learning_rate = 8.58e-04, loss_average = 5.27e-02
  validation accuracy: 96.19 (556 / 578), f1 (binary): 54.17, loss: 3.24e+00
  time: 4584s (wall 375s)
[hybrid_pooling] step 9000 / 51990 (epoch 173.11 / 1000):
  learning_rate = 8.41e-04, loss_average = 2.09e-04
  validation accuracy: 97.40 (563 / 578), f1 (binary): 57.14, loss: 3.53e+00
  time: 5157s (wall 421s)
[hybrid_pooling] step 10000 / 51990 (epoch 192.34 / 1000):
  learning_rate = 8.25e-04, loss_average = 8.10e-05
  validation accuracy: 97.58 (564 / 578), f1 (binary): 58.82, loss: 3.76e+00
  time: 5730s (wall 467s)
[hybrid_pooling] step 11000 / 51990 (epoch 211.58 / 1000):
  learning_rate = 8.10e-04, loss_average = 4.00e-05
  validation accuracy: 97.58 (564 / 578), f1 (binary): 58.82, loss: 3.96e+00
  time: 6304s (wall 513s)
[hybrid_pooling] step 12000 / 51990 (epoch 230.81 / 1000):
  learning_rate = 7.94e-04, loss_average = 1.84e-05
  validation accuracy: 97.58 (564 / 578), f1 (binary): 58.82, loss: 4.12e+00
  time: 6879s (wall 560s)
[hybrid_pooling] step 13000 / 51990 (epoch 250.05 / 1000):
  learning_rate = 7.79e-04, loss_average = 1.28e-05
  validation accuracy: 97.58 (564 / 578), f1 (binary): 58.82, loss: 4.28e+00
  time: 7453s (wall 606s)
[hybrid_pooling] step 14000 / 51990 (epoch 269.28 / 1000):
  learning_rate = 7.64e-04, loss_average = 5.95e-06
  validation accuracy: 97.58 (564 / 578), f1 (binary): 58.82, loss: 4.42e+00
  time: 8025s (wall 653s)
[hybrid_pooling] step 15000 / 51990 (epoch 288.52 / 1000):
  learning_rate = 7.50e-04, loss_average = 3.11e-06
  validation accuracy: 97.40 (563 / 578), f1 (binary): 57.14, loss: 4.57e+00
  time: 8600s (wall 699s)
[hybrid_pooling] step 16000 / 51990 (epoch 307.75 / 1000):
  learning_rate = 7.36e-04, loss_average = 2.02e-06
  validation accuracy: 97.40 (563 / 578), f1 (binary): 57.14, loss: 4.72e+00
  time: 9174s (wall 746s)
[hybrid_pooling] step 17000 / 51990 (epoch 326.99 / 1000):
  learning_rate = 7.22e-04, loss_average = 1.23e-06
  validation accuracy: 97.40 (563 / 578), f1 (binary): 59.46, loss: 4.86e+00
  time: 9748s (wall 792s)
[hybrid_pooling] step 18000 / 51990 (epoch 346.22 / 1000):
  learning_rate = 7.07e-04, loss_average = 6.06e-07
  validation accuracy: 97.40 (563 / 578), f1 (binary): 59.46, loss: 5.01e+00
  time: 10322s (wall 839s)
[hybrid_pooling] step 19000 / 51990 (epoch 365.45 / 1000):
  learning_rate = 6.94e-04, loss_average = 3.28e-07
  validation accuracy: 97.40 (563 / 578), f1 (binary): 59.46, loss: 5.17e+00
  time: 10896s (wall 885s)
[hybrid_pooling] step 20000 / 51990 (epoch 384.69 / 1000):
  learning_rate = 6.81e-04, loss_average = 1.94e-07
  validation accuracy: 97.40 (563 / 578), f1 (binary): 59.46, loss: 5.34e+00
  time: 11468s (wall 931s)
[hybrid_pooling] step 21000 / 51990 (epoch 403.92 / 1000):
  learning_rate = 6.68e-04, loss_average = 1.18e-07
  validation accuracy: 97.40 (563 / 578), f1 (binary): 59.46, loss: 5.49e+00
  time: 12040s (wall 978s)
[hybrid_pooling] step 22000 / 51990 (epoch 423.16 / 1000):
  learning_rate = 6.55e-04, loss_average = 6.23e-08
  validation accuracy: 97.40 (563 / 578), f1 (binary): 59.46, loss: 5.65e+00
  time: 12612s (wall 1024s)
[hybrid_pooling] step 23000 / 51990 (epoch 442.39 / 1000):
  learning_rate = 6.43e-04, loss_average = 3.82e-08
  validation accuracy: 97.40 (563 / 578), f1 (binary): 59.46, loss: 5.79e+00
  time: 13185s (wall 1070s)
[hybrid_pooling] step 24000 / 51990 (epoch 461.63 / 1000):
  learning_rate = 6.31e-04, loss_average = 2.17e-08
  validation accuracy: 97.40 (563 / 578), f1 (binary): 59.46, loss: 5.94e+00
  time: 13758s (wall 1117s)
[hybrid_pooling] step 25000 / 51990 (epoch 480.86 / 1000):
  learning_rate = 6.19e-04, loss_average = 1.34e-08
  validation accuracy: 97.40 (563 / 578), f1 (binary): 59.46, loss: 6.08e+00
  time: 14331s (wall 1163s)
[hybrid_pooling] step 26000 / 51990 (epoch 500.10 / 1000):
  learning_rate = 6.06e-04, loss_average = 6.95e-09
  validation accuracy: 97.40 (563 / 578), f1 (binary): 59.46, loss: 6.21e+00
  time: 14903s (wall 1209s)
[hybrid_pooling] step 27000 / 51990 (epoch 519.33 / 1000):
  learning_rate = 5.95e-04, loss_average = 4.61e-09
  validation accuracy: 97.40 (563 / 578), f1 (binary): 59.46, loss: 6.35e+00
  time: 15477s (wall 1256s)
[hybrid_pooling] step 28000 / 51990 (epoch 538.57 / 1000):
  learning_rate = 5.84e-04, loss_average = 2.45e-09
  validation accuracy: 97.40 (563 / 578), f1 (binary): 59.46, loss: 6.49e+00
  time: 16050s (wall 1302s)
[hybrid_pooling] step 29000 / 51990 (epoch 557.80 / 1000):
  learning_rate = 5.73e-04, loss_average = 1.37e-09
  validation accuracy: 97.40 (563 / 578), f1 (binary): 59.46, loss: 6.63e+00
  time: 16623s (wall 1348s)
[hybrid_pooling] step 30000 / 51990 (epoch 577.03 / 1000):
  learning_rate = 5.61e-04, loss_average = 9.93e-10
  validation accuracy: 97.40 (563 / 578), f1 (binary): 59.46, loss: 6.80e+00
  time: 17197s (wall 1395s)
[hybrid_pooling] step 31000 / 51990 (epoch 596.27 / 1000):
  learning_rate = 5.51e-04, loss_average = 6.65e-10
  validation accuracy: 97.40 (563 / 578), f1 (binary): 59.46, loss: 7.03e+00
  time: 17770s (wall 1441s)
[hybrid_pooling] step 32000 / 51990 (epoch 615.50 / 1000):
  learning_rate = 5.40e-04, loss_average = 2.87e-02
  validation accuracy: 96.19 (556 / 578), f1 (binary): 45.00, loss: 6.03e+00
  time: 18343s (wall 1487s)
[hybrid_pooling] step 33000 / 51990 (epoch 634.74 / 1000):
  learning_rate = 5.30e-04, loss_average = 6.89e-04
  validation accuracy: 97.23 (562 / 578), f1 (binary): 60.00, loss: 6.50e+00
  time: 18916s (wall 1534s)
[hybrid_pooling] step 34000 / 51990 (epoch 653.97 / 1000):
  learning_rate = 5.20e-04, loss_average = 1.52e-04
  validation accuracy: 97.23 (562 / 578), f1 (binary): 60.00, loss: 7.07e+00
  time: 19490s (wall 1580s)
[hybrid_pooling] step 35000 / 51990 (epoch 673.21 / 1000):
  learning_rate = 5.10e-04, loss_average = 7.38e-05
  validation accuracy: 97.40 (563 / 578), f1 (binary): 61.54, loss: 7.53e+00
  time: 20062s (wall 1626s)
[hybrid_pooling] step 36000 / 51990 (epoch 692.44 / 1000):
  learning_rate = 5.00e-04, loss_average = 3.42e-05
  validation accuracy: 97.40 (563 / 578), f1 (binary): 61.54, loss: 7.91e+00
  time: 20635s (wall 1673s)
[hybrid_pooling] step 37000 / 51990 (epoch 711.68 / 1000):
  learning_rate = 4.91e-04, loss_average = 1.76e-05
  validation accuracy: 97.58 (564 / 578), f1 (binary): 63.16, loss: 8.14e+00
  time: 21208s (wall 1719s)
[hybrid_pooling] step 38000 / 51990 (epoch 730.91 / 1000):
  learning_rate = 4.82e-04, loss_average = 9.46e-06
  validation accuracy: 97.40 (563 / 578), f1 (binary): 61.54, loss: 8.30e+00
  time: 21781s (wall 1765s)
[hybrid_pooling] step 39000 / 51990 (epoch 750.14 / 1000):
  learning_rate = 4.72e-04, loss_average = 5.08e-06
  validation accuracy: 97.40 (563 / 578), f1 (binary): 61.54, loss: 8.52e+00
  time: 22355s (wall 1812s)
[hybrid_pooling] step 40000 / 51990 (epoch 769.38 / 1000):
  learning_rate = 4.63e-04, loss_average = 2.00e-06
  validation accuracy: 97.23 (562 / 578), f1 (binary): 60.00, loss: 8.75e+00
  time: 22927s (wall 1858s)
[hybrid_pooling] step 41000 / 51990 (epoch 788.61 / 1000):
  learning_rate = 4.55e-04, loss_average = 8.10e-07
  validation accuracy: 97.40 (563 / 578), f1 (binary): 61.54, loss: 9.00e+00
  time: 23500s (wall 1905s)
[hybrid_pooling] step 42000 / 51990 (epoch 807.85 / 1000):
  learning_rate = 4.46e-04, loss_average = 9.00e-07
  validation accuracy: 97.23 (562 / 578), f1 (binary): 60.00, loss: 9.26e+00
  time: 24073s (wall 1951s)
[hybrid_pooling] step 43000 / 51990 (epoch 827.08 / 1000):
  learning_rate = 4.37e-04, loss_average = 4.67e-07
  validation accuracy: 97.23 (562 / 578), f1 (binary): 60.00, loss: 9.51e+00
  time: 24646s (wall 1997s)
[hybrid_pooling] step 44000 / 51990 (epoch 846.32 / 1000):
  learning_rate = 4.29e-04, loss_average = 2.03e-07
  validation accuracy: 97.23 (562 / 578), f1 (binary): 60.00, loss: 9.79e+00
  time: 25219s (wall 2044s)
[hybrid_pooling] step 45000 / 51990 (epoch 865.55 / 1000):
  learning_rate = 4.21e-04, loss_average = 1.22e-07
  validation accuracy: 97.23 (562 / 578), f1 (binary): 60.00, loss: 1.00e+01
  time: 25791s (wall 2090s)
[hybrid_pooling] step 46000 / 51990 (epoch 884.79 / 1000):
  learning_rate = 4.13e-04, loss_average = 8.21e-08
  validation accuracy: 97.40 (563 / 578), f1 (binary): 61.54, loss: 1.03e+01
  time: 26363s (wall 2136s)
[hybrid_pooling] step 47000 / 51990 (epoch 904.02 / 1000):
  learning_rate = 4.05e-04, loss_average = 5.57e-08
  validation accuracy: 97.23 (562 / 578), f1 (binary): 60.00, loss: 1.05e+01
  time: 26935s (wall 2182s)
[hybrid_pooling] step 48000 / 51990 (epoch 923.25 / 1000):
  learning_rate = 3.97e-04, loss_average = 2.59e-08
  validation accuracy: 97.23 (562 / 578), f1 (binary): 60.00, loss: 1.08e+01
  time: 27508s (wall 2229s)
[hybrid_pooling] step 49000 / 51990 (epoch 942.49 / 1000):
  learning_rate = 3.90e-04, loss_average = 1.31e-08
  validation accuracy: 97.23 (562 / 578), f1 (binary): 60.00, loss: 1.10e+01
  time: 28079s (wall 2275s)
[hybrid_pooling] step 50000 / 51990 (epoch 961.72 / 1000):
  learning_rate = 3.82e-04, loss_average = 9.46e-09
  validation accuracy: 97.23 (562 / 578), f1 (binary): 60.00, loss: 1.12e+01
  time: 28652s (wall 2321s)
[hybrid_pooling] step 51000 / 51990 (epoch 980.96 / 1000):
  learning_rate = 3.75e-04, loss_average = 4.99e-09
  validation accuracy: 97.23 (562 / 578), f1 (binary): 60.00, loss: 1.13e+01
  time: 29225s (wall 2367s)
[hybrid_pooling] step 51990 / 51990 (epoch 1000.00 / 1000):
  learning_rate = 3.68e-04, loss_average = 2.92e-09
  validation accuracy: 97.06 (561 / 578), f1 (binary): 56.41, loss: 1.15e+01
  time: 29792s (wall 2413s)
validation accuracy: peak = 97.58, mean = 97.23
train accuracy: 100.00 (5199 / 5199), f1 (binary): 100.00, loss: 1.53e-04
time: 14s (wall 2s)
test  accuracy: 97.06 (561 / 578), f1 (binary): 56.41, loss: 1.15e+01
time: 2s (wall 1s)
 
Showing results...
 
    {n = 25, norm-Laplacian, num_epochs = 1000, batch_size = 100, 
     reg = 0, dropout = 0, momentum = 0
     ADAM, learning_rate = 0.001}
 
Region: NYC
    aggregation_pooling = {F = [16, 16], K = [16, 16], M = [2]}
    c_cheb_a = {F = [14, 28], K = [7, 14], M = [2]}
    hybrid_pooling = {F = [[8, 16], [8, 16]], K = [[8, 8], [8, 8]], M = [2]}
    np_3 = {F = [14, 28], K = [7, 14], M = [2]}
    selection_pooling = {F = [16, 16], K = [16, 16], M = [2]}
 
    Results:
      accuracy        F1        parameters    time [ms]  name
    test  train   test  train   
    96.37 99.73   40.00 96.52      4544         14       aggregation_pooling
    95.85 97.71   42.86 70.76      6034         27       c_cheb_a
    97.06 100.00   56.41 100.00     48000         46       hybrid_pooling
    98.27 99.81   76.19 97.46      6986         23       np_3
    96.37 96.21    0.00  0.00      4832         39       selection_pooling
 
 
Clustering graph sizes:
S_c[0]: 32
S_c[1]: 16
S_c[2]: 8
