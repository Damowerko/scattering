/glob/intel-python/python3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
/glob/intel-python/python3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  "This module will be removed in 0.20.", DeprecationWarning)
/glob/intel-python/python3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.
  DeprecationWarning)
Setting up problem parameters... DONE
Gathering data... Numbmer of datapoints: 5777
Building graph support... DONE
Running Neural Networks: BEGINNING
 
Training model: c_cheb_a
 
  architecture/L = 2
  architecture/N = [28, 14, 7]
CNNGS Architecture: c_cheb_a (clustering)
  input: M_0 = N = 28
  l_1: gsconv_1
    input dimension : M_0 = F_0 N_0 =  1 * 28 = 28
    output dimension: M_1 = F_1 N_1 = 14 * 14 = 196
    parameters: K_1 F_1 F_0 = 7 * 14 * 1 = 98
  l_2: gsconv_2
    input dimension : M_1 = F_1 N_1 = 14 * 14 = 196
    output dimension: M_2 = F_2 N_2 = 28 *  7 = 196
    parameters: K_2 F_2 F_1 = 14 * 28 * 14 = 5488
  l_3: softmax
    input dimension : M_2 = 196
    output dimension: M_3 = 2
    parameters: M_3 M_2 = 2 * 196 = 392
  Total parameters = 5978
 
2018-06-29 00:28:11.114823: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
[c_cheb_a] step 500 / 25995 (epoch 9.62 / 500):
  learning_rate = 9.91e-04, loss_average = 6.02e+04
  validation accuracy: 88.24 (510 / 578), f1 (binary): 22.73, loss: 3.71e+04
  time: 89s (wall 12s)
[c_cheb_a] step 1000 / 25995 (epoch 19.23 / 500):
  learning_rate = 9.81e-04, loss_average = 5.44e+04
/glob/intel-python/python3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 4.62e+04
  time: 179s (wall 25s)
[c_cheb_a] step 1500 / 25995 (epoch 28.85 / 500):
  learning_rate = 9.72e-04, loss_average = 1.88e+04
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 2.71e+04
  time: 270s (wall 39s)
[c_cheb_a] step 2000 / 25995 (epoch 38.47 / 500):
  learning_rate = 9.63e-04, loss_average = 2.37e+04
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 5.35e+04
  time: 360s (wall 51s)
[c_cheb_a] step 2500 / 25995 (epoch 48.09 / 500):
  learning_rate = 9.53e-04, loss_average = 6.26e+03
  validation accuracy: 95.50 (552 / 578), f1 (binary): 27.78, loss: 5.66e+03
  time: 449s (wall 63s)
[c_cheb_a] step 3000 / 25995 (epoch 57.70 / 500):
  learning_rate = 9.45e-04, loss_average = 3.05e+04
  validation accuracy: 95.33 (551 / 578), f1 (binary): 6.90, loss: 1.34e+04
  time: 539s (wall 75s)
[c_cheb_a] step 3500 / 25995 (epoch 67.32 / 500):
  learning_rate = 9.35e-04, loss_average = 7.51e+03
  validation accuracy: 95.85 (554 / 578), f1 (binary): 40.00, loss: 3.04e+03
  time: 630s (wall 87s)
[c_cheb_a] step 4000 / 25995 (epoch 76.94 / 500):
  learning_rate = 9.27e-04, loss_average = 6.55e+03
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.66e+04
  time: 720s (wall 100s)
[c_cheb_a] step 4500 / 25995 (epoch 86.56 / 500):
  learning_rate = 9.18e-04, loss_average = 2.82e+03
  validation accuracy: 97.06 (561 / 578), f1 (binary): 51.43, loss: 2.49e+03
  time: 809s (wall 112s)
[c_cheb_a] step 5000 / 25995 (epoch 96.17 / 500):
  learning_rate = 9.08e-04, loss_average = 2.50e+03
  validation accuracy: 95.67 (553 / 578), f1 (binary): 52.83, loss: 2.60e+03
  time: 899s (wall 124s)
[c_cheb_a] step 5500 / 25995 (epoch 105.79 / 500):
  learning_rate = 9.00e-04, loss_average = 3.02e+03
  validation accuracy: 95.16 (550 / 578), f1 (binary): 53.33, loss: 2.93e+03
  time: 989s (wall 136s)
[c_cheb_a] step 6000 / 25995 (epoch 115.41 / 500):
  learning_rate = 8.91e-04, loss_average = 2.29e+03
  validation accuracy: 92.56 (535 / 578), f1 (binary): 44.16, loss: 2.96e+03
  time: 1079s (wall 148s)
[c_cheb_a] step 6500 / 25995 (epoch 125.02 / 500):
  learning_rate = 8.82e-04, loss_average = 4.01e+03
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 6.92e+03
  time: 1169s (wall 161s)
[c_cheb_a] step 7000 / 25995 (epoch 134.64 / 500):
  learning_rate = 8.75e-04, loss_average = 1.79e+03
  validation accuracy: 96.37 (557 / 578), f1 (binary): 16.00, loss: 2.40e+03
  time: 1259s (wall 173s)
[c_cheb_a] step 7500 / 25995 (epoch 144.26 / 500):
  learning_rate = 8.66e-04, loss_average = 1.83e+03
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 2.13e+03
  time: 1349s (wall 185s)
[c_cheb_a] step 8000 / 25995 (epoch 153.88 / 500):
  learning_rate = 8.58e-04, loss_average = 2.78e+03
  validation accuracy: 95.85 (554 / 578), f1 (binary): 0.00, loss: 6.28e+03
  time: 1438s (wall 197s)
[c_cheb_a] step 8500 / 25995 (epoch 163.49 / 500):
  learning_rate = 8.50e-04, loss_average = 1.91e+03
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 2.81e+03
  time: 1529s (wall 210s)
[c_cheb_a] step 9000 / 25995 (epoch 173.11 / 500):
  learning_rate = 8.41e-04, loss_average = 1.43e+03
  validation accuracy: 96.02 (555 / 578), f1 (binary): 20.69, loss: 1.35e+03
  time: 1618s (wall 222s)
[c_cheb_a] step 9500 / 25995 (epoch 182.73 / 500):
  learning_rate = 8.34e-04, loss_average = 1.02e+03
  validation accuracy: 96.19 (556 / 578), f1 (binary): 15.38, loss: 9.69e+02
  time: 1708s (wall 234s)
[c_cheb_a] step 10000 / 25995 (epoch 192.34 / 500):
  learning_rate = 8.25e-04, loss_average = 7.28e+02
  validation accuracy: 96.89 (560 / 578), f1 (binary): 47.06, loss: 5.59e+02
  time: 1799s (wall 246s)
[c_cheb_a] step 10500 / 25995 (epoch 201.96 / 500):
  learning_rate = 8.18e-04, loss_average = 2.78e+02
  validation accuracy: 95.50 (552 / 578), f1 (binary): 18.75, loss: 4.03e+02
  time: 1889s (wall 259s)
[c_cheb_a] step 11000 / 25995 (epoch 211.58 / 500):
  learning_rate = 8.10e-04, loss_average = 5.52e+02
  validation accuracy: 95.85 (554 / 578), f1 (binary): 33.33, loss: 7.03e+02
  time: 1979s (wall 271s)
[c_cheb_a] step 11500 / 25995 (epoch 221.20 / 500):
  learning_rate = 8.02e-04, loss_average = 2.44e+02
  validation accuracy: 90.83 (525 / 578), f1 (binary): 44.21, loss: 4.69e+02
  time: 2069s (wall 283s)
[c_cheb_a] step 12000 / 25995 (epoch 230.81 / 500):
  learning_rate = 7.94e-04, loss_average = 8.18e+02
  validation accuracy: 69.55 (402 / 578), f1 (binary): 20.72, loss: 2.90e+03
  time: 2160s (wall 295s)
[c_cheb_a] step 12500 / 25995 (epoch 240.43 / 500):
  learning_rate = 7.87e-04, loss_average = 3.43e+02
  validation accuracy: 95.67 (553 / 578), f1 (binary): 0.00, loss: 7.26e+02
  time: 2250s (wall 308s)
[c_cheb_a] step 13000 / 25995 (epoch 250.05 / 500):
  learning_rate = 7.79e-04, loss_average = 5.52e+01
  validation accuracy: 95.33 (551 / 578), f1 (binary): 0.00, loss: 5.46e+01
  time: 2341s (wall 320s)
[c_cheb_a] step 13500 / 25995 (epoch 259.67 / 500):
  learning_rate = 7.72e-04, loss_average = 1.94e+01
  validation accuracy: 91.87 (531 / 578), f1 (binary): 33.80, loss: 1.87e+01
  time: 2431s (wall 332s)
[c_cheb_a] step 14000 / 25995 (epoch 269.28 / 500):
  learning_rate = 7.64e-04, loss_average = 1.09e+01
  validation accuracy: 80.10 (463 / 578), f1 (binary): 22.82, loss: 3.17e+01
  time: 2522s (wall 344s)
[c_cheb_a] step 14500 / 25995 (epoch 278.90 / 500):
  learning_rate = 7.57e-04, loss_average = 1.98e+01
  validation accuracy: 95.50 (552 / 578), f1 (binary): 0.00, loss: 1.84e+01
  time: 2613s (wall 357s)
[c_cheb_a] step 15000 / 25995 (epoch 288.52 / 500):
  learning_rate = 7.50e-04, loss_average = 3.16e+01
  validation accuracy: 85.64 (495 / 578), f1 (binary): 32.52, loss: 4.43e+01
  time: 2704s (wall 369s)
[c_cheb_a] step 15500 / 25995 (epoch 298.13 / 500):
  learning_rate = 7.42e-04, loss_average = 2.27e+01
  validation accuracy: 94.81 (548 / 578), f1 (binary): 0.00, loss: 1.98e+01
  time: 2794s (wall 381s)
[c_cheb_a] step 16000 / 25995 (epoch 307.75 / 500):
  learning_rate = 7.36e-04, loss_average = 4.40e+01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 4.54e+01
  time: 2884s (wall 393s)
[c_cheb_a] step 16500 / 25995 (epoch 317.37 / 500):
  learning_rate = 7.28e-04, loss_average = 1.33e+01
  validation accuracy: 93.25 (539 / 578), f1 (binary): 17.02, loss: 1.17e+01
  time: 2975s (wall 406s)
[c_cheb_a] step 17000 / 25995 (epoch 326.99 / 500):
  learning_rate = 7.22e-04, loss_average = 2.14e+01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 4.52e+01
  time: 3065s (wall 418s)
[c_cheb_a] step 17500 / 25995 (epoch 336.60 / 500):
  learning_rate = 7.15e-04, loss_average = 1.26e+01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 3.17e+01
  time: 3155s (wall 430s)
[c_cheb_a] step 18000 / 25995 (epoch 346.22 / 500):
  learning_rate = 7.07e-04, loss_average = 3.50e+01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 3.58e+01
  time: 3246s (wall 442s)
[c_cheb_a] step 18500 / 25995 (epoch 355.84 / 500):
  learning_rate = 7.01e-04, loss_average = 8.29e+00
  validation accuracy: 94.64 (547 / 578), f1 (binary): 11.43, loss: 9.39e+00
  time: 3336s (wall 455s)
[c_cheb_a] step 19000 / 25995 (epoch 365.45 / 500):
  learning_rate = 6.94e-04, loss_average = 2.06e+01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 2.36e+01
  time: 3427s (wall 467s)
[c_cheb_a] step 19500 / 25995 (epoch 375.07 / 500):
  learning_rate = 6.87e-04, loss_average = 8.18e+00
  validation accuracy: 94.98 (549 / 578), f1 (binary): 17.14, loss: 4.62e+00
  time: 3518s (wall 479s)
[c_cheb_a] step 20000 / 25995 (epoch 384.69 / 500):
  learning_rate = 6.81e-04, loss_average = 1.43e+01
  validation accuracy: 95.16 (550 / 578), f1 (binary): 26.32, loss: 5.98e+00
  time: 3609s (wall 491s)
[c_cheb_a] step 20500 / 25995 (epoch 394.31 / 500):
  learning_rate = 6.74e-04, loss_average = 1.17e+01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.98e+01
  time: 3699s (wall 504s)
[c_cheb_a] step 21000 / 25995 (epoch 403.92 / 500):
  learning_rate = 6.68e-04, loss_average = 9.71e+00
  validation accuracy: 82.53 (477 / 578), f1 (binary): 27.34, loss: 1.12e+01
  time: 3790s (wall 516s)
[c_cheb_a] step 21500 / 25995 (epoch 413.54 / 500):
  learning_rate = 6.62e-04, loss_average = 1.18e+01
  validation accuracy: 95.50 (552 / 578), f1 (binary): 13.33, loss: 6.22e+00
  time: 3880s (wall 528s)
[c_cheb_a] step 22000 / 25995 (epoch 423.16 / 500):
  learning_rate = 6.55e-04, loss_average = 5.71e+00
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 6.52e+00
  time: 3971s (wall 540s)
[c_cheb_a] step 22500 / 25995 (epoch 432.78 / 500):
  learning_rate = 6.49e-04, loss_average = 7.61e+00
  validation accuracy: 78.37 (453 / 578), f1 (binary): 26.90, loss: 1.58e+01
  time: 4061s (wall 553s)
[c_cheb_a] step 23000 / 25995 (epoch 442.39 / 500):
  learning_rate = 6.43e-04, loss_average = 5.33e+00
  validation accuracy: 92.56 (535 / 578), f1 (binary): 29.51, loss: 1.76e+00
  time: 4152s (wall 565s)
[c_cheb_a] step 23500 / 25995 (epoch 452.01 / 500):
  learning_rate = 6.37e-04, loss_average = 3.70e+00
  validation accuracy: 94.29 (545 / 578), f1 (binary): 15.38, loss: 2.42e+00
  time: 4243s (wall 577s)
[c_cheb_a] step 24000 / 25995 (epoch 461.63 / 500):
  learning_rate = 6.31e-04, loss_average = 4.80e+00
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 6.05e+00
  time: 4334s (wall 590s)
[c_cheb_a] step 24500 / 25995 (epoch 471.24 / 500):
  learning_rate = 6.24e-04, loss_average = 1.32e+01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 2.13e+01
  time: 4425s (wall 602s)
[c_cheb_a] step 25000 / 25995 (epoch 480.86 / 500):
  learning_rate = 6.19e-04, loss_average = 3.46e+02
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 8.13e+00
  time: 4516s (wall 614s)
[c_cheb_a] step 25500 / 25995 (epoch 490.48 / 500):
  learning_rate = 6.12e-04, loss_average = 5.79e+00
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 6.34e+00
  time: 4606s (wall 626s)
[c_cheb_a] step 25995 / 25995 (epoch 500.00 / 500):
  learning_rate = 6.07e-04, loss_average = 1.08e+01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.32e+01
  time: 4696s (wall 639s)
validation accuracy: peak = 97.06, mean = 93.69
train accuracy: 96.29 (5006 / 5199), f1 (binary): 2.03, loss: 1.14e+01
time: 6s (wall 1s)
test  accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.32e+01
time: 1s (wall 0s)
 
Training model: np_3
 
  architecture/L = 2
  architecture/N = [25, 25, 25]
CNNGS Architecture: np_3 (no-pooling)
  input: M_0 = N = 25
  l_1: gsconv_1
    input dimension : M_0 = F_0 N_0 =  1 * 25 = 25
    output dimension: M_1 = F_1 N_1 = 14 * 25 = 350
    parameters: K_1 F_1 F_0 = 7 * 14 * 1 = 98
  l_2: gsconv_2
    input dimension : M_1 = F_1 N_1 = 14 * 25 = 350
    output dimension: M_2 = F_2 N_2 = 28 * 25 = 700
    parameters: K_2 F_2 F_1 = 14 * 28 * 14 = 5488
  l_3: softmax
    input dimension : M_2 = 700
    output dimension: M_3 = 2
    parameters: M_3 M_2 = 2 * 700 = 1400
  Total parameters = 6986
 
[np_3] step 500 / 25995 (epoch 9.62 / 500):
  learning_rate = 9.91e-04, loss_average = 2.80e+02
  validation accuracy: 96.02 (555 / 578), f1 (binary): 14.81, loss: 2.87e+02
  time: 153s (wall 12s)
[np_3] step 1000 / 25995 (epoch 19.23 / 500):
  learning_rate = 9.81e-04, loss_average = 1.01e+02
  validation accuracy: 95.85 (554 / 578), f1 (binary): 47.83, loss: 9.89e+01
  time: 305s (wall 24s)
[np_3] step 1500 / 25995 (epoch 28.85 / 500):
  learning_rate = 9.72e-04, loss_average = 1.45e+02
  validation accuracy: 97.06 (561 / 578), f1 (binary): 51.43, loss: 9.66e+01
  time: 457s (wall 36s)
[np_3] step 2000 / 25995 (epoch 38.47 / 500):
  learning_rate = 9.63e-04, loss_average = 1.31e+02
  validation accuracy: 95.85 (554 / 578), f1 (binary): 14.29, loss: 1.08e+02
  time: 609s (wall 47s)
[np_3] step 2500 / 25995 (epoch 48.09 / 500):
  learning_rate = 9.53e-04, loss_average = 6.04e+01
  validation accuracy: 96.89 (560 / 578), f1 (binary): 47.06, loss: 6.72e+01
  time: 762s (wall 59s)
[np_3] step 3000 / 25995 (epoch 57.70 / 500):
  learning_rate = 9.45e-04, loss_average = 5.50e+01
  validation accuracy: 94.46 (546 / 578), f1 (binary): 55.56, loss: 6.06e+01
  time: 915s (wall 71s)
[np_3] step 3500 / 25995 (epoch 67.32 / 500):
  learning_rate = 9.35e-04, loss_average = 2.44e+01
  validation accuracy: 97.06 (561 / 578), f1 (binary): 45.16, loss: 4.17e+01
  time: 1068s (wall 83s)
[np_3] step 4000 / 25995 (epoch 76.94 / 500):
  learning_rate = 9.27e-04, loss_average = 2.46e+01
  validation accuracy: 95.33 (551 / 578), f1 (binary): 59.70, loss: 2.82e+01
  time: 1222s (wall 94s)
[np_3] step 4500 / 25995 (epoch 86.56 / 500):
  learning_rate = 9.18e-04, loss_average = 1.70e+01
  validation accuracy: 93.94 (543 / 578), f1 (binary): 54.55, loss: 3.20e+01
  time: 1376s (wall 106s)
[np_3] step 5000 / 25995 (epoch 96.17 / 500):
  learning_rate = 9.08e-04, loss_average = 1.67e+01
  validation accuracy: 95.33 (551 / 578), f1 (binary): 55.74, loss: 1.79e+01
  time: 1529s (wall 118s)
[np_3] step 5500 / 25995 (epoch 105.79 / 500):
  learning_rate = 9.00e-04, loss_average = 1.16e+01
  validation accuracy: 96.89 (560 / 578), f1 (binary): 47.06, loss: 1.89e+01
  time: 1683s (wall 130s)
[np_3] step 6000 / 25995 (epoch 115.41 / 500):
  learning_rate = 8.91e-04, loss_average = 7.71e+00
  validation accuracy: 93.60 (541 / 578), f1 (binary): 43.08, loss: 2.45e+01
  time: 1836s (wall 142s)
[np_3] step 6500 / 25995 (epoch 125.02 / 500):
  learning_rate = 8.82e-04, loss_average = 5.03e+00
  validation accuracy: 97.75 (565 / 578), f1 (binary): 66.67, loss: 2.89e+00
  time: 1989s (wall 153s)
[np_3] step 7000 / 25995 (epoch 134.64 / 500):
  learning_rate = 8.75e-04, loss_average = 1.28e+01
  validation accuracy: 97.23 (562 / 578), f1 (binary): 63.64, loss: 5.64e+00
  time: 2142s (wall 165s)
[np_3] step 7500 / 25995 (epoch 144.26 / 500):
  learning_rate = 8.66e-04, loss_average = 6.56e+00
  validation accuracy: 97.40 (563 / 578), f1 (binary): 65.12, loss: 5.44e+00
  time: 2295s (wall 177s)
[np_3] step 8000 / 25995 (epoch 153.88 / 500):
  learning_rate = 8.58e-04, loss_average = 8.16e+00
  validation accuracy: 97.40 (563 / 578), f1 (binary): 51.61, loss: 4.17e+00
  time: 2447s (wall 188s)
[np_3] step 8500 / 25995 (epoch 163.49 / 500):
  learning_rate = 8.50e-04, loss_average = 1.34e+00
  validation accuracy: 96.19 (556 / 578), f1 (binary): 21.43, loss: 2.71e+00
  time: 2599s (wall 200s)
[np_3] step 9000 / 25995 (epoch 173.11 / 500):
  learning_rate = 8.41e-04, loss_average = 9.20e-01
  validation accuracy: 96.89 (560 / 578), f1 (binary): 40.00, loss: 2.07e+00
  time: 2751s (wall 212s)
[np_3] step 9500 / 25995 (epoch 182.73 / 500):
  learning_rate = 8.34e-04, loss_average = 1.61e+00
  validation accuracy: 96.19 (556 / 578), f1 (binary): 35.29, loss: 2.79e+00
  time: 2903s (wall 224s)
[np_3] step 10000 / 25995 (epoch 192.34 / 500):
  learning_rate = 8.25e-04, loss_average = 6.20e+00
  validation accuracy: 95.85 (554 / 578), f1 (binary): 0.00, loss: 1.50e+01
  time: 3054s (wall 235s)
[np_3] step 10500 / 25995 (epoch 201.96 / 500):
  learning_rate = 8.18e-04, loss_average = 2.52e+00
  validation accuracy: 95.67 (553 / 578), f1 (binary): 48.98, loss: 1.61e+00
  time: 3207s (wall 247s)
[np_3] step 11000 / 25995 (epoch 211.58 / 500):
  learning_rate = 8.10e-04, loss_average = 1.37e+00
  validation accuracy: 94.81 (548 / 578), f1 (binary): 50.00, loss: 1.85e+00
  time: 3359s (wall 259s)
[np_3] step 11500 / 25995 (epoch 221.20 / 500):
  learning_rate = 8.02e-04, loss_average = 7.44e-01
  validation accuracy: 95.85 (554 / 578), f1 (binary): 0.00, loss: 3.35e+00
  time: 3511s (wall 270s)
[np_3] step 12000 / 25995 (epoch 230.81 / 500):
  learning_rate = 7.94e-04, loss_average = 1.45e+00
  validation accuracy: 96.89 (560 / 578), f1 (binary): 50.00, loss: 1.52e+00
  time: 3664s (wall 282s)
[np_3] step 12500 / 25995 (epoch 240.43 / 500):
  learning_rate = 7.87e-04, loss_average = 1.96e+00
  validation accuracy: 92.91 (537 / 578), f1 (binary): 49.38, loss: 1.36e+00
  time: 3817s (wall 294s)
[np_3] step 13000 / 25995 (epoch 250.05 / 500):
  learning_rate = 7.79e-04, loss_average = 1.18e+00
  validation accuracy: 94.29 (545 / 578), f1 (binary): 49.23, loss: 7.82e-01
  time: 3969s (wall 306s)
[np_3] step 13500 / 25995 (epoch 259.67 / 500):
  learning_rate = 7.72e-04, loss_average = 2.00e+01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.68e+01
  time: 4122s (wall 317s)
[np_3] step 14000 / 25995 (epoch 269.28 / 500):
  learning_rate = 7.64e-04, loss_average = 1.43e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.65e-01
  time: 4274s (wall 329s)
[np_3] step 14500 / 25995 (epoch 278.90 / 500):
  learning_rate = 7.57e-04, loss_average = 1.33e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.46e-01
  time: 4426s (wall 341s)
[np_3] step 15000 / 25995 (epoch 288.52 / 500):
  learning_rate = 7.50e-04, loss_average = 9.64e-02
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.30e-01
  time: 4578s (wall 353s)
[np_3] step 15500 / 25995 (epoch 298.13 / 500):
  learning_rate = 7.42e-04, loss_average = 1.18e-01
  validation accuracy: 96.19 (556 / 578), f1 (binary): 8.33, loss: 1.17e-01
  time: 4731s (wall 364s)
[np_3] step 16000 / 25995 (epoch 307.75 / 500):
  learning_rate = 7.36e-04, loss_average = 1.14e-01
  validation accuracy: 96.19 (556 / 578), f1 (binary): 8.33, loss: 1.20e-01
  time: 4884s (wall 376s)
[np_3] step 16500 / 25995 (epoch 317.37 / 500):
  learning_rate = 7.28e-04, loss_average = 1.39e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.39e-01
  time: 5036s (wall 388s)
[np_3] step 17000 / 25995 (epoch 326.99 / 500):
  learning_rate = 7.22e-04, loss_average = 1.28e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.60e-01
  time: 5188s (wall 400s)
[np_3] step 17500 / 25995 (epoch 336.60 / 500):
  learning_rate = 7.15e-04, loss_average = 1.23e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.42e-01
  time: 5341s (wall 411s)
[np_3] step 18000 / 25995 (epoch 346.22 / 500):
  learning_rate = 7.07e-04, loss_average = 1.42e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.44e-01
  time: 5492s (wall 423s)
[np_3] step 18500 / 25995 (epoch 355.84 / 500):
  learning_rate = 7.01e-04, loss_average = 1.10e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.39e-01
  time: 5644s (wall 435s)
[np_3] step 19000 / 25995 (epoch 365.45 / 500):
  learning_rate = 6.94e-04, loss_average = 1.23e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.34e-01
  time: 5796s (wall 447s)
[np_3] step 19500 / 25995 (epoch 375.07 / 500):
  learning_rate = 6.87e-04, loss_average = 1.22e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.38e-01
  time: 5947s (wall 458s)
[np_3] step 20000 / 25995 (epoch 384.69 / 500):
  learning_rate = 6.81e-04, loss_average = 1.29e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.37e-01
  time: 6099s (wall 470s)
[np_3] step 20500 / 25995 (epoch 394.31 / 500):
  learning_rate = 6.74e-04, loss_average = 1.42e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.65e-01
  time: 6250s (wall 482s)
[np_3] step 21000 / 25995 (epoch 403.92 / 500):
  learning_rate = 6.68e-04, loss_average = 1.45e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.69e-01
  time: 6403s (wall 493s)
[np_3] step 21500 / 25995 (epoch 413.54 / 500):
  learning_rate = 6.62e-04, loss_average = 1.44e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.65e-01
  time: 6555s (wall 505s)
[np_3] step 22000 / 25995 (epoch 423.16 / 500):
  learning_rate = 6.55e-04, loss_average = 1.51e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.69e-01
  time: 6707s (wall 517s)
[np_3] step 22500 / 25995 (epoch 432.78 / 500):
  learning_rate = 6.49e-04, loss_average = 1.75e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.69e-01
  time: 6859s (wall 529s)
[np_3] step 23000 / 25995 (epoch 442.39 / 500):
  learning_rate = 6.43e-04, loss_average = 1.55e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.69e-01
  time: 7012s (wall 540s)
[np_3] step 23500 / 25995 (epoch 452.01 / 500):
  learning_rate = 6.37e-04, loss_average = 1.48e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.69e-01
  time: 7164s (wall 552s)
[np_3] step 24000 / 25995 (epoch 461.63 / 500):
  learning_rate = 6.31e-04, loss_average = 1.48e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.69e-01
  time: 7318s (wall 564s)
[np_3] step 24500 / 25995 (epoch 471.24 / 500):
  learning_rate = 6.24e-04, loss_average = 1.34e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.70e-01
  time: 7470s (wall 576s)
[np_3] step 25000 / 25995 (epoch 480.86 / 500):
  learning_rate = 6.19e-04, loss_average = 1.67e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.69e-01
  time: 7624s (wall 587s)
[np_3] step 25500 / 25995 (epoch 490.48 / 500):
  learning_rate = 6.12e-04, loss_average = 1.48e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.69e-01
  time: 7777s (wall 599s)
[np_3] step 25995 / 25995 (epoch 500.00 / 500):
  learning_rate = 6.07e-04, loss_average = 1.45e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.69e-01
  time: 7929s (wall 611s)
validation accuracy: peak = 97.75, mean = 96.02
train accuracy: 96.29 (5006 / 5199), f1 (binary): 2.03, loss: 1.59e-01
time: 10s (wall 1s)
test  accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.69e-01
time: 1s (wall 0s)
 
Training model: selection_pooling
 
  architecture/L = 2
  architecture/N = [25, 25, 15]
CNNGS Architecture: selection_pooling (selection)
  input: M_0 = N = 25
  l_1: gsconv_1
    input dimension : M_0 = F_0 N_0 =  1 * 25 = 25
    output dimension: M_1 = F_1 N_1 = 16 * 25 = 400
    parameters: K_1 F_1 F_0 = 16 * 16 * 1 = 256
  l_2: gsconv_2
    input dimension : M_1 = F_1 N_1 = 16 * 25 = 400
    output dimension: M_2 = F_2 N_2 = 16 * 15 = 240
    parameters: K_2 F_2 F_1 = 16 * 16 * 16 = 4096
  l_3: softmax
    input dimension : M_2 = 240
    output dimension: M_3 = 2
    parameters: M_3 M_2 = 2 * 240 = 480
  Total parameters = 4832
 
[selection_pooling] step 500 / 25995 (epoch 9.62 / 500):
  learning_rate = 9.91e-04, loss_average = 1.11e+03
  validation accuracy: 75.09 (434 / 578), f1 (binary): 21.74, loss: 2.07e+03
  time: 268s (wall 20s)
[selection_pooling] step 1000 / 25995 (epoch 19.23 / 500):
  learning_rate = 9.81e-04, loss_average = 6.96e+02
  validation accuracy: 92.39 (534 / 578), f1 (binary): 12.00, loss: 3.36e+02
  time: 539s (wall 40s)
[selection_pooling] step 1500 / 25995 (epoch 28.85 / 500):
  learning_rate = 9.72e-04, loss_average = 3.12e+02
  validation accuracy: 94.64 (547 / 578), f1 (binary): 24.39, loss: 1.31e+02
  time: 808s (wall 60s)
[selection_pooling] step 2000 / 25995 (epoch 38.47 / 500):
  learning_rate = 9.63e-04, loss_average = 3.97e+02
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 3.42e+02
  time: 1079s (wall 79s)
[selection_pooling] step 2500 / 25995 (epoch 48.09 / 500):
  learning_rate = 9.53e-04, loss_average = 1.57e+02
  validation accuracy: 91.52 (529 / 578), f1 (binary): 0.00, loss: 8.12e+01
  time: 1348s (wall 99s)
[selection_pooling] step 3000 / 25995 (epoch 57.70 / 500):
  learning_rate = 9.45e-04, loss_average = 5.01e+01
  validation accuracy: 95.50 (552 / 578), f1 (binary): 0.00, loss: 4.64e+01
  time: 1617s (wall 119s)
[selection_pooling] step 3500 / 25995 (epoch 67.32 / 500):
  learning_rate = 9.35e-04, loss_average = 3.37e+01
  validation accuracy: 96.37 (557 / 578), f1 (binary): 36.36, loss: 2.04e+01
  time: 1886s (wall 138s)
[selection_pooling] step 4000 / 25995 (epoch 76.94 / 500):
  learning_rate = 9.27e-04, loss_average = 2.97e+01
  validation accuracy: 82.01 (474 / 578), f1 (binary): 17.46, loss: 4.59e+01
  time: 2155s (wall 158s)
[selection_pooling] step 4500 / 25995 (epoch 86.56 / 500):
  learning_rate = 9.18e-04, loss_average = 7.27e+01
  validation accuracy: 94.81 (548 / 578), f1 (binary): 0.00, loss: 2.64e+01
  time: 2423s (wall 178s)
[selection_pooling] step 5000 / 25995 (epoch 96.17 / 500):
  learning_rate = 9.08e-04, loss_average = 1.55e+01
  validation accuracy: 94.81 (548 / 578), f1 (binary): 0.00, loss: 8.82e+00
  time: 2692s (wall 197s)
[selection_pooling] step 5500 / 25995 (epoch 105.79 / 500):
  learning_rate = 9.00e-04, loss_average = 4.53e+03
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.79e+03
  time: 2961s (wall 217s)
[selection_pooling] step 6000 / 25995 (epoch 115.41 / 500):
  learning_rate = 8.91e-04, loss_average = 1.80e+02
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 2.32e+02
  time: 3228s (wall 237s)
[selection_pooling] step 6500 / 25995 (epoch 125.02 / 500):
  learning_rate = 8.82e-04, loss_average = 1.23e+02
  validation accuracy: 95.16 (550 / 578), f1 (binary): 12.50, loss: 5.17e+01
  time: 3497s (wall 256s)
[selection_pooling] step 7000 / 25995 (epoch 134.64 / 500):
  learning_rate = 8.75e-04, loss_average = 2.05e+01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.37e+01
  time: 3764s (wall 276s)
[selection_pooling] step 7500 / 25995 (epoch 144.26 / 500):
  learning_rate = 8.66e-04, loss_average = 4.74e+01
  validation accuracy: 87.89 (508 / 578), f1 (binary): 18.60, loss: 2.39e+01
  time: 4032s (wall 296s)
[selection_pooling] step 8000 / 25995 (epoch 153.88 / 500):
  learning_rate = 8.58e-04, loss_average = 1.21e+01
  validation accuracy: 85.81 (496 / 578), f1 (binary): 8.89, loss: 1.18e+01
  time: 4299s (wall 315s)
[selection_pooling] step 8500 / 25995 (epoch 163.49 / 500):
  learning_rate = 8.50e-04, loss_average = 1.74e+02
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.12e+02
  time: 4565s (wall 335s)
[selection_pooling] step 9000 / 25995 (epoch 173.11 / 500):
  learning_rate = 8.41e-04, loss_average = 9.81e+00
  validation accuracy: 89.79 (519 / 578), f1 (binary): 19.18, loss: 1.06e+01
  time: 4832s (wall 354s)
[selection_pooling] step 9500 / 25995 (epoch 182.73 / 500):
  learning_rate = 8.34e-04, loss_average = 1.49e+01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 2.34e+01
  time: 5099s (wall 374s)
[selection_pooling] step 10000 / 25995 (epoch 192.34 / 500):
  learning_rate = 8.25e-04, loss_average = 2.96e+00
  validation accuracy: 92.56 (535 / 578), f1 (binary): 24.56, loss: 2.38e+00
  time: 5364s (wall 394s)
[selection_pooling] step 10500 / 25995 (epoch 201.96 / 500):
  learning_rate = 8.18e-04, loss_average = 1.63e+00
  validation accuracy: 94.81 (548 / 578), f1 (binary): 16.67, loss: 1.04e+00
  time: 5632s (wall 413s)
[selection_pooling] step 11000 / 25995 (epoch 211.58 / 500):
  learning_rate = 8.10e-04, loss_average = 6.23e+00
  validation accuracy: 95.85 (554 / 578), f1 (binary): 0.00, loss: 8.15e+00
  time: 5899s (wall 433s)
[selection_pooling] step 11500 / 25995 (epoch 221.20 / 500):
  learning_rate = 8.02e-04, loss_average = 1.04e+00
  validation accuracy: 95.50 (552 / 578), f1 (binary): 0.00, loss: 1.17e+00
  time: 6166s (wall 452s)
[selection_pooling] step 12000 / 25995 (epoch 230.81 / 500):
  learning_rate = 7.94e-04, loss_average = 1.06e+00
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 2.73e+00
  time: 6432s (wall 472s)
[selection_pooling] step 12500 / 25995 (epoch 240.43 / 500):
  learning_rate = 7.87e-04, loss_average = 4.66e+01
  validation accuracy: 95.67 (553 / 578), f1 (binary): 0.00, loss: 4.51e+01
  time: 6698s (wall 492s)
[selection_pooling] step 13000 / 25995 (epoch 250.05 / 500):
  learning_rate = 7.79e-04, loss_average = 7.77e+00
  validation accuracy: 92.04 (532 / 578), f1 (binary): 0.00, loss: 2.76e+00
  time: 6964s (wall 511s)
[selection_pooling] step 13500 / 25995 (epoch 259.67 / 500):
  learning_rate = 7.72e-04, loss_average = 5.58e-01
  validation accuracy: 95.33 (551 / 578), f1 (binary): 0.00, loss: 1.50e+00
  time: 7231s (wall 531s)
[selection_pooling] step 14000 / 25995 (epoch 269.28 / 500):
  learning_rate = 7.64e-04, loss_average = 1.44e-01
  validation accuracy: 95.50 (552 / 578), f1 (binary): 0.00, loss: 1.77e-01
  time: 7498s (wall 550s)
[selection_pooling] step 14500 / 25995 (epoch 278.90 / 500):
  learning_rate = 7.57e-04, loss_average = 1.40e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.63e-01
  time: 7766s (wall 570s)
[selection_pooling] step 15000 / 25995 (epoch 288.52 / 500):
  learning_rate = 7.50e-04, loss_average = 1.46e-01
  validation accuracy: 95.85 (554 / 578), f1 (binary): 0.00, loss: 1.67e-01
  time: 8031s (wall 590s)
[selection_pooling] step 15500 / 25995 (epoch 298.13 / 500):
  learning_rate = 7.42e-04, loss_average = 1.31e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.60e-01
  time: 8299s (wall 609s)
[selection_pooling] step 16000 / 25995 (epoch 307.75 / 500):
  learning_rate = 7.36e-04, loss_average = 1.21e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.55e-01
  time: 8564s (wall 629s)
[selection_pooling] step 16500 / 25995 (epoch 317.37 / 500):
  learning_rate = 7.28e-04, loss_average = 4.34e-01
  validation accuracy: 95.67 (553 / 578), f1 (binary): 0.00, loss: 2.23e-01
  time: 8830s (wall 648s)
[selection_pooling] step 17000 / 25995 (epoch 326.99 / 500):
  learning_rate = 7.22e-04, loss_average = 1.31e+02
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.47e+01
  time: 9096s (wall 668s)
[selection_pooling] step 17500 / 25995 (epoch 336.60 / 500):
  learning_rate = 7.15e-04, loss_average = 2.12e-01
  validation accuracy: 95.85 (554 / 578), f1 (binary): 0.00, loss: 3.13e-01
  time: 9363s (wall 688s)
[selection_pooling] step 18000 / 25995 (epoch 346.22 / 500):
  learning_rate = 7.07e-04, loss_average = 1.67e-01
  validation accuracy: 95.85 (554 / 578), f1 (binary): 0.00, loss: 3.23e-01
  time: 9631s (wall 707s)
[selection_pooling] step 18500 / 25995 (epoch 355.84 / 500):
  learning_rate = 7.01e-04, loss_average = 1.40e-01
  validation accuracy: 95.85 (554 / 578), f1 (binary): 0.00, loss: 3.15e-01
  time: 9897s (wall 727s)
[selection_pooling] step 19000 / 25995 (epoch 365.45 / 500):
  learning_rate = 6.94e-04, loss_average = 1.24e-01
  validation accuracy: 95.85 (554 / 578), f1 (binary): 0.00, loss: 3.12e-01
  time: 10164s (wall 747s)
[selection_pooling] step 19500 / 25995 (epoch 375.07 / 500):
  learning_rate = 6.87e-04, loss_average = 1.32e-01
  validation accuracy: 95.85 (554 / 578), f1 (binary): 0.00, loss: 3.12e-01
  time: 10431s (wall 766s)
[selection_pooling] step 20000 / 25995 (epoch 384.69 / 500):
  learning_rate = 6.81e-04, loss_average = 1.28e-01
  validation accuracy: 95.85 (554 / 578), f1 (binary): 0.00, loss: 3.13e-01
  time: 10697s (wall 786s)
[selection_pooling] step 20500 / 25995 (epoch 394.31 / 500):
  learning_rate = 6.74e-04, loss_average = 1.33e-01
  validation accuracy: 95.85 (554 / 578), f1 (binary): 0.00, loss: 3.14e-01
  time: 10963s (wall 805s)
[selection_pooling] step 21000 / 25995 (epoch 403.92 / 500):
  learning_rate = 6.68e-04, loss_average = 1.40e-01
  validation accuracy: 95.85 (554 / 578), f1 (binary): 0.00, loss: 3.13e-01
  time: 11232s (wall 825s)
[selection_pooling] step 21500 / 25995 (epoch 413.54 / 500):
  learning_rate = 6.62e-04, loss_average = 1.26e-01
  validation accuracy: 95.85 (554 / 578), f1 (binary): 0.00, loss: 3.14e-01
  time: 11499s (wall 845s)
[selection_pooling] step 22000 / 25995 (epoch 423.16 / 500):
  learning_rate = 6.55e-04, loss_average = 1.41e-01
  validation accuracy: 95.85 (554 / 578), f1 (binary): 0.00, loss: 3.14e-01
  time: 11765s (wall 864s)
[selection_pooling] step 22500 / 25995 (epoch 432.78 / 500):
  learning_rate = 6.49e-04, loss_average = 1.28e-01
  validation accuracy: 95.85 (554 / 578), f1 (binary): 0.00, loss: 3.14e-01
  time: 12031s (wall 884s)
[selection_pooling] step 23000 / 25995 (epoch 442.39 / 500):
  learning_rate = 6.43e-04, loss_average = 1.40e-01
  validation accuracy: 95.85 (554 / 578), f1 (binary): 0.00, loss: 3.13e-01
  time: 12298s (wall 904s)
[selection_pooling] step 23500 / 25995 (epoch 452.01 / 500):
  learning_rate = 6.37e-04, loss_average = 7.15e+00
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 9.57e+00
  time: 12563s (wall 923s)
[selection_pooling] step 24000 / 25995 (epoch 461.63 / 500):
  learning_rate = 6.31e-04, loss_average = 1.49e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.63e-01
  time: 12832s (wall 943s)
[selection_pooling] step 24500 / 25995 (epoch 471.24 / 500):
  learning_rate = 6.24e-04, loss_average = 1.56e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.62e-01
  time: 13100s (wall 962s)
[selection_pooling] step 25000 / 25995 (epoch 480.86 / 500):
  learning_rate = 6.19e-04, loss_average = 1.44e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.62e-01
  time: 13367s (wall 982s)
[selection_pooling] step 25500 / 25995 (epoch 490.48 / 500):
  learning_rate = 6.12e-04, loss_average = 1.51e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.62e-01
  time: 13635s (wall 1002s)
[selection_pooling] step 25995 / 25995 (epoch 500.00 / 500):
  learning_rate = 6.07e-04, loss_average = 1.40e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.62e-01
  time: 13900s (wall 1021s)
validation accuracy: peak = 96.37, mean = 95.95
train accuracy: 96.27 (5005 / 5199), f1 (binary): 1.02, loss: 1.52e-01
time: 16s (wall 1s)
test  accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 1.62e-01
time: 2s (wall 0s)
 
Training model: aggregation_pooling
 
  architecture/L = 2
  architecture/N = [25, 12, 6]
CNNGS Architecture: aggregation_pooling (aggregation)
  input: M_0 = N = 25
  l_1: gsconv_1
    input dimension : M_0 = F_0 N_0 =  1 * 25 = 25
    output dimension: M_1 = F_1 N_1 = 16 * 12 = 192
    parameters: K_1 F_1 F_0 = 16 * 16 * 1 = 256
  l_2: gsconv_2
    input dimension : M_1 = F_1 N_1 = 16 * 12 = 192
    output dimension: M_2 = F_2 N_2 = 16 *  6 = 96
    parameters: K_2 F_2 F_1 = 16 * 16 * 16 = 4096
  l_3: softmax
    input dimension : M_2 = 96
    output dimension: M_3 = 2
    parameters: M_3 M_2 = 2 * 96 = 192
  Total parameters = 4544
 
[aggregation_pooling] step 500 / 25995 (epoch 9.62 / 500):
  learning_rate = 9.91e-04, loss_average = 2.01e-01
  validation accuracy: 79.76 (461 / 578), f1 (binary): 14.60, loss: 4.73e-01
  time: 51s (wall 7s)
[aggregation_pooling] step 1000 / 25995 (epoch 19.23 / 500):
  learning_rate = 9.81e-04, loss_average = 2.55e-01
  validation accuracy: 95.67 (553 / 578), f1 (binary): 0.00, loss: 3.87e-01
  time: 101s (wall 14s)
[aggregation_pooling] step 1500 / 25995 (epoch 28.85 / 500):
  learning_rate = 9.72e-04, loss_average = 5.02e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 0.00, loss: 6.80e-01
  time: 152s (wall 22s)
[aggregation_pooling] step 2000 / 25995 (epoch 38.47 / 500):
  learning_rate = 9.63e-04, loss_average = 1.18e-01
  validation accuracy: 95.67 (553 / 578), f1 (binary): 7.41, loss: 1.98e-01
  time: 202s (wall 29s)
[aggregation_pooling] step 2500 / 25995 (epoch 48.09 / 500):
  learning_rate = 9.53e-04, loss_average = 1.08e-01
  validation accuracy: 95.50 (552 / 578), f1 (binary): 0.00, loss: 1.96e-01
  time: 253s (wall 36s)
[aggregation_pooling] step 3000 / 25995 (epoch 57.70 / 500):
  learning_rate = 9.45e-04, loss_average = 9.76e-02
  validation accuracy: 95.67 (553 / 578), f1 (binary): 0.00, loss: 2.62e-01
  time: 303s (wall 43s)
[aggregation_pooling] step 3500 / 25995 (epoch 67.32 / 500):
  learning_rate = 9.35e-04, loss_average = 1.16e-01
  validation accuracy: 95.50 (552 / 578), f1 (binary): 7.14, loss: 2.12e-01
  time: 354s (wall 50s)
[aggregation_pooling] step 4000 / 25995 (epoch 76.94 / 500):
  learning_rate = 9.27e-04, loss_average = 1.30e-01
  validation accuracy: 95.85 (554 / 578), f1 (binary): 7.69, loss: 2.12e-01
  time: 404s (wall 57s)
[aggregation_pooling] step 4500 / 25995 (epoch 86.56 / 500):
  learning_rate = 9.18e-04, loss_average = 9.21e-02
  validation accuracy: 94.29 (545 / 578), f1 (binary): 15.38, loss: 2.00e-01
  time: 455s (wall 64s)
[aggregation_pooling] step 5000 / 25995 (epoch 96.17 / 500):
  learning_rate = 9.08e-04, loss_average = 1.16e-01
  validation accuracy: 95.50 (552 / 578), f1 (binary): 7.14, loss: 1.91e-01
  time: 506s (wall 72s)
[aggregation_pooling] step 5500 / 25995 (epoch 105.79 / 500):
  learning_rate = 9.00e-04, loss_average = 1.73e-01
  validation accuracy: 95.85 (554 / 578), f1 (binary): 7.69, loss: 1.85e-01
  time: 556s (wall 79s)
[aggregation_pooling] step 6000 / 25995 (epoch 115.41 / 500):
  learning_rate = 8.91e-04, loss_average = 1.22e-01
  validation accuracy: 95.67 (553 / 578), f1 (binary): 7.41, loss: 1.70e-01
  time: 607s (wall 86s)
[aggregation_pooling] step 6500 / 25995 (epoch 125.02 / 500):
  learning_rate = 8.82e-04, loss_average = 1.01e-01
  validation accuracy: 94.29 (545 / 578), f1 (binary): 26.67, loss: 1.89e-01
  time: 658s (wall 93s)
[aggregation_pooling] step 7000 / 25995 (epoch 134.64 / 500):
  learning_rate = 8.75e-04, loss_average = 1.08e-01
  validation accuracy: 94.81 (548 / 578), f1 (binary): 31.82, loss: 1.84e-01
  time: 709s (wall 100s)
[aggregation_pooling] step 7500 / 25995 (epoch 144.26 / 500):
  learning_rate = 8.66e-04, loss_average = 1.00e-01
  validation accuracy: 96.19 (556 / 578), f1 (binary): 15.38, loss: 1.69e-01
  time: 760s (wall 107s)
[aggregation_pooling] step 8000 / 25995 (epoch 153.88 / 500):
  learning_rate = 8.58e-04, loss_average = 8.35e-02
  validation accuracy: 94.64 (547 / 578), f1 (binary): 36.73, loss: 1.58e-01
  time: 810s (wall 115s)
[aggregation_pooling] step 8500 / 25995 (epoch 163.49 / 500):
  learning_rate = 8.50e-04, loss_average = 8.54e-02
  validation accuracy: 96.19 (556 / 578), f1 (binary): 8.33, loss: 1.95e-01
  time: 860s (wall 122s)
[aggregation_pooling] step 9000 / 25995 (epoch 173.11 / 500):
  learning_rate = 8.41e-04, loss_average = 1.03e-01
  validation accuracy: 96.02 (555 / 578), f1 (binary): 48.89, loss: 1.53e-01
  time: 910s (wall 129s)
[aggregation_pooling] step 9500 / 25995 (epoch 182.73 / 500):
  learning_rate = 8.34e-04, loss_average = 7.07e-02
  validation accuracy: 96.02 (555 / 578), f1 (binary): 14.81, loss: 1.49e-01
  time: 960s (wall 136s)
[aggregation_pooling] step 10000 / 25995 (epoch 192.34 / 500):
  learning_rate = 8.25e-04, loss_average = 8.09e-02
  validation accuracy: 96.02 (555 / 578), f1 (binary): 8.00, loss: 1.63e-01
  time: 1010s (wall 143s)
[aggregation_pooling] step 10500 / 25995 (epoch 201.96 / 500):
  learning_rate = 8.18e-04, loss_average = 8.50e-02
  validation accuracy: 96.19 (556 / 578), f1 (binary): 8.33, loss: 2.09e-01
  time: 1060s (wall 150s)
[aggregation_pooling] step 11000 / 25995 (epoch 211.58 / 500):
  learning_rate = 8.10e-04, loss_average = 6.64e-02
  validation accuracy: 96.19 (556 / 578), f1 (binary): 15.38, loss: 1.65e-01
  time: 1110s (wall 158s)
[aggregation_pooling] step 11500 / 25995 (epoch 221.20 / 500):
  learning_rate = 8.02e-04, loss_average = 6.46e-02
  validation accuracy: 96.19 (556 / 578), f1 (binary): 15.38, loss: 2.05e-01
  time: 1160s (wall 165s)
[aggregation_pooling] step 12000 / 25995 (epoch 230.81 / 500):
  learning_rate = 7.94e-04, loss_average = 4.70e-02
  validation accuracy: 96.37 (557 / 578), f1 (binary): 27.59, loss: 2.07e-01
  time: 1210s (wall 172s)
[aggregation_pooling] step 12500 / 25995 (epoch 240.43 / 500):
  learning_rate = 7.87e-04, loss_average = 7.43e-02
  validation accuracy: 96.54 (558 / 578), f1 (binary): 28.57, loss: 1.92e-01
  time: 1260s (wall 179s)
[aggregation_pooling] step 13000 / 25995 (epoch 250.05 / 500):
  learning_rate = 7.79e-04, loss_average = 4.97e-02
  validation accuracy: 96.71 (559 / 578), f1 (binary): 42.42, loss: 1.58e-01
  time: 1311s (wall 186s)
[aggregation_pooling] step 13500 / 25995 (epoch 259.67 / 500):
  learning_rate = 7.72e-04, loss_average = 5.74e-02
  validation accuracy: 96.02 (555 / 578), f1 (binary): 8.00, loss: 2.56e-01
  time: 1361s (wall 193s)
[aggregation_pooling] step 14000 / 25995 (epoch 269.28 / 500):
  learning_rate = 7.64e-04, loss_average = 5.44e-02
  validation accuracy: 96.37 (557 / 578), f1 (binary): 40.00, loss: 1.91e-01
  time: 1411s (wall 201s)
[aggregation_pooling] step 14500 / 25995 (epoch 278.90 / 500):
  learning_rate = 7.57e-04, loss_average = 5.42e-02
  validation accuracy: 96.54 (558 / 578), f1 (binary): 41.18, loss: 1.63e-01
  time: 1462s (wall 208s)
[aggregation_pooling] step 15000 / 25995 (epoch 288.52 / 500):
  learning_rate = 7.50e-04, loss_average = 4.42e-02
  validation accuracy: 96.54 (558 / 578), f1 (binary): 52.38, loss: 1.87e-01
  time: 1513s (wall 215s)
[aggregation_pooling] step 15500 / 25995 (epoch 298.13 / 500):
  learning_rate = 7.42e-04, loss_average = 3.42e-02
  validation accuracy: 96.19 (556 / 578), f1 (binary): 35.29, loss: 2.03e-01
  time: 1563s (wall 222s)
[aggregation_pooling] step 16000 / 25995 (epoch 307.75 / 500):
  learning_rate = 7.36e-04, loss_average = 4.63e-02
  validation accuracy: 96.02 (555 / 578), f1 (binary): 46.51, loss: 2.15e-01
  time: 1614s (wall 229s)
[aggregation_pooling] step 16500 / 25995 (epoch 317.37 / 500):
  learning_rate = 7.28e-04, loss_average = 5.21e-02
  validation accuracy: 96.19 (556 / 578), f1 (binary): 31.25, loss: 2.20e-01
  time: 1665s (wall 237s)
[aggregation_pooling] step 17000 / 25995 (epoch 326.99 / 500):
  learning_rate = 7.22e-04, loss_average = 3.49e-02
  validation accuracy: 96.37 (557 / 578), f1 (binary): 36.36, loss: 2.10e-01
  time: 1715s (wall 244s)
[aggregation_pooling] step 17500 / 25995 (epoch 336.60 / 500):
  learning_rate = 7.15e-04, loss_average = 4.26e-02
  validation accuracy: 94.64 (547 / 578), f1 (binary): 47.46, loss: 2.21e-01
  time: 1766s (wall 251s)
[aggregation_pooling] step 18000 / 25995 (epoch 346.22 / 500):
  learning_rate = 7.07e-04, loss_average = 4.04e-02
  validation accuracy: 96.37 (557 / 578), f1 (binary): 43.24, loss: 1.92e-01
  time: 1817s (wall 258s)
[aggregation_pooling] step 18500 / 25995 (epoch 355.84 / 500):
  learning_rate = 7.01e-04, loss_average = 3.80e-02
  validation accuracy: 96.19 (556 / 578), f1 (binary): 47.62, loss: 1.90e-01
  time: 1866s (wall 265s)
[aggregation_pooling] step 19000 / 25995 (epoch 365.45 / 500):
  learning_rate = 6.94e-04, loss_average = 2.87e-02
  validation accuracy: 96.54 (558 / 578), f1 (binary): 44.44, loss: 2.40e-01
  time: 1917s (wall 273s)
[aggregation_pooling] step 19500 / 25995 (epoch 375.07 / 500):
  learning_rate = 6.87e-04, loss_average = 3.56e-02
  validation accuracy: 96.54 (558 / 578), f1 (binary): 33.33, loss: 2.39e-01
  time: 1967s (wall 280s)
[aggregation_pooling] step 20000 / 25995 (epoch 384.69 / 500):
  learning_rate = 6.81e-04, loss_average = 4.17e-02
  validation accuracy: 96.54 (558 / 578), f1 (binary): 28.57, loss: 2.43e-01
  time: 2017s (wall 287s)
[aggregation_pooling] step 20500 / 25995 (epoch 394.31 / 500):
  learning_rate = 6.74e-04, loss_average = 2.81e-02
  validation accuracy: 96.71 (559 / 578), f1 (binary): 34.48, loss: 2.14e-01
  time: 2067s (wall 294s)
[aggregation_pooling] step 21000 / 25995 (epoch 403.92 / 500):
  learning_rate = 6.68e-04, loss_average = 4.87e-02
  validation accuracy: 96.02 (555 / 578), f1 (binary): 53.06, loss: 1.77e-01
  time: 2118s (wall 301s)
[aggregation_pooling] step 21500 / 25995 (epoch 413.54 / 500):
  learning_rate = 6.62e-04, loss_average = 3.42e-02
  validation accuracy: 96.19 (556 / 578), f1 (binary): 15.38, loss: 2.46e-01
  time: 2168s (wall 309s)
[aggregation_pooling] step 22000 / 25995 (epoch 423.16 / 500):
  learning_rate = 6.55e-04, loss_average = 2.05e-02
  validation accuracy: 96.37 (557 / 578), f1 (binary): 40.00, loss: 1.99e-01
  time: 2219s (wall 316s)
[aggregation_pooling] step 22500 / 25995 (epoch 432.78 / 500):
  learning_rate = 6.49e-04, loss_average = 1.86e-02
  validation accuracy: 96.71 (559 / 578), f1 (binary): 45.71, loss: 2.28e-01
  time: 2269s (wall 323s)
[aggregation_pooling] step 23000 / 25995 (epoch 442.39 / 500):
  learning_rate = 6.43e-04, loss_average = 2.30e-02
  validation accuracy: 96.19 (556 / 578), f1 (binary): 47.62, loss: 2.43e-01
  time: 2319s (wall 330s)
[aggregation_pooling] step 23500 / 25995 (epoch 452.01 / 500):
  learning_rate = 6.37e-04, loss_average = 3.19e-02
  validation accuracy: 96.71 (559 / 578), f1 (binary): 29.63, loss: 2.96e-01
  time: 2370s (wall 337s)
[aggregation_pooling] step 24000 / 25995 (epoch 461.63 / 500):
  learning_rate = 6.31e-04, loss_average = 3.36e-02
  validation accuracy: 96.89 (560 / 578), f1 (binary): 52.63, loss: 2.14e-01
  time: 2420s (wall 345s)
[aggregation_pooling] step 24500 / 25995 (epoch 471.24 / 500):
  learning_rate = 6.24e-04, loss_average = 2.44e-02
  validation accuracy: 96.37 (557 / 578), f1 (binary): 22.22, loss: 2.79e-01
  time: 2471s (wall 352s)
[aggregation_pooling] step 25000 / 25995 (epoch 480.86 / 500):
  learning_rate = 6.19e-04, loss_average = 2.46e-02
  validation accuracy: 96.71 (559 / 578), f1 (binary): 29.63, loss: 2.98e-01
  time: 2521s (wall 359s)
[aggregation_pooling] step 25500 / 25995 (epoch 490.48 / 500):
  learning_rate = 6.12e-04, loss_average = 2.00e-02
  validation accuracy: 96.02 (555 / 578), f1 (binary): 53.06, loss: 2.17e-01
  time: 2572s (wall 366s)
[aggregation_pooling] step 25995 / 25995 (epoch 500.00 / 500):
  learning_rate = 6.07e-04, loss_average = 1.86e-02
  validation accuracy: 96.54 (558 / 578), f1 (binary): 33.33, loss: 2.85e-01
  time: 2622s (wall 373s)
validation accuracy: peak = 96.89, mean = 96.47
train accuracy: 99.23 (5159 / 5199), f1 (binary): 89.01, loss: 2.26e-02
time: 3s (wall 1s)
test  accuracy: 96.54 (558 / 578), f1 (binary): 33.33, loss: 2.85e-01
time: 0s (wall 0s)
 
Training model: hybrid_pooling
 
  architecture/L = 2
  architecture/N = [25, 25, 10]
CNNGS Architecture: hybrid_pooling (hybrid)
  input: M_0 = N = 25
  l_1: gsconv_1
    input dimension : M_0 = F_0 N_0 =  1 * 25 = 25
    output dimension: M_1 = F_1 N_1 = 16 * 25 = 400
    parameters_1 detail:
      parameters_(1,1): K_(1,1) F_(1,1) F_(1,0) = 8 * 8 * 1 = 64
      parameters_(1,2): K_(1,2) F_(1,2) F_(1,1) = 8 * 16 * 8 = 1024
    parameters = parameters_1 N_1 = 1088 * 25 = 27200
  l_2: gsconv_2
    input dimension : M_1 = F_1 N_1 = 16 * 25 = 400
    output dimension: M_2 = F_2 N_2 = 16 * 10 = 160
    parameters_2 detail:
      parameters_(2,1): K_(2,1) F_(2,1) F_(2,0) = 8 * 8 * 16 = 1024
      parameters_(2,2): K_(2,2) F_(2,2) F_(2,1) = 8 * 16 * 8 = 1024
    parameters = parameters_2 N_2 = 2048 * 10 = 20480
  l_3: softmax
    input dimension : M_2 = 160
    output dimension: M_3 = 2
    parameters: M_3 M_2 = 2 * 160 = 320
  Total parameters = 48000
 
[hybrid_pooling] step 500 / 25995 (epoch 9.62 / 500):
  learning_rate = 9.91e-04, loss_average = 9.89e-02
  validation accuracy: 96.37 (557 / 578), f1 (binary): 22.22, loss: 1.21e-01
  time: 289s (wall 26s)
[hybrid_pooling] step 1000 / 25995 (epoch 19.23 / 500):
  learning_rate = 9.81e-04, loss_average = 7.35e-02
  validation accuracy: 96.89 (560 / 578), f1 (binary): 50.00, loss: 1.05e-01
  time: 579s (wall 51s)
[hybrid_pooling] step 1500 / 25995 (epoch 28.85 / 500):
  learning_rate = 9.72e-04, loss_average = 7.90e-02
  validation accuracy: 96.54 (558 / 578), f1 (binary): 23.08, loss: 1.49e-01
  time: 868s (wall 74s)
[hybrid_pooling] step 2000 / 25995 (epoch 38.47 / 500):
  learning_rate = 9.63e-04, loss_average = 7.11e-02
  validation accuracy: 96.89 (560 / 578), f1 (binary): 52.63, loss: 1.08e-01
  time: 1158s (wall 98s)
[hybrid_pooling] step 2500 / 25995 (epoch 48.09 / 500):
  learning_rate = 9.53e-04, loss_average = 5.75e-02
  validation accuracy: 97.40 (563 / 578), f1 (binary): 59.46, loss: 1.01e-01
  time: 1447s (wall 122s)
[hybrid_pooling] step 3000 / 25995 (epoch 57.70 / 500):
  learning_rate = 9.45e-04, loss_average = 6.96e-02
  validation accuracy: 96.89 (560 / 578), f1 (binary): 62.50, loss: 1.00e-01
  time: 1736s (wall 145s)
[hybrid_pooling] step 3500 / 25995 (epoch 67.32 / 500):
  learning_rate = 9.35e-04, loss_average = 5.27e-02
  validation accuracy: 96.89 (560 / 578), f1 (binary): 52.63, loss: 1.24e-01
  time: 2024s (wall 169s)
[hybrid_pooling] step 4000 / 25995 (epoch 76.94 / 500):
  learning_rate = 9.27e-04, loss_average = 4.14e-02
  validation accuracy: 97.06 (561 / 578), f1 (binary): 41.38, loss: 1.57e-01
  time: 2313s (wall 192s)
[hybrid_pooling] step 4500 / 25995 (epoch 86.56 / 500):
  learning_rate = 9.18e-04, loss_average = 3.78e-02
  validation accuracy: 97.58 (564 / 578), f1 (binary): 56.25, loss: 1.40e-01
  time: 2603s (wall 216s)
[hybrid_pooling] step 5000 / 25995 (epoch 96.17 / 500):
  learning_rate = 9.08e-04, loss_average = 3.99e-02
  validation accuracy: 97.06 (561 / 578), f1 (binary): 48.48, loss: 1.49e-01
  time: 2892s (wall 240s)
[hybrid_pooling] step 5500 / 25995 (epoch 105.79 / 500):
  learning_rate = 9.00e-04, loss_average = 2.53e-02
  validation accuracy: 97.75 (565 / 578), f1 (binary): 64.86, loss: 1.50e-01
  time: 3182s (wall 263s)
[hybrid_pooling] step 6000 / 25995 (epoch 115.41 / 500):
  learning_rate = 8.91e-04, loss_average = 3.18e-02
  validation accuracy: 97.75 (565 / 578), f1 (binary): 60.61, loss: 1.50e-01
  time: 3471s (wall 287s)
[hybrid_pooling] step 6500 / 25995 (epoch 125.02 / 500):
  learning_rate = 8.82e-04, loss_average = 1.81e-02
  validation accuracy: 97.58 (564 / 578), f1 (binary): 61.11, loss: 1.70e-01
  time: 3760s (wall 311s)
[hybrid_pooling] step 7000 / 25995 (epoch 134.64 / 500):
  learning_rate = 8.75e-04, loss_average = 1.34e-02
  validation accuracy: 97.40 (563 / 578), f1 (binary): 51.61, loss: 2.04e-01
  time: 4050s (wall 334s)
[hybrid_pooling] step 7500 / 25995 (epoch 144.26 / 500):
  learning_rate = 8.66e-04, loss_average = 2.66e-02
  validation accuracy: 97.40 (563 / 578), f1 (binary): 65.12, loss: 1.95e-01
  time: 4340s (wall 358s)
[hybrid_pooling] step 8000 / 25995 (epoch 153.88 / 500):
  learning_rate = 8.58e-04, loss_average = 2.09e-02
  validation accuracy: 97.92 (566 / 578), f1 (binary): 66.67, loss: 1.75e-01
  time: 4628s (wall 382s)
[hybrid_pooling] step 8500 / 25995 (epoch 163.49 / 500):
  learning_rate = 8.50e-04, loss_average = 2.61e-03
  validation accuracy: 98.10 (567 / 578), f1 (binary): 70.27, loss: 1.76e-01
  time: 4918s (wall 405s)
[hybrid_pooling] step 9000 / 25995 (epoch 173.11 / 500):
  learning_rate = 8.41e-04, loss_average = 5.80e-04
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 2.30e-01
  time: 5206s (wall 429s)
[hybrid_pooling] step 9500 / 25995 (epoch 182.73 / 500):
  learning_rate = 8.34e-04, loss_average = 2.02e-04
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 2.48e-01
  time: 5496s (wall 453s)
[hybrid_pooling] step 10000 / 25995 (epoch 192.34 / 500):
  learning_rate = 8.25e-04, loss_average = 1.10e-04
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 2.54e-01
  time: 5785s (wall 476s)
[hybrid_pooling] step 10500 / 25995 (epoch 201.96 / 500):
  learning_rate = 8.18e-04, loss_average = 8.37e-05
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 2.75e-01
  time: 6075s (wall 500s)
[hybrid_pooling] step 11000 / 25995 (epoch 211.58 / 500):
  learning_rate = 8.10e-04, loss_average = 5.13e-05
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 2.86e-01
  time: 6365s (wall 524s)
[hybrid_pooling] step 11500 / 25995 (epoch 221.20 / 500):
  learning_rate = 8.02e-04, loss_average = 3.41e-05
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 2.97e-01
  time: 6654s (wall 547s)
[hybrid_pooling] step 12000 / 25995 (epoch 230.81 / 500):
  learning_rate = 7.94e-04, loss_average = 2.40e-05
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 3.00e-01
  time: 6944s (wall 571s)
[hybrid_pooling] step 12500 / 25995 (epoch 240.43 / 500):
  learning_rate = 7.87e-04, loss_average = 1.64e-05
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 3.13e-01
  time: 7232s (wall 595s)
[hybrid_pooling] step 13000 / 25995 (epoch 250.05 / 500):
  learning_rate = 7.79e-04, loss_average = 1.44e-05
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 3.27e-01
  time: 7522s (wall 618s)
[hybrid_pooling] step 13500 / 25995 (epoch 259.67 / 500):
  learning_rate = 7.72e-04, loss_average = 1.17e-05
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 3.28e-01
  time: 7812s (wall 642s)
[hybrid_pooling] step 14000 / 25995 (epoch 269.28 / 500):
  learning_rate = 7.64e-04, loss_average = 4.73e-06
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 3.47e-01
  time: 8101s (wall 666s)
[hybrid_pooling] step 14500 / 25995 (epoch 278.90 / 500):
  learning_rate = 7.57e-04, loss_average = 5.28e-06
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 3.50e-01
  time: 8390s (wall 689s)
[hybrid_pooling] step 15000 / 25995 (epoch 288.52 / 500):
  learning_rate = 7.50e-04, loss_average = 3.73e-06
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 3.56e-01
  time: 8678s (wall 713s)
[hybrid_pooling] step 15500 / 25995 (epoch 298.13 / 500):
  learning_rate = 7.42e-04, loss_average = 2.57e-06
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 3.73e-01
  time: 8967s (wall 736s)
[hybrid_pooling] step 16000 / 25995 (epoch 307.75 / 500):
  learning_rate = 7.36e-04, loss_average = 2.08e-06
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 3.81e-01
  time: 9256s (wall 760s)
[hybrid_pooling] step 16500 / 25995 (epoch 317.37 / 500):
  learning_rate = 7.28e-04, loss_average = 1.59e-06
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 3.80e-01
  time: 9544s (wall 784s)
[hybrid_pooling] step 17000 / 25995 (epoch 326.99 / 500):
  learning_rate = 7.22e-04, loss_average = 1.23e-06
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 3.92e-01
  time: 9833s (wall 807s)
[hybrid_pooling] step 17500 / 25995 (epoch 336.60 / 500):
  learning_rate = 7.15e-04, loss_average = 9.02e-07
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 3.98e-01
  time: 10121s (wall 831s)
[hybrid_pooling] step 18000 / 25995 (epoch 346.22 / 500):
  learning_rate = 7.07e-04, loss_average = 4.71e-07
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 4.05e-01
  time: 10408s (wall 854s)
[hybrid_pooling] step 18500 / 25995 (epoch 355.84 / 500):
  learning_rate = 7.01e-04, loss_average = 6.05e-07
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 4.15e-01
  time: 10695s (wall 877s)
[hybrid_pooling] step 19000 / 25995 (epoch 365.45 / 500):
  learning_rate = 6.94e-04, loss_average = 3.12e-07
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 4.23e-01
  time: 10983s (wall 901s)
[hybrid_pooling] step 19500 / 25995 (epoch 375.07 / 500):
  learning_rate = 6.87e-04, loss_average = 2.65e-07
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 4.48e-01
  time: 11272s (wall 925s)
[hybrid_pooling] step 20000 / 25995 (epoch 384.69 / 500):
  learning_rate = 6.81e-04, loss_average = 2.11e-07
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 4.46e-01
  time: 11561s (wall 948s)
[hybrid_pooling] step 20500 / 25995 (epoch 394.31 / 500):
  learning_rate = 6.74e-04, loss_average = 1.59e-07
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 4.53e-01
  time: 11848s (wall 972s)
[hybrid_pooling] step 21000 / 25995 (epoch 403.92 / 500):
  learning_rate = 6.68e-04, loss_average = 1.25e-07
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 4.59e-01
  time: 12137s (wall 995s)
[hybrid_pooling] step 21500 / 25995 (epoch 413.54 / 500):
  learning_rate = 6.62e-04, loss_average = 7.93e-08
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 4.70e-01
  time: 12426s (wall 1019s)
[hybrid_pooling] step 22000 / 25995 (epoch 423.16 / 500):
  learning_rate = 6.55e-04, loss_average = 7.09e-08
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 4.80e-01
  time: 12714s (wall 1042s)
[hybrid_pooling] step 22500 / 25995 (epoch 432.78 / 500):
  learning_rate = 6.49e-04, loss_average = 5.22e-08
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 4.89e-01
  time: 13002s (wall 1066s)
[hybrid_pooling] step 23000 / 25995 (epoch 442.39 / 500):
  learning_rate = 6.43e-04, loss_average = 4.03e-08
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 4.97e-01
  time: 13291s (wall 1089s)
[hybrid_pooling] step 23500 / 25995 (epoch 452.01 / 500):
  learning_rate = 6.37e-04, loss_average = 2.57e-08
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 4.97e-01
  time: 13580s (wall 1113s)
[hybrid_pooling] step 24000 / 25995 (epoch 461.63 / 500):
  learning_rate = 6.31e-04, loss_average = 2.53e-08
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 5.08e-01
  time: 13868s (wall 1136s)
[hybrid_pooling] step 24500 / 25995 (epoch 471.24 / 500):
  learning_rate = 6.24e-04, loss_average = 1.82e-08
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 5.17e-01
  time: 14156s (wall 1160s)
[hybrid_pooling] step 25000 / 25995 (epoch 480.86 / 500):
  learning_rate = 6.19e-04, loss_average = 1.05e-08
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 5.32e-01
  time: 14444s (wall 1183s)
[hybrid_pooling] step 25500 / 25995 (epoch 490.48 / 500):
  learning_rate = 6.12e-04, loss_average = 6.76e-09
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 5.43e-01
  time: 14733s (wall 1207s)
[hybrid_pooling] step 25995 / 25995 (epoch 500.00 / 500):
  learning_rate = 6.07e-04, loss_average = 6.50e-09
  validation accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 5.43e-01
  time: 15020s (wall 1230s)
validation accuracy: peak = 98.10, mean = 97.75
train accuracy: 100.00 (5199 / 5199), f1 (binary): 100.00, loss: 1.52e-04
time: 14s (wall 2s)
test  accuracy: 97.75 (565 / 578), f1 (binary): 62.86, loss: 5.43e-01
time: 2s (wall 1s)
 
Showing results...
 
    {n = 25, norm-Laplacian, num_epochs = 500, batch_size = 100, 
     reg = 0, dropout = 0, momentum = 0
     ADAM, learning_rate = 0.001}
 
Region: NYC
    aggregation_pooling = {F = [16, 16], K = [16, 16], M = [2]}
    c_cheb_a = {F = [14, 28], K = [7, 14], M = [2]}
    hybrid_pooling = {F = [[8, 16], [8, 16]], K = [[8, 8], [8, 8]], M = [2]}
    np_3 = {F = [14, 28], K = [7, 14], M = [2]}
    selection_pooling = {F = [16, 16], K = [16, 16], M = [2]}
 
    Results:
      accuracy        F1        parameters    time [ms]  name
    test  train   test  train   
    96.54 99.23   33.33 89.01      4544         14       aggregation_pooling
    96.02 96.29    0.00  2.03      5978         25       c_cheb_a
    97.75 100.00   62.86 100.00     48000         47       hybrid_pooling
    96.02 96.29    0.00  2.03      6986         24       np_3
    96.02 96.27    0.00  1.02      4832         39       selection_pooling
 
 
Clustering graph sizes:
S_c[0]: 28
S_c[1]: 14
S_c[2]: 7
