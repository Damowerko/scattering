Setting up problem parameters... DONE
Gathering data... Numbmer of datapoints: 5777
Building graph support... DONE
Running Neural Networks: BEGINNING
 
Training model: c_cheb_a
 
  architecture/L = 2
  architecture/N = [32, 16, 8]
CNNGS Architecture: c_cheb_a (clustering)
  input: M_0 = N = 32
  l_1: gsconv_1
    input dimension : M_0 = F_0 N_0 =  1 * 32 = 32
    output dimension: M_1 = F_1 N_1 = 14 * 16 = 224
    parameters: K_1 F_1 F_0 = 7 * 14 * 1 = 98
  l_2: gsconv_2
    input dimension : M_1 = F_1 N_1 = 14 * 16 = 224
    output dimension: M_2 = F_2 N_2 = 28 *  8 = 224
    parameters: K_2 F_2 F_1 = 14 * 28 * 14 = 5488
  l_3: softmax
    input dimension : M_2 = 224
    output dimension: M_3 = 2
    parameters: M_3 M_2 = 2 * 224 = 448
  Total parameters = 6034
 
[c_cheb_a] step 500 / 5199 (epoch 9.62 / 100):
  learning_rate = 9.91e-04, loss_average = 8.81e+04
  validation accuracy: 96.02 (555 / 578), f1 (weighted): 94.07, loss: 1.62e+05
  time: 883s (wall 39s)
[c_cheb_a] step 1000 / 5199 (epoch 19.23 / 100):
  learning_rate = 9.81e-04, loss_average = 1.26e+04
  validation accuracy: 96.02 (555 / 578), f1 (weighted): 94.65, loss: 1.09e+04
  time: 1711s (wall 75s)
[c_cheb_a] step 1500 / 5199 (epoch 28.85 / 100):
  learning_rate = 9.72e-04, loss_average = 1.04e+04
  validation accuracy: 96.37 (557 / 578), f1 (weighted): 95.33, loss: 8.53e+03
  time: 2595s (wall 112s)
[c_cheb_a] step 2000 / 5199 (epoch 38.47 / 100):
  learning_rate = 9.63e-04, loss_average = 5.47e+03
  validation accuracy: 93.94 (543 / 578), f1 (weighted): 94.65, loss: 4.95e+03
  time: 3441s (wall 148s)
[c_cheb_a] step 2500 / 5199 (epoch 48.09 / 100):
  learning_rate = 9.53e-04, loss_average = 1.51e+04
  validation accuracy: 96.02 (555 / 578), f1 (weighted): 94.07, loss: 3.10e+04
  time: 4289s (wall 184s)
[c_cheb_a] step 3000 / 5199 (epoch 57.70 / 100):
  learning_rate = 9.45e-04, loss_average = 8.63e+03
  validation accuracy: 96.02 (555 / 578), f1 (weighted): 94.07, loss: 8.37e+03
  time: 5120s (wall 219s)
[c_cheb_a] step 3500 / 5199 (epoch 67.32 / 100):
  learning_rate = 9.35e-04, loss_average = 3.88e+03
  validation accuracy: 96.02 (555 / 578), f1 (weighted): 94.88, loss: 3.48e+03
  time: 5966s (wall 255s)
[c_cheb_a] step 4000 / 5199 (epoch 76.94 / 100):
  learning_rate = 9.27e-04, loss_average = 2.18e+03
  validation accuracy: 94.98 (549 / 578), f1 (weighted): 95.44, loss: 1.92e+03
  time: 6788s (wall 290s)
[c_cheb_a] step 4500 / 5199 (epoch 86.56 / 100):
  learning_rate = 9.18e-04, loss_average = 2.49e+03
  validation accuracy: 95.67 (553 / 578), f1 (weighted): 95.80, loss: 1.31e+03
  time: 7617s (wall 325s)
[c_cheb_a] step 5000 / 5199 (epoch 96.17 / 100):
  learning_rate = 9.08e-04, loss_average = 1.45e+03
  validation accuracy: 95.85 (554 / 578), f1 (weighted): 94.29, loss: 1.77e+03
  time: 8479s (wall 362s)
[c_cheb_a] step 5199 / 5199 (epoch 100.00 / 100):
  learning_rate = 9.06e-04, loss_average = 1.85e+03
  validation accuracy: 93.94 (543 / 578), f1 (weighted): 94.78, loss: 1.69e+03
  time: 8803s (wall 376s)
validation accuracy: peak = 96.37, mean = 95.48
train accuracy: 95.06 (4942 / 5199), f1 (weighted): 95.63, loss: 1.08e+03
time: 29s (wall 2s)
test  accuracy: 93.94 (543 / 578), f1 (weighted): 94.78, loss: 1.69e+03
time: 9s (wall 1s)
 
Training model: np_3
 
  architecture/L = 2
  architecture/N = [25, 25, 25]
CNNGS Architecture: np_3 (no-pooling)
  input: M_0 = N = 25
  l_1: gsconv_1
    input dimension : M_0 = F_0 N_0 =  1 * 25 = 25
    output dimension: M_1 = F_1 N_1 = 14 * 25 = 350
    parameters: K_1 F_1 F_0 = 7 * 14 * 1 = 98
  l_2: gsconv_2
    input dimension : M_1 = F_1 N_1 = 14 * 25 = 350
    output dimension: M_2 = F_2 N_2 = 28 * 25 = 700
    parameters: K_2 F_2 F_1 = 14 * 28 * 14 = 5488
  l_3: softmax
    input dimension : M_2 = 700
    output dimension: M_3 = 2
    parameters: M_3 M_2 = 2 * 700 = 1400
  Total parameters = 6986
 
[np_3] step 500 / 5199 (epoch 9.62 / 100):
  learning_rate = 9.91e-04, loss_average = 2.15e+02
  validation accuracy: 96.71 (559 / 578), f1 (weighted): 96.87, loss: 8.17e+01
  time: 909s (wall 39s)
[np_3] step 1000 / 5199 (epoch 19.23 / 100):
  learning_rate = 9.81e-04, loss_average = 1.72e+02
  validation accuracy: 95.85 (554 / 578), f1 (weighted): 93.98, loss: 4.10e+02
  time: 1833s (wall 78s)
[np_3] step 1500 / 5199 (epoch 28.85 / 100):
  learning_rate = 9.72e-04, loss_average = 1.18e+02
  validation accuracy: 93.77 (542 / 578), f1 (weighted): 94.82, loss: 1.60e+02
  time: 2745s (wall 117s)
[np_3] step 2000 / 5199 (epoch 38.47 / 100):
  learning_rate = 9.63e-04, loss_average = 1.67e+02
  validation accuracy: 96.54 (558 / 578), f1 (weighted): 95.80, loss: 9.76e+01
  time: 3673s (wall 156s)
[np_3] step 2500 / 5199 (epoch 48.09 / 100):
  learning_rate = 9.53e-04, loss_average = 1.21e+02
  validation accuracy: 96.02 (555 / 578), f1 (weighted): 94.07, loss: 1.89e+02
  time: 4601s (wall 195s)
[np_3] step 3000 / 5199 (epoch 57.70 / 100):
  learning_rate = 9.45e-04, loss_average = 6.92e+01
  validation accuracy: 96.89 (560 / 578), f1 (weighted): 95.91, loss: 9.86e+01
  time: 5529s (wall 234s)
[np_3] step 3500 / 5199 (epoch 67.32 / 100):
  learning_rate = 9.35e-04, loss_average = 7.45e+01
  validation accuracy: 96.89 (560 / 578), f1 (weighted): 96.22, loss: 5.19e+01
  time: 6477s (wall 274s)
[np_3] step 4000 / 5199 (epoch 76.94 / 100):
  learning_rate = 9.27e-04, loss_average = 4.76e+01
  validation accuracy: 97.06 (561 / 578), f1 (weighted): 96.37, loss: 5.23e+01
  time: 7404s (wall 314s)
[np_3] step 4500 / 5199 (epoch 86.56 / 100):
  learning_rate = 9.18e-04, loss_average = 3.46e+01
  validation accuracy: 97.75 (565 / 578), f1 (weighted): 97.68, loss: 2.84e+01
  time: 8328s (wall 353s)
[np_3] step 5000 / 5199 (epoch 96.17 / 100):
  learning_rate = 9.08e-04, loss_average = 4.64e+01
  validation accuracy: 97.06 (561 / 578), f1 (weighted): 97.03, loss: 2.95e+01
  time: 9263s (wall 392s)
[np_3] step 5199 / 5199 (epoch 100.00 / 100):
  learning_rate = 9.06e-04, loss_average = 2.22e+01
  validation accuracy: 97.40 (563 / 578), f1 (weighted): 97.32, loss: 1.61e+01
  time: 9629s (wall 408s)
validation accuracy: peak = 97.75, mean = 96.52
train accuracy: 97.98 (5094 / 5199), f1 (weighted): 98.00, loss: 1.18e+01
time: 25s (wall 1s)
test  accuracy: 97.40 (563 / 578), f1 (weighted): 97.32, loss: 1.61e+01
time: 7s (wall 1s)
 
Training model: selection_pooling
 
  architecture/L = 2
  architecture/N = [25, 25, 15]
CNNGS Architecture: selection_pooling (selection)
  input: M_0 = N = 25
  l_1: gsconv_1
    input dimension : M_0 = F_0 N_0 =  1 * 25 = 25
    output dimension: M_1 = F_1 N_1 = 14 * 25 = 350
    parameters: K_1 F_1 F_0 = 7 * 14 * 1 = 98
  l_2: gsconv_2
    input dimension : M_1 = F_1 N_1 = 14 * 25 = 350
    output dimension: M_2 = F_2 N_2 = 28 * 15 = 420
    parameters: K_2 F_2 F_1 = 14 * 28 * 14 = 5488
  l_3: softmax
    input dimension : M_2 = 420
    output dimension: M_3 = 2
    parameters: M_3 M_2 = 2 * 420 = 840
  Total parameters = 6426
 
[selection_pooling] step 500 / 5199 (epoch 9.62 / 100):
  learning_rate = 9.91e-04, loss_average = 4.32e+01
  validation accuracy: 64.53 (373 / 578), f1 (weighted): 75.00, loss: 1.05e+02
  time: 1050s (wall 45s)
[selection_pooling] step 1000 / 5199 (epoch 19.23 / 100):
  learning_rate = 9.81e-04, loss_average = 1.13e+01
  validation accuracy: 78.55 (454 / 578), f1 (weighted): 84.91, loss: 8.94e+00
  time: 2102s (wall 90s)
[selection_pooling] step 1500 / 5199 (epoch 28.85 / 100):
  learning_rate = 9.72e-04, loss_average = 2.32e+00
  validation accuracy: 94.98 (549 / 578), f1 (weighted): 94.39, loss: 1.27e+00
  time: 3172s (wall 135s)
[selection_pooling] step 2000 / 5199 (epoch 38.47 / 100):
  learning_rate = 9.63e-04, loss_average = 2.77e+00
  validation accuracy: 96.19 (556 / 578), f1 (weighted): 94.49, loss: 2.17e+00
  time: 4247s (wall 181s)
[selection_pooling] step 2500 / 5199 (epoch 48.09 / 100):
  learning_rate = 9.53e-04, loss_average = 3.16e+00
  validation accuracy: 94.12 (544 / 578), f1 (weighted): 93.85, loss: 1.42e+00
  time: 5337s (wall 226s)
[selection_pooling] step 3000 / 5199 (epoch 57.70 / 100):
  learning_rate = 9.45e-04, loss_average = 1.87e+00
  validation accuracy: 95.85 (554 / 578), f1 (weighted): 93.98, loss: 1.66e+00
  time: 6399s (wall 271s)
[selection_pooling] step 3500 / 5199 (epoch 67.32 / 100):
  learning_rate = 9.35e-04, loss_average = 1.94e+00
  validation accuracy: 95.85 (554 / 578), f1 (weighted): 94.29, loss: 1.97e+00
  time: 7481s (wall 317s)
[selection_pooling] step 4000 / 5199 (epoch 76.94 / 100):
  learning_rate = 9.27e-04, loss_average = 4.83e-01
  validation accuracy: 95.67 (553 / 578), f1 (weighted): 95.72, loss: 2.89e-01
  time: 8562s (wall 363s)
[selection_pooling] step 4500 / 5199 (epoch 86.56 / 100):
  learning_rate = 9.18e-04, loss_average = 1.49e+01
  validation accuracy: 74.22 (429 / 578), f1 (weighted): 82.06, loss: 2.53e+01
  time: 9654s (wall 409s)
[selection_pooling] step 5000 / 5199 (epoch 96.17 / 100):
  learning_rate = 9.08e-04, loss_average = 6.07e-01
  validation accuracy: 93.77 (542 / 578), f1 (weighted): 94.11, loss: 4.66e-01
  time: 10730s (wall 454s)
[selection_pooling] step 5199 / 5199 (epoch 100.00 / 100):
  learning_rate = 9.06e-04, loss_average = 2.84e-01
  validation accuracy: 93.60 (541 / 578), f1 (weighted): 93.90, loss: 1.98e-01
  time: 11150s (wall 472s)
validation accuracy: peak = 96.19, mean = 91.28
train accuracy: 94.46 (4911 / 5199), f1 (weighted): 94.58, loss: 1.60e-01
time: 35s (wall 2s)
test  accuracy: 93.60 (541 / 578), f1 (weighted): 93.90, loss: 1.98e-01
time: 6s (wall 1s)
 
Training model: aggregation_pooling
 
  architecture/L = 2
  architecture/N = [25, 6, 1]
CNNGS Architecture: aggregation_pooling (aggregation)
  input: M_0 = N = 25
  l_1: gsconv_1
    input dimension : M_0 = F_0 N_0 =  1 * 25 = 25
    output dimension: M_1 = F_1 N_1 = 14 *  6 = 84
    parameters: K_1 F_1 F_0 = 7 * 14 * 1 = 98
  l_2: gsconv_2
    input dimension : M_1 = F_1 N_1 = 14 *  6 = 84
    output dimension: M_2 = F_2 N_2 = 28 *  1 = 28
    parameters: K_2 F_2 F_1 = 14 * 28 * 14 = 5488
  l_3: softmax
    input dimension : M_2 = 28
    output dimension: M_3 = 2
    parameters: M_3 M_2 = 2 * 28 = 56
  Total parameters = 5642
 
[aggregation_pooling] step 500 / 5199 (epoch 9.62 / 100):
  learning_rate = 9.91e-04, loss_average = 1.45e-01
  validation accuracy: 95.33 (551 / 578), f1 (weighted): 93.72, loss: 2.02e-01
  time: 423s (wall 19s)
[aggregation_pooling] step 1000 / 5199 (epoch 19.23 / 100):
  learning_rate = 9.81e-04, loss_average = 1.53e-01
  validation accuracy: 96.02 (555 / 578), f1 (weighted): 94.39, loss: 1.63e-01
  time: 834s (wall 36s)
[aggregation_pooling] step 1500 / 5199 (epoch 28.85 / 100):
  learning_rate = 9.72e-04, loss_average = 1.88e-01
  validation accuracy: 95.85 (554 / 578), f1 (weighted): 93.98, loss: 1.96e-01
  time: 1255s (wall 54s)
[aggregation_pooling] step 2000 / 5199 (epoch 38.47 / 100):
  learning_rate = 9.63e-04, loss_average = 1.13e-01
  validation accuracy: 96.02 (555 / 578), f1 (weighted): 94.07, loss: 1.83e-01
  time: 1698s (wall 72s)
[aggregation_pooling] step 2500 / 5199 (epoch 48.09 / 100):
  learning_rate = 9.53e-04, loss_average = 1.53e-01
  validation accuracy: 95.85 (554 / 578), f1 (weighted): 93.98, loss: 2.44e-01
  time: 2110s (wall 90s)
[aggregation_pooling] step 3000 / 5199 (epoch 57.70 / 100):
  learning_rate = 9.45e-04, loss_average = 1.25e-01
  validation accuracy: 95.85 (554 / 578), f1 (weighted): 93.98, loss: 1.53e-01
  time: 2534s (wall 108s)
[aggregation_pooling] step 3500 / 5199 (epoch 67.32 / 100):
  learning_rate = 9.35e-04, loss_average = 1.04e-01
  validation accuracy: 96.02 (555 / 578), f1 (weighted): 94.39, loss: 1.33e-01
  time: 2987s (wall 127s)
[aggregation_pooling] step 4000 / 5199 (epoch 76.94 / 100):
  learning_rate = 9.27e-04, loss_average = 9.97e-02
  validation accuracy: 95.85 (554 / 578), f1 (weighted): 93.98, loss: 2.12e-01
  time: 3443s (wall 146s)
[aggregation_pooling] step 4500 / 5199 (epoch 86.56 / 100):
  learning_rate = 9.18e-04, loss_average = 1.20e-01
  validation accuracy: 95.85 (554 / 578), f1 (weighted): 93.98, loss: 1.52e-01
  time: 3846s (wall 163s)
[aggregation_pooling] step 5000 / 5199 (epoch 96.17 / 100):
  learning_rate = 9.08e-04, loss_average = 9.41e-02
  validation accuracy: 96.71 (559 / 578), f1 (weighted): 95.58, loss: 1.25e-01
  time: 4275s (wall 181s)
[aggregation_pooling] step 5199 / 5199 (epoch 100.00 / 100):
  learning_rate = 9.06e-04, loss_average = 1.01e-01
  validation accuracy: 95.50 (552 / 578), f1 (weighted): 94.90, loss: 1.38e-01
  time: 4435s (wall 188s)
validation accuracy: peak = 96.71, mean = 95.95
train accuracy: 97.06 (5046 / 5199), f1 (weighted): 96.31, loss: 8.71e-02
time: 12s (wall 1s)
test  accuracy: 95.50 (552 / 578), f1 (weighted): 94.90, loss: 1.38e-01
time: 5s (wall 0s)
 
Training model: hybrid_pooling
 
  architecture/L = 2
  architecture/N = [25, 25, 10]
CNNGS Architecture: hybrid_pooling (hybrid)
  input: M_0 = N = 25
  l_1: gsconv_1
    input dimension : M_0 = F_0 N_0 =  1 * 25 = 25
    output dimension: M_1 = F_1 N_1 = 16 * 25 = 400
    parameters_1 detail:
      parameters_(1,1): K_(1,1) F_(1,1) F_(1,0) = 7 * 8 * 1 = 56
      parameters_(1,2): K_(1,2) F_(1,2) F_(1,1) = 14 * 16 * 8 = 1792
    parameters = parameters_1 N_1 = 1848 * 25 = 46200
  l_2: gsconv_2
    input dimension : M_1 = F_1 N_1 = 16 * 25 = 400
    output dimension: M_2 = F_2 N_2 = 16 * 10 = 160
    parameters_2 detail:
      parameters_(2,1): K_(2,1) F_(2,1) F_(2,0) = 7 * 8 * 16 = 896
      parameters_(2,2): K_(2,2) F_(2,2) F_(2,1) = 14 * 16 * 8 = 1792
    parameters = parameters_2 N_2 = 2688 * 10 = 26880
  l_3: softmax
    input dimension : M_2 = 160
    output dimension: M_3 = 2
    parameters: M_3 M_2 = 2 * 160 = 320
  Total parameters = 73400
 
[hybrid_pooling] step 500 / 5199 (epoch 9.62 / 100):
  learning_rate = 9.91e-04, loss_average = 1.14e-01
  validation accuracy: 96.71 (559 / 578), f1 (weighted): 95.94, loss: 1.06e-01
  time: 3673s (wall 166s)
[hybrid_pooling] step 1000 / 5199 (epoch 19.23 / 100):
  learning_rate = 9.81e-04, loss_average = 6.36e-02
  validation accuracy: 96.71 (559 / 578), f1 (weighted): 95.58, loss: 1.38e-01
  time: 7309s (wall 324s)
[hybrid_pooling] step 1500 / 5199 (epoch 28.85 / 100):
  learning_rate = 9.72e-04, loss_average = 7.58e-02
  validation accuracy: 97.23 (562 / 578), f1 (weighted): 96.86, loss: 1.19e-01
  time: 10954s (wall 480s)
[hybrid_pooling] step 2000 / 5199 (epoch 38.47 / 100):
  learning_rate = 9.63e-04, loss_average = 4.82e-02
  validation accuracy: 97.40 (563 / 578), f1 (weighted): 97.25, loss: 1.08e-01
  time: 14545s (wall 635s)
[hybrid_pooling] step 2500 / 5199 (epoch 48.09 / 100):
  learning_rate = 9.53e-04, loss_average = 4.17e-02
  validation accuracy: 97.40 (563 / 578), f1 (weighted): 96.79, loss: 1.35e-01
  time: 18149s (wall 790s)
[hybrid_pooling] step 3000 / 5199 (epoch 57.70 / 100):
  learning_rate = 9.45e-04, loss_average = 5.36e-02
  validation accuracy: 97.40 (563 / 578), f1 (weighted): 97.01, loss: 1.13e-01
  time: 21845s (wall 949s)
[hybrid_pooling] step 3500 / 5199 (epoch 67.32 / 100):
  learning_rate = 9.35e-04, loss_average = 3.90e-02
  validation accuracy: 97.23 (562 / 578), f1 (weighted): 96.95, loss: 1.34e-01
  time: 25419s (wall 1103s)
[hybrid_pooling] step 4000 / 5199 (epoch 76.94 / 100):
  learning_rate = 9.27e-04, loss_average = 4.09e-02
  validation accuracy: 97.40 (563 / 578), f1 (weighted): 97.38, loss: 1.18e-01
  time: 28928s (wall 1254s)
[hybrid_pooling] step 4500 / 5199 (epoch 86.56 / 100):
  learning_rate = 9.18e-04, loss_average = 3.18e-02
  validation accuracy: 97.06 (561 / 578), f1 (weighted): 97.03, loss: 1.26e-01
  time: 32367s (wall 1402s)
[hybrid_pooling] step 5000 / 5199 (epoch 96.17 / 100):
  learning_rate = 9.08e-04, loss_average = 1.46e-02
  validation accuracy: 97.58 (564 / 578), f1 (weighted): 97.16, loss: 1.54e-01
  time: 35876s (wall 1553s)
[hybrid_pooling] step 5199 / 5199 (epoch 100.00 / 100):
  learning_rate = 9.06e-04, loss_average = 2.19e-02
  validation accuracy: 97.23 (562 / 578), f1 (weighted): 96.76, loss: 1.72e-01
  time: 37301s (wall 1617s)
validation accuracy: peak = 97.58, mean = 97.27
train accuracy: 99.08 (5151 / 5199), f1 (weighted): 99.06, loss: 2.55e-02
time: 166s (wall 10s)
test  accuracy: 97.23 (562 / 578), f1 (weighted): 96.76, loss: 1.72e-01
time: 31s (wall 4s)
 
Showing results...
 
    {n = 25, norm-Laplacian, num_epochs = 100, batch_size = 100, 
     reg = 0, dropout = 0.5, momentum = 0
     ADAM, learning_rate = 0.001}
 
    aggregation_pooling = {F = [14, 28], K = [7, 14], M = [2]}
    c_cheb_a = {F = [14, 28], K = [7, 14], M = [2]}
    hybrid_pooling = {F = [[8, 16], [8, 16]], K = [[7, 14], [7, 14]], M = [2]}
    np_3 = {F = [14, 28], K = [7, 14], M = [2]}
    selection_pooling = {F = [14, 28], K = [7, 14], M = [2]}
 
    Results:
      accuracy        F1        parameters    time [ms]  name
    test  train   test  train   
    95.50 97.06   94.90 96.31      5642         36       aggregation_pooling
    93.94 95.06   94.78 95.63      6034         73       c_cheb_a
    97.23 99.08   96.76 99.06     73400        312       hybrid_pooling
    97.40 97.98   97.32 98.00      6986         79       np_3
    93.60 94.46   93.90 94.58      6426         91       selection_pooling
 
 
Clustering graph sizes:
S_c[0]: 32
S_c[1]: 16
S_c[2]: 8
