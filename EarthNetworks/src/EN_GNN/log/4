Setting up problem parameters... DONE
Gathering data... Numbmer of datapoints: 5777
Building graph support... DONE
Running Neural Networks: BEGINNING
 
Training model: c_cheb_a
 
  architecture/L = 2
  architecture/N = [32, 16, 8]
CNNGS Architecture: c_cheb_a (clustering)
  input: M_0 = N = 32
  l_1: gsconv_1
    input dimension : M_0 = F_0 N_0 =  1 * 32 = 32
    output dimension: M_1 = F_1 N_1 = 14 * 16 = 224
    parameters: K_1 F_1 F_0 = 7 * 14 * 1 = 98
  l_2: gsconv_2
    input dimension : M_1 = F_1 N_1 = 14 * 16 = 224
    output dimension: M_2 = F_2 N_2 = 28 *  8 = 224
    parameters: K_2 F_2 F_1 = 14 * 28 * 14 = 5488
  l_3: softmax
    input dimension : M_2 = 224
    output dimension: M_3 = 2
    parameters: M_3 M_2 = 2 * 224 = 448
  Total parameters = 6034
 
[c_cheb_a] step 500 / 5199 (epoch 9.62 / 100):
  learning_rate = 9.91e-04, loss_average = 2.01e+04
  validation accuracy: 94.29 (545 / 578), f1 (weighted): 93.17, loss: 3.44e+04
  time: 855s (wall 38s)
[c_cheb_a] step 1000 / 5199 (epoch 19.23 / 100):
  learning_rate = 9.81e-04, loss_average = 1.38e+04
  validation accuracy: 94.81 (548 / 578), f1 (weighted): 93.69, loss: 8.49e+03
  time: 1692s (wall 74s)
[c_cheb_a] step 1500 / 5199 (epoch 28.85 / 100):
  learning_rate = 9.72e-04, loss_average = 1.15e+04
  validation accuracy: 95.67 (553 / 578), f1 (weighted): 94.00, loss: 9.85e+03
  time: 2524s (wall 109s)
[c_cheb_a] step 2000 / 5199 (epoch 38.47 / 100):
  learning_rate = 9.63e-04, loss_average = 1.34e+04
  validation accuracy: 95.16 (550 / 578), f1 (weighted): 93.43, loss: 9.07e+03
  time: 3332s (wall 143s)
[c_cheb_a] step 2500 / 5199 (epoch 48.09 / 100):
  learning_rate = 9.53e-04, loss_average = 1.10e+04
  validation accuracy: 94.46 (546 / 578), f1 (weighted): 94.24, loss: 5.92e+03
  time: 4183s (wall 179s)
[c_cheb_a] step 3000 / 5199 (epoch 57.70 / 100):
  learning_rate = 9.45e-04, loss_average = 5.94e+03
  validation accuracy: 95.33 (551 / 578), f1 (weighted): 94.58, loss: 6.42e+03
  time: 5000s (wall 214s)
[c_cheb_a] step 3500 / 5199 (epoch 67.32 / 100):
  learning_rate = 9.35e-04, loss_average = 5.76e+03
  validation accuracy: 87.89 (508 / 578), f1 (weighted): 90.75, loss: 8.39e+03
  time: 5828s (wall 249s)
[c_cheb_a] step 4000 / 5199 (epoch 76.94 / 100):
  learning_rate = 9.27e-04, loss_average = 3.86e+03
  validation accuracy: 91.00 (526 / 578), f1 (weighted): 92.74, loss: 4.69e+03
  time: 6680s (wall 285s)
[c_cheb_a] step 4500 / 5199 (epoch 86.56 / 100):
  learning_rate = 9.18e-04, loss_average = 1.86e+03
  validation accuracy: 91.87 (531 / 578), f1 (weighted): 93.27, loss: 3.81e+03
  time: 7530s (wall 323s)
[c_cheb_a] step 5000 / 5199 (epoch 96.17 / 100):
  learning_rate = 9.08e-04, loss_average = 1.71e+03
  validation accuracy: 95.67 (553 / 578), f1 (weighted): 94.47, loss: 1.53e+03
  time: 8389s (wall 359s)
[c_cheb_a] step 5199 / 5199 (epoch 100.00 / 100):
  learning_rate = 9.06e-04, loss_average = 1.43e+03
  validation accuracy: 95.16 (550 / 578), f1 (weighted): 94.85, loss: 1.13e+03
  time: 8735s (wall 374s)
validation accuracy: peak = 95.67, mean = 93.70
train accuracy: 95.36 (4958 / 5199), f1 (weighted): 95.47, loss: 9.24e+02
time: 26s (wall 1s)
test  accuracy: 95.16 (550 / 578), f1 (weighted): 94.85, loss: 1.13e+03
time: 7s (wall 1s)
 
Training model: np_3
 
  architecture/L = 2
  architecture/N = [25, 25, 25]
CNNGS Architecture: np_3 (no-pooling)
  input: M_0 = N = 25
  l_1: gsconv_1
    input dimension : M_0 = F_0 N_0 =  1 * 25 = 25
    output dimension: M_1 = F_1 N_1 = 14 * 25 = 350
    parameters: K_1 F_1 F_0 = 7 * 14 * 1 = 98
  l_2: gsconv_2
    input dimension : M_1 = F_1 N_1 = 14 * 25 = 350
    output dimension: M_2 = F_2 N_2 = 28 * 25 = 700
    parameters: K_2 F_2 F_1 = 14 * 28 * 14 = 5488
  l_3: softmax
    input dimension : M_2 = 700
    output dimension: M_3 = 2
    parameters: M_3 M_2 = 2 * 700 = 1400
  Total parameters = 6986
 
[np_3] step 500 / 5199 (epoch 9.62 / 100):
  learning_rate = 9.91e-04, loss_average = 4.11e+02
  validation accuracy: 95.67 (553 / 578), f1 (weighted): 93.72, loss: 4.81e+02
  time: 905s (wall 39s)
[np_3] step 1000 / 5199 (epoch 19.23 / 100):
  learning_rate = 9.81e-04, loss_average = 1.76e+02
  validation accuracy: 95.33 (551 / 578), f1 (weighted): 95.87, loss: 1.78e+02
  time: 1810s (wall 78s)
[np_3] step 1500 / 5199 (epoch 28.85 / 100):
  learning_rate = 9.72e-04, loss_average = 2.97e+02
  validation accuracy: 95.50 (552 / 578), f1 (weighted): 93.30, loss: 9.41e+02
  time: 2715s (wall 116s)
[np_3] step 2000 / 5199 (epoch 38.47 / 100):
  learning_rate = 9.63e-04, loss_average = 1.44e+02
  validation accuracy: 96.89 (560 / 578), f1 (weighted): 96.22, loss: 1.94e+02
  time: 3616s (wall 154s)
[np_3] step 2500 / 5199 (epoch 48.09 / 100):
  learning_rate = 9.53e-04, loss_average = 2.89e+02
  validation accuracy: 93.43 (540 / 578), f1 (weighted): 94.49, loss: 2.19e+02
  time: 4506s (wall 192s)
[np_3] step 3000 / 5199 (epoch 57.70 / 100):
  learning_rate = 9.45e-04, loss_average = 2.90e+02
  validation accuracy: 96.54 (558 / 578), f1 (weighted): 95.65, loss: 2.99e+02
  time: 5408s (wall 230s)
[np_3] step 3500 / 5199 (epoch 67.32 / 100):
  learning_rate = 9.35e-04, loss_average = 6.08e+01
  validation accuracy: 93.60 (541 / 578), f1 (weighted): 94.70, loss: 1.51e+02
  time: 6312s (wall 268s)
[np_3] step 4000 / 5199 (epoch 76.94 / 100):
  learning_rate = 9.27e-04, loss_average = 7.98e+01
  validation accuracy: 89.10 (515 / 578), f1 (weighted): 91.69, loss: 2.71e+02
  time: 7217s (wall 306s)
[np_3] step 4500 / 5199 (epoch 86.56 / 100):
  learning_rate = 9.18e-04, loss_average = 3.17e+01
  validation accuracy: 96.37 (557 / 578), f1 (weighted): 95.35, loss: 1.47e+02
  time: 8110s (wall 344s)
[np_3] step 5000 / 5199 (epoch 96.17 / 100):
  learning_rate = 9.08e-04, loss_average = 4.90e+01
  validation accuracy: 91.18 (527 / 578), f1 (weighted): 92.91, loss: 1.24e+02
  time: 9010s (wall 382s)
[np_3] step 5199 / 5199 (epoch 100.00 / 100):
  learning_rate = 9.06e-04, loss_average = 4.81e+01
  validation accuracy: 93.43 (540 / 578), f1 (weighted): 94.54, loss: 6.93e+01
  time: 9388s (wall 398s)
validation accuracy: peak = 96.89, mean = 94.13
train accuracy: 95.35 (4957 / 5199), f1 (weighted): 96.12, loss: 4.93e+01
time: 34s (wall 2s)
test  accuracy: 93.43 (540 / 578), f1 (weighted): 94.54, loss: 6.93e+01
time: 7s (wall 1s)
 
Training model: selection_pooling
 
  architecture/L = 2
  architecture/N = [25, 25, 15]
CNNGS Architecture: selection_pooling (selection)
  input: M_0 = N = 25
  l_1: gsconv_1
    input dimension : M_0 = F_0 N_0 =  1 * 25 = 25
    output dimension: M_1 = F_1 N_1 = 14 * 25 = 350
    parameters: K_1 F_1 F_0 = 7 * 14 * 1 = 98
  l_2: gsconv_2
    input dimension : M_1 = F_1 N_1 = 14 * 25 = 350
    output dimension: M_2 = F_2 N_2 = 28 * 15 = 420
    parameters: K_2 F_2 F_1 = 14 * 28 * 14 = 5488
  l_3: softmax
    input dimension : M_2 = 420
    output dimension: M_3 = 2
    parameters: M_3 M_2 = 2 * 420 = 840
  Total parameters = 6426
 
[selection_pooling] step 500 / 5199 (epoch 9.62 / 100):
  learning_rate = 9.91e-04, loss_average = 1.88e+01
  validation accuracy: 95.33 (551 / 578), f1 (weighted): 93.22, loss: 1.88e+01
  time: 1032s (wall 45s)
[selection_pooling] step 1000 / 5199 (epoch 19.23 / 100):
  learning_rate = 9.81e-04, loss_average = 1.84e+01
  validation accuracy: 95.50 (552 / 578), f1 (weighted): 93.30, loss: 3.05e+01
  time: 2038s (wall 87s)
[selection_pooling] step 1500 / 5199 (epoch 28.85 / 100):
  learning_rate = 9.72e-04, loss_average = 4.83e+00
  validation accuracy: 93.94 (543 / 578), f1 (weighted): 93.77, loss: 2.30e+00
  time: 3084s (wall 132s)
[selection_pooling] step 2000 / 5199 (epoch 38.47 / 100):
  learning_rate = 9.63e-04, loss_average = 2.39e+00
  validation accuracy: 95.67 (553 / 578), f1 (weighted): 93.72, loss: 3.55e+00
  time: 4117s (wall 175s)
[selection_pooling] step 2500 / 5199 (epoch 48.09 / 100):
  learning_rate = 9.53e-04, loss_average = 4.16e+00
  validation accuracy: 95.50 (552 / 578), f1 (weighted): 93.30, loss: 1.13e+01
  time: 5160s (wall 219s)
[selection_pooling] step 3000 / 5199 (epoch 57.70 / 100):
  learning_rate = 9.45e-04, loss_average = 1.52e+00
  validation accuracy: 84.60 (489 / 578), f1 (weighted): 88.62, loss: 3.06e+00
  time: 6192s (wall 263s)
[selection_pooling] step 3500 / 5199 (epoch 67.32 / 100):
  learning_rate = 9.35e-04, loss_average = 9.62e-01
  validation accuracy: 95.50 (552 / 578), f1 (weighted): 94.14, loss: 9.38e-01
  time: 7245s (wall 307s)
[selection_pooling] step 4000 / 5199 (epoch 76.94 / 100):
  learning_rate = 9.27e-04, loss_average = 5.90e-01
  validation accuracy: 95.85 (554 / 578), f1 (weighted): 94.11, loss: 1.02e+00
  time: 8272s (wall 351s)
[selection_pooling] step 4500 / 5199 (epoch 86.56 / 100):
  learning_rate = 9.18e-04, loss_average = 1.58e+00
  validation accuracy: 91.87 (531 / 578), f1 (weighted): 92.63, loss: 1.28e+00
  time: 9314s (wall 395s)
[selection_pooling] step 5000 / 5199 (epoch 96.17 / 100):
  learning_rate = 9.08e-04, loss_average = 7.94e-01
  validation accuracy: 95.85 (554 / 578), f1 (weighted): 94.78, loss: 7.08e-01
  time: 10353s (wall 439s)
[selection_pooling] step 5199 / 5199 (epoch 100.00 / 100):
  learning_rate = 9.06e-04, loss_average = 5.42e-01
  validation accuracy: 92.73 (536 / 578), f1 (weighted): 93.54, loss: 5.26e-01
  time: 10761s (wall 456s)
validation accuracy: peak = 95.85, mean = 93.70
train accuracy: 93.54 (4863 / 5199), f1 (weighted): 94.43, loss: 3.08e-01
time: 35s (wall 2s)
test  accuracy: 92.73 (536 / 578), f1 (weighted): 93.54, loss: 5.26e-01
time: 7s (wall 1s)
 
Training model: aggregation_pooling
 
  architecture/L = 2
  architecture/N = [25, 12, 6]
CNNGS Architecture: aggregation_pooling (aggregation)
  input: M_0 = N = 25
  l_1: gsconv_1
    input dimension : M_0 = F_0 N_0 =  1 * 25 = 25
    output dimension: M_1 = F_1 N_1 = 14 * 12 = 168
    parameters: K_1 F_1 F_0 = 7 * 14 * 1 = 98
  l_2: gsconv_2
    input dimension : M_1 = F_1 N_1 = 14 * 12 = 168
    output dimension: M_2 = F_2 N_2 = 28 *  6 = 168
    parameters: K_2 F_2 F_1 = 14 * 28 * 14 = 5488
  l_3: softmax
    input dimension : M_2 = 168
    output dimension: M_3 = 2
    parameters: M_3 M_2 = 2 * 168 = 336
  Total parameters = 5922
 
[aggregation_pooling] step 500 / 5199 (epoch 9.62 / 100):
  learning_rate = 9.91e-04, loss_average = 1.51e-01
  validation accuracy: 95.50 (552 / 578), f1 (weighted): 93.30, loss: 1.91e-01
  time: 459s (wall 20s)
[aggregation_pooling] step 1000 / 5199 (epoch 19.23 / 100):
  learning_rate = 9.81e-04, loss_average = 1.47e-01
  validation accuracy: 95.50 (552 / 578), f1 (weighted): 93.30, loss: 2.17e-01
  time: 910s (wall 39s)
[aggregation_pooling] step 1500 / 5199 (epoch 28.85 / 100):
  learning_rate = 9.72e-04, loss_average = 1.63e-01
  validation accuracy: 93.08 (538 / 578), f1 (weighted): 92.95, loss: 2.31e-01
  time: 1352s (wall 58s)
[aggregation_pooling] step 2000 / 5199 (epoch 38.47 / 100):
  learning_rate = 9.63e-04, loss_average = 1.02e-01
  validation accuracy: 95.50 (552 / 578), f1 (weighted): 93.62, loss: 1.72e-01
  time: 1799s (wall 77s)
[aggregation_pooling] step 2500 / 5199 (epoch 48.09 / 100):
  learning_rate = 9.53e-04, loss_average = 1.27e-01
  validation accuracy: 94.29 (545 / 578), f1 (weighted): 93.55, loss: 1.86e-01
  time: 2259s (wall 96s)
[aggregation_pooling] step 3000 / 5199 (epoch 57.70 / 100):
  learning_rate = 9.45e-04, loss_average = 1.32e-01
  validation accuracy: 95.33 (551 / 578), f1 (weighted): 93.22, loss: 1.82e-01
  time: 2688s (wall 114s)
[aggregation_pooling] step 3500 / 5199 (epoch 67.32 / 100):
  learning_rate = 9.35e-04, loss_average = 1.10e-01
  validation accuracy: 95.85 (554 / 578), f1 (weighted): 94.95, loss: 1.53e-01
  time: 3093s (wall 132s)
[aggregation_pooling] step 4000 / 5199 (epoch 76.94 / 100):
  learning_rate = 9.27e-04, loss_average = 1.05e-01
  validation accuracy: 95.67 (553 / 578), f1 (weighted): 93.72, loss: 1.90e-01
  time: 3526s (wall 150s)
[aggregation_pooling] step 4500 / 5199 (epoch 86.56 / 100):
  learning_rate = 9.18e-04, loss_average = 9.89e-02
  validation accuracy: 95.50 (552 / 578), f1 (weighted): 93.30, loss: 1.79e-01
  time: 3960s (wall 168s)
[aggregation_pooling] step 5000 / 5199 (epoch 96.17 / 100):
  learning_rate = 9.08e-04, loss_average = 9.56e-02
  validation accuracy: 95.67 (553 / 578), f1 (weighted): 94.66, loss: 1.62e-01
  time: 4413s (wall 187s)
[aggregation_pooling] step 5199 / 5199 (epoch 100.00 / 100):
  learning_rate = 9.06e-04, loss_average = 9.58e-02
  validation accuracy: 95.16 (550 / 578), f1 (weighted): 94.60, loss: 1.46e-01
  time: 4577s (wall 195s)
validation accuracy: peak = 95.85, mean = 95.16
train accuracy: 97.33 (5060 / 5199), f1 (weighted): 96.93, loss: 8.16e-02
time: 13s (wall 1s)
test  accuracy: 95.16 (550 / 578), f1 (weighted): 94.60, loss: 1.46e-01
time: 6s (wall 0s)
 
Training model: hybrid_pooling
 
  architecture/L = 2
  architecture/N = [25, 25, 10]
CNNGS Architecture: hybrid_pooling (hybrid)
  input: M_0 = N = 25
  l_1: gsconv_1
    input dimension : M_0 = F_0 N_0 =  1 * 25 = 25
    output dimension: M_1 = F_1 N_1 = 16 * 25 = 400
    parameters_1 detail:
      parameters_(1,1): K_(1,1) F_(1,1) F_(1,0) = 7 * 8 * 1 = 56
      parameters_(1,2): K_(1,2) F_(1,2) F_(1,1) = 14 * 16 * 8 = 1792
    parameters = parameters_1 N_1 = 1848 * 25 = 46200
  l_2: gsconv_2
    input dimension : M_1 = F_1 N_1 = 16 * 25 = 400
    output dimension: M_2 = F_2 N_2 = 16 * 10 = 160
    parameters_2 detail:
      parameters_(2,1): K_(2,1) F_(2,1) F_(2,0) = 7 * 8 * 16 = 896
      parameters_(2,2): K_(2,2) F_(2,2) F_(2,1) = 14 * 16 * 8 = 1792
    parameters = parameters_2 N_2 = 2688 * 10 = 26880
  l_3: softmax
    input dimension : M_2 = 160
    output dimension: M_3 = 2
    parameters: M_3 M_2 = 2 * 160 = 320
  Total parameters = 73400
 
[hybrid_pooling] step 500 / 5199 (epoch 9.62 / 100):
  learning_rate = 9.91e-04, loss_average = 1.72e-01
  validation accuracy: 94.29 (545 / 578), f1 (weighted): 94.61, loss: 1.54e-01
  time: 3640s (wall 165s)
[hybrid_pooling] step 1000 / 5199 (epoch 19.23 / 100):
  learning_rate = 9.81e-04, loss_average = 1.08e-01
  validation accuracy: 96.02 (555 / 578), f1 (weighted): 94.48, loss: 1.89e-01
  time: 7327s (wall 325s)
[hybrid_pooling] step 1500 / 5199 (epoch 28.85 / 100):
  learning_rate = 9.72e-04, loss_average = 7.72e-02
  validation accuracy: 96.37 (557 / 578), f1 (weighted): 96.10, loss: 1.23e-01
  time: 10848s (wall 477s)
[hybrid_pooling] step 2000 / 5199 (epoch 38.47 / 100):
  learning_rate = 9.63e-04, loss_average = 5.15e-02
  validation accuracy: 96.02 (555 / 578), f1 (weighted): 95.08, loss: 1.50e-01
  time: 14450s (wall 632s)
[hybrid_pooling] step 2500 / 5199 (epoch 48.09 / 100):
  learning_rate = 9.53e-04, loss_average = 8.51e-02
  validation accuracy: 95.67 (553 / 578), f1 (weighted): 94.00, loss: 2.29e-01
  time: 18218s (wall 794s)
[hybrid_pooling] step 3000 / 5199 (epoch 57.70 / 100):
  learning_rate = 9.45e-04, loss_average = 5.44e-02
  validation accuracy: 95.50 (552 / 578), f1 (weighted): 93.90, loss: 2.07e-01
  time: 22273s (wall 968s)
[hybrid_pooling] step 3500 / 5199 (epoch 67.32 / 100):
  learning_rate = 9.35e-04, loss_average = 4.22e-02
  validation accuracy: 95.67 (553 / 578), f1 (weighted): 95.45, loss: 1.89e-01
  time: 26061s (wall 1130s)
[hybrid_pooling] step 4000 / 5199 (epoch 76.94 / 100):
  learning_rate = 9.27e-04, loss_average = 5.29e-02
  validation accuracy: 96.02 (555 / 578), f1 (weighted): 95.38, loss: 2.02e-01
  time: 29802s (wall 1291s)
[hybrid_pooling] step 4500 / 5199 (epoch 86.56 / 100):
  learning_rate = 9.18e-04, loss_average = 3.64e-02
  validation accuracy: 96.02 (555 / 578), f1 (weighted): 96.12, loss: 1.89e-01
  time: 33511s (wall 1450s)
[hybrid_pooling] step 5000 / 5199 (epoch 96.17 / 100):
  learning_rate = 9.08e-04, loss_average = 3.74e-02
  validation accuracy: 95.33 (551 / 578), f1 (weighted): 95.29, loss: 2.23e-01
  time: 37269s (wall 1612s)
[hybrid_pooling] step 5199 / 5199 (epoch 100.00 / 100):
  learning_rate = 9.06e-04, loss_average = 2.51e-02
  validation accuracy: 96.54 (558 / 578), f1 (weighted): 96.04, loss: 2.26e-01
  time: 38724s (wall 1677s)
validation accuracy: peak = 96.54, mean = 95.92
train accuracy: 99.33 (5164 / 5199), f1 (weighted): 99.30, loss: 1.47e-02
time: 185s (wall 11s)
test  accuracy: 96.54 (558 / 578), f1 (weighted): 96.04, loss: 2.26e-01
time: 46s (wall 5s)
 
Showing results...
 
    {n = 25, norm-Laplacian, num_epochs = 100, batch_size = 100, 
     reg = 0, dropout = 0.5, momentum = 0
     ADAM, learning_rate = 0.001}
 
    aggregation_pooling = {F = [14, 28], K = [7, 14], M = [2]}
    c_cheb_a = {F = [14, 28], K = [7, 14], M = [2]}
    hybrid_pooling = {F = [[8, 16], [8, 16]], K = [[7, 14], [7, 14]], M = [2]}
    np_3 = {F = [14, 28], K = [7, 14], M = [2]}
    selection_pooling = {F = [14, 28], K = [7, 14], M = [2]}
 
    Results:
      accuracy        F1        parameters    time [ms]  name
    test  train   test  train   
    95.16 97.33   94.60 96.93      5922         38       aggregation_pooling
    95.16 95.36   94.85 95.47      6034         72       c_cheb_a
    96.54 99.33   96.04 99.30     73400        324       hybrid_pooling
    93.43 95.35   94.54 96.12      6986         77       np_3
    92.73 93.54   93.54 94.43      6426         88       selection_pooling
 
 
Clustering graph sizes:
S_c[0]: 32
S_c[1]: 16
S_c[2]: 8
grid search: 1 combinations to evaluate


  {}  


  architecture/L = 2
  architecture/N = [25, 25, 10]
CNNGS Architecture: hybrid_pooling (hybrid)
  input: M_0 = N = 25
  l_1: gsconv_1
    input dimension : M_0 = F_0 N_0 =  1 * 25 = 25
    output dimension: M_1 = F_1 N_1 = 16 * 25 = 400
    parameters_1 detail:
      parameters_(1,1): K_(1,1) F_(1,1) F_(1,0) = 7 * 8 * 1 = 56
      parameters_(1,2): K_(1,2) F_(1,2) F_(1,1) = 14 * 16 * 8 = 1792
    parameters = parameters_1 N_1 = 1848 * 25 = 46200
  l_2: gsconv_2
    input dimension : M_1 = F_1 N_1 = 16 * 25 = 400
    output dimension: M_2 = F_2 N_2 = 16 * 10 = 160
    parameters_2 detail:
      parameters_(2,1): K_(2,1) F_(2,1) F_(2,0) = 7 * 8 * 16 = 896
      parameters_(2,2): K_(2,2) F_(2,2) F_(2,1) = 14 * 16 * 8 = 1792
    parameters = parameters_2 N_2 = 2688 * 10 = 26880
  l_3: softmax
    input dimension : M_2 = 160
    output dimension: M_3 = 2
    parameters: M_3 M_2 = 2 * 160 = 320
  Total parameters = 73400
 
[hybrid_pooling] step 500 / 5199 (epoch 9.62 / 100):
  learning_rate = 9.91e-04, loss_average = 8.67e-02
  validation accuracy: 95.85 (554 / 578), f1 (weighted): 94.11, loss: 1.39e-01
  time: 3782s (wall 171s)
[hybrid_pooling] step 1000 / 5199 (epoch 19.23 / 100):
  learning_rate = 9.81e-04, loss_average = 9.85e-02
  validation accuracy: 95.16 (550 / 578), f1 (weighted): 94.85, loss: 1.25e-01
  time: 7542s (wall 334s)
[hybrid_pooling] step 1500 / 5199 (epoch 28.85 / 100):
  learning_rate = 9.72e-04, loss_average = 6.43e-02
  validation accuracy: 96.71 (559 / 578), f1 (weighted): 95.94, loss: 1.17e-01
  time: 11307s (wall 496s)
[hybrid_pooling] step 2000 / 5199 (epoch 38.47 / 100):
  learning_rate = 9.63e-04, loss_average = 5.96e-02
  validation accuracy: 95.50 (552 / 578), f1 (weighted): 93.30, loss: 2.50e-01
  time: 14892s (wall 650s)
[hybrid_pooling] step 2500 / 5199 (epoch 48.09 / 100):
  learning_rate = 9.53e-04, loss_average = 3.12e-02
  validation accuracy: 97.06 (561 / 578), f1 (weighted): 96.48, loss: 1.17e-01
  time: 18662s (wall 812s)
[hybrid_pooling] step 3000 / 5199 (epoch 57.70 / 100):
  learning_rate = 9.45e-04, loss_average = 2.92e-02
  validation accuracy: 96.19 (556 / 578), f1 (weighted): 96.04, loss: 1.34e-01
  time: 22491s (wall 976s)
[hybrid_pooling] step 3500 / 5199 (epoch 67.32 / 100):
  learning_rate = 9.35e-04, loss_average = 3.04e-02
  validation accuracy: 97.06 (561 / 578), f1 (weighted): 96.48, loss: 1.78e-01
  time: 26259s (wall 1138s)
[hybrid_pooling] step 4000 / 5199 (epoch 76.94 / 100):
  learning_rate = 9.27e-04, loss_average = 3.45e-02
  validation accuracy: 96.54 (558 / 578), f1 (weighted): 95.65, loss: 2.06e-01
  time: 29959s (wall 1297s)
[hybrid_pooling] step 4500 / 5199 (epoch 86.56 / 100):
  learning_rate = 9.18e-04, loss_average = 2.41e-02
  validation accuracy: 97.06 (561 / 578), f1 (weighted): 96.97, loss: 1.20e-01
  time: 33684s (wall 1457s)
[hybrid_pooling] step 5000 / 5199 (epoch 96.17 / 100):
  learning_rate = 9.08e-04, loss_average = 1.15e-02
  validation accuracy: 97.23 (562 / 578), f1 (weighted): 97.23, loss: 1.08e-01
  time: 37511s (wall 1622s)
[hybrid_pooling] step 5199 / 5199 (epoch 100.00 / 100):
  learning_rate = 9.06e-04, loss_average = 2.79e-02
  validation accuracy: 96.54 (558 / 578), f1 (weighted): 96.04, loss: 1.56e-01
  time: 38978s (wall 1688s)
validation accuracy: peak = 97.23, mean = 96.51
train accuracy: 99.17 (5156 / 5199), f1 (weighted): 99.12, loss: 1.84e-02
time: 210s (wall 11s)
test  accuracy: 96.54 (558 / 578), f1 (weighted): 96.04, loss: 1.56e-01
time: 31s (wall 4s)



Train accuracy:      99.17
Test accuracy:       96.54
Train F1 (weighted): 99.12
Test F1 (weighted):  96.04
{} --> 99.17 96.54 99.12 96.04
