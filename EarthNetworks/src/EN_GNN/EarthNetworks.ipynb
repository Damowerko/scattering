{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up problem parameters... DONE\n",
      "Gathering data... Numbmer of datapoints: 5777\n",
      "Building graph support... DONE\n",
      "Running Neural Networks: BEGINNING\n",
      " \n",
      "Training model: c_cheb_a\n",
      " \n",
      "  architecture/L = 2\n",
      "  architecture/N = [32, 16, 8]\n",
      "CNNGS Architecture: c_cheb_a (clustering)\n",
      "  input: M_0 = N = 32\n",
      "  l_1: gsconv_1\n",
      "    input dimension : M_0 = F_0 N_0 =  1 * 32 = 32\n",
      "    output dimension: M_1 = F_1 N_1 = 14 * 16 = 224\n",
      "    parameters: K_1 F_1 F_0 = 7 * 14 * 1 = 98\n",
      "  l_2: gsconv_2\n",
      "    input dimension : M_1 = F_1 N_1 = 14 * 16 = 224\n",
      "    output dimension: M_2 = F_2 N_2 = 28 *  8 = 224\n",
      "    parameters: K_2 F_2 F_1 = 14 * 28 * 14 = 5488\n",
      "  l_3: softmax\n",
      "    input dimension : M_2 = 224\n",
      "    output dimension: M_3 = 2\n",
      "    parameters: M_3 M_2 = 2 * 224 = 448\n",
      "  Total parameters = 6034\n",
      " \n",
      "[c_cheb_a] step 1000 / 10398 (epoch 19.23 / 200):\n",
      "  learning_rate = 9.81e-04, loss_average = 3.01e+03\n",
      "  validation accuracy: 95.33 (551 / 578), f1 (binary): 0.00, loss: 1.30e+04\n",
      "  time: 165s (wall 30s)\n",
      "[c_cheb_a] step 2000 / 10398 (epoch 38.47 / 200):\n",
      "  learning_rate = 9.63e-04, loss_average = 6.04e+02\n",
      "  validation accuracy: 95.67 (553 / 578), f1 (binary): 56.14, loss: 1.05e+04\n",
      "  time: 330s (wall 60s)\n",
      "[c_cheb_a] step 3000 / 10398 (epoch 57.70 / 200):\n",
      "  learning_rate = 9.45e-04, loss_average = 7.84e+02\n",
      "  validation accuracy: 91.18 (527 / 578), f1 (binary): 46.32, loss: 8.29e+03\n",
      "  time: 494s (wall 89s)\n",
      "[c_cheb_a] step 4000 / 10398 (epoch 76.94 / 200):\n",
      "  learning_rate = 9.27e-04, loss_average = 3.31e+02\n",
      "  validation accuracy: 96.37 (557 / 578), f1 (binary): 57.14, loss: 6.27e+03\n",
      "  time: 659s (wall 119s)\n",
      "[c_cheb_a] step 5000 / 10398 (epoch 96.17 / 200):\n",
      "  learning_rate = 9.08e-04, loss_average = 1.55e+02\n",
      "  validation accuracy: 96.54 (558 / 578), f1 (binary): 58.33, loss: 7.45e+03\n",
      "  time: 824s (wall 149s)\n",
      "[c_cheb_a] step 6000 / 10398 (epoch 115.41 / 200):\n",
      "  learning_rate = 8.91e-04, loss_average = 5.41e+02\n",
      "  validation accuracy: 95.50 (552 / 578), f1 (binary): 0.00, loss: 8.68e+03\n",
      "  time: 988s (wall 178s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glob/intel-python/versions/2018u2/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_cheb_a] step 7000 / 10398 (epoch 134.64 / 200):\n",
      "  learning_rate = 8.75e-04, loss_average = 6.45e+01\n",
      "  validation accuracy: 95.67 (553 / 578), f1 (binary): 7.41, loss: 6.47e+03\n",
      "  time: 1152s (wall 208s)\n",
      "[c_cheb_a] step 8000 / 10398 (epoch 153.88 / 200):\n",
      "  learning_rate = 8.58e-04, loss_average = 9.76e+01\n",
      "  validation accuracy: 95.16 (550 / 578), f1 (binary): 0.00, loss: 6.07e+03\n",
      "  time: 1316s (wall 237s)\n",
      "[c_cheb_a] step 9000 / 10398 (epoch 173.11 / 200):\n",
      "  learning_rate = 8.41e-04, loss_average = 8.77e+01\n",
      "  validation accuracy: 94.12 (544 / 578), f1 (binary): 41.38, loss: 4.03e+03\n",
      "  time: 1481s (wall 267s)\n",
      "[c_cheb_a] step 10000 / 10398 (epoch 192.34 / 200):\n",
      "  learning_rate = 8.25e-04, loss_average = 5.52e+01\n",
      "  validation accuracy: 95.67 (553 / 578), f1 (binary): 28.57, loss: 2.23e+03\n",
      "  time: 1645s (wall 297s)\n",
      "[c_cheb_a] step 10398 / 10398 (epoch 200.00 / 200):\n",
      "  learning_rate = 8.19e-04, loss_average = 5.58e+01\n",
      "  validation accuracy: 95.16 (550 / 578), f1 (binary): 51.72, loss: 2.04e+03\n",
      "  time: 1711s (wall 309s)\n",
      "validation accuracy: peak = 96.54, mean = 95.10\n",
      "INFO:tensorflow:Restoring parameters from /home/u8634/EarthNetworks/src/cnngs/../checkpoints/1_c_cheb_a/model-10398\n",
      "train accuracy: 95.98 (4990 / 5199), f1 (binary): 55.25, loss: 2.76e+01\n",
      "time: 5s (wall 1s)\n",
      "INFO:tensorflow:Restoring parameters from /home/u8634/EarthNetworks/src/cnngs/../checkpoints/1_c_cheb_a/model-10398\n",
      "test  accuracy: 95.16 (550 / 578), f1 (binary): 51.72, loss: 2.04e+03\n",
      "time: 1s (wall 0s)\n",
      " \n",
      "Training model: np_3\n",
      " \n",
      "  architecture/L = 2\n",
      "  architecture/N = [25, 25, 25]\n",
      "CNNGS Architecture: np_3 (no-pooling)\n",
      "  input: M_0 = N = 25\n",
      "  l_1: gsconv_1\n",
      "    input dimension : M_0 = F_0 N_0 =  1 * 25 = 25\n",
      "    output dimension: M_1 = F_1 N_1 = 14 * 25 = 350\n",
      "    parameters: K_1 F_1 F_0 = 7 * 14 * 1 = 98\n",
      "  l_2: gsconv_2\n",
      "    input dimension : M_1 = F_1 N_1 = 14 * 25 = 350\n",
      "    output dimension: M_2 = F_2 N_2 = 28 * 25 = 700\n",
      "    parameters: K_2 F_2 F_1 = 14 * 28 * 14 = 5488\n",
      "  l_3: softmax\n",
      "    input dimension : M_2 = 700\n",
      "    output dimension: M_3 = 2\n",
      "    parameters: M_3 M_2 = 2 * 700 = 1400\n",
      "  Total parameters = 6986\n",
      " \n",
      "[np_3] step 1000 / 10398 (epoch 19.23 / 200):\n",
      "  learning_rate = 9.81e-04, loss_average = 2.44e+02\n",
      "  validation accuracy: 95.33 (551 / 578), f1 (binary): 0.00, loss: 4.63e+02\n",
      "  time: 92s (wall 12s)\n",
      "[np_3] step 2000 / 10398 (epoch 38.47 / 200):\n",
      "  learning_rate = 9.63e-04, loss_average = 8.47e+01\n",
      "  validation accuracy: 95.67 (553 / 578), f1 (binary): 19.35, loss: 1.67e+02\n",
      "  time: 184s (wall 24s)\n",
      "[np_3] step 3000 / 10398 (epoch 57.70 / 200):\n",
      "  learning_rate = 9.45e-04, loss_average = 1.12e+02\n",
      "  validation accuracy: 85.29 (493 / 578), f1 (binary): 36.09, loss: 4.25e+02\n",
      "  time: 276s (wall 35s)\n",
      "[np_3] step 4000 / 10398 (epoch 76.94 / 200):\n",
      "  learning_rate = 9.27e-04, loss_average = 2.95e+01\n",
      "  validation accuracy: 95.85 (554 / 578), f1 (binary): 61.29, loss: 1.03e+02\n",
      "  time: 367s (wall 47s)\n",
      "[np_3] step 5000 / 10398 (epoch 96.17 / 200):\n",
      "  learning_rate = 9.08e-04, loss_average = 3.86e+01\n",
      "  validation accuracy: 96.89 (560 / 578), f1 (binary): 59.09, loss: 1.02e+02\n",
      "  time: 459s (wall 58s)\n",
      "[np_3] step 6000 / 10398 (epoch 115.41 / 200):\n",
      "  learning_rate = 8.91e-04, loss_average = 2.80e+01\n",
      "  validation accuracy: 96.71 (559 / 578), f1 (binary): 45.71, loss: 6.37e+01\n",
      "  time: 551s (wall 70s)\n",
      "[np_3] step 7000 / 10398 (epoch 134.64 / 200):\n",
      "  learning_rate = 8.75e-04, loss_average = 1.52e+01\n",
      "  validation accuracy: 96.89 (560 / 578), f1 (binary): 47.06, loss: 4.42e+01\n",
      "  time: 642s (wall 81s)\n",
      "[np_3] step 8000 / 10398 (epoch 153.88 / 200):\n",
      "  learning_rate = 8.58e-04, loss_average = 1.27e+01\n",
      "  validation accuracy: 90.83 (525 / 578), f1 (binary): 44.21, loss: 4.27e+01\n",
      "  time: 734s (wall 93s)\n",
      "[np_3] step 9000 / 10398 (epoch 173.11 / 200):\n",
      "  learning_rate = 8.41e-04, loss_average = 2.22e+00\n",
      "  validation accuracy: 96.19 (556 / 578), f1 (binary): 62.07, loss: 1.99e+01\n",
      "  time: 825s (wall 104s)\n",
      "[np_3] step 10000 / 10398 (epoch 192.34 / 200):\n",
      "  learning_rate = 8.25e-04, loss_average = 7.82e+00\n",
      "  validation accuracy: 96.71 (559 / 578), f1 (binary): 53.66, loss: 1.89e+01\n",
      "  time: 917s (wall 116s)\n",
      "[np_3] step 10398 / 10398 (epoch 200.00 / 200):\n",
      "  learning_rate = 8.19e-04, loss_average = 1.52e+00\n",
      "  validation accuracy: 97.58 (564 / 578), f1 (binary): 69.57, loss: 1.11e+01\n",
      "  time: 954s (wall 121s)\n",
      "validation accuracy: peak = 97.58, mean = 94.86\n",
      "INFO:tensorflow:Restoring parameters from /home/u8634/EarthNetworks/src/cnngs/../checkpoints/1_np_3/model-10398\n",
      "train accuracy: 98.13 (5102 / 5199), f1 (binary): 72.98, loss: 8.50e-01\n",
      "time: 3s (wall 0s)\n",
      "INFO:tensorflow:Restoring parameters from /home/u8634/EarthNetworks/src/cnngs/../checkpoints/1_np_3/model-10398\n",
      "test  accuracy: 97.58 (564 / 578), f1 (binary): 69.57, loss: 1.11e+01\n",
      "time: 1s (wall 0s)\n",
      " \n",
      "Training model: selection_pooling\n",
      " \n",
      "  architecture/L = 2\n",
      "  architecture/N = [25, 25, 15]\n",
      "CNNGS Architecture: selection_pooling (selection)\n",
      "  input: M_0 = N = 25\n",
      "  l_1: gsconv_1\n",
      "    input dimension : M_0 = F_0 N_0 =  1 * 25 = 25\n",
      "    output dimension: M_1 = F_1 N_1 = 16 * 25 = 400\n",
      "    parameters: K_1 F_1 F_0 = 16 * 16 * 1 = 256\n",
      "  l_2: gsconv_2\n",
      "    input dimension : M_1 = F_1 N_1 = 16 * 25 = 400\n",
      "    output dimension: M_2 = F_2 N_2 = 16 * 15 = 240\n",
      "    parameters: K_2 F_2 F_1 = 16 * 16 * 16 = 4096\n",
      "  l_3: softmax\n",
      "    input dimension : M_2 = 240\n",
      "    output dimension: M_3 = 2\n",
      "    parameters: M_3 M_2 = 2 * 240 = 480\n",
      "  Total parameters = 4832\n",
      " \n",
      "[selection_pooling] step 1000 / 10398 (epoch 19.23 / 200):\n",
      "  learning_rate = 9.81e-04, loss_average = 7.34e+02\n",
      "  validation accuracy: 95.50 (552 / 578), f1 (binary): 0.00, loss: 1.90e+03\n",
      "  time: 167s (wall 21s)\n",
      "[selection_pooling] step 2000 / 10398 (epoch 38.47 / 200):\n",
      "  learning_rate = 9.63e-04, loss_average = 3.75e+02\n",
      "  validation accuracy: 89.27 (516 / 578), f1 (binary): 22.50, loss: 5.23e+02\n",
      "  time: 334s (wall 43s)\n",
      "[selection_pooling] step 3000 / 10398 (epoch 57.70 / 200):\n",
      "  learning_rate = 9.45e-04, loss_average = 1.22e+02\n",
      "  validation accuracy: 95.50 (552 / 578), f1 (binary): 0.00, loss: 1.67e+02\n",
      "  time: 502s (wall 64s)\n",
      "[selection_pooling] step 4000 / 10398 (epoch 76.94 / 200):\n",
      "  learning_rate = 9.27e-04, loss_average = 2.13e+01\n",
      "  validation accuracy: 95.50 (552 / 578), f1 (binary): 0.00, loss: 8.81e+01\n",
      "  time: 669s (wall 85s)\n",
      "[selection_pooling] step 5000 / 10398 (epoch 96.17 / 200):\n",
      "  learning_rate = 9.08e-04, loss_average = 1.92e+01\n",
      "  validation accuracy: 95.16 (550 / 578), f1 (binary): 0.00, loss: 2.24e+01\n",
      "  time: 834s (wall 106s)\n",
      "[selection_pooling] step 6000 / 10398 (epoch 115.41 / 200):\n",
      "  learning_rate = 8.91e-04, loss_average = 1.95e+01\n",
      "  validation accuracy: 95.50 (552 / 578), f1 (binary): 0.00, loss: 4.80e+01\n",
      "  time: 999s (wall 127s)\n",
      "[selection_pooling] step 7000 / 10398 (epoch 134.64 / 200):\n",
      "  learning_rate = 8.75e-04, loss_average = 2.76e+01\n",
      "  validation accuracy: 95.50 (552 / 578), f1 (binary): 27.78, loss: 7.82e+00\n",
      "  time: 1164s (wall 148s)\n",
      "[selection_pooling] step 8000 / 10398 (epoch 153.88 / 200):\n",
      "  learning_rate = 8.58e-04, loss_average = 5.97e+00\n",
      "  validation accuracy: 89.79 (519 / 578), f1 (binary): 21.33, loss: 5.54e+00\n",
      "  time: 1330s (wall 169s)\n",
      "[selection_pooling] step 9000 / 10398 (epoch 173.11 / 200):\n",
      "  learning_rate = 8.41e-04, loss_average = 4.04e+00\n",
      "  validation accuracy: 95.16 (550 / 578), f1 (binary): 6.67, loss: 2.82e+01\n",
      "  time: 1495s (wall 190s)\n",
      "[selection_pooling] step 10000 / 10398 (epoch 192.34 / 200):\n",
      "  learning_rate = 8.25e-04, loss_average = 1.43e+00\n",
      "  validation accuracy: 90.83 (525 / 578), f1 (binary): 15.87, loss: 2.92e+01\n",
      "  time: 1661s (wall 211s)\n",
      "[selection_pooling] step 10398 / 10398 (epoch 200.00 / 200):\n",
      "  learning_rate = 8.19e-04, loss_average = 1.06e+00\n",
      "  validation accuracy: 95.33 (551 / 578), f1 (binary): 0.00, loss: 2.51e+01\n",
      "  time: 1727s (wall 219s)\n",
      "validation accuracy: peak = 95.50, mean = 93.75\n",
      "INFO:tensorflow:Restoring parameters from /home/u8634/EarthNetworks/src/cnngs/../checkpoints/1_selection_pooling/model-10398\n",
      "train accuracy: 96.23 (5003 / 5199), f1 (binary): 0.00, loss: 1.55e+00\n",
      "time: 6s (wall 1s)\n",
      "INFO:tensorflow:Restoring parameters from /home/u8634/EarthNetworks/src/cnngs/../checkpoints/1_selection_pooling/model-10398\n",
      "test  accuracy: 95.33 (551 / 578), f1 (binary): 0.00, loss: 2.51e+01\n",
      "time: 1s (wall 0s)\n",
      " \n",
      "Training model: aggregation_pooling\n",
      " \n",
      "  architecture/L = 2\n",
      "  architecture/N = [25, 12, 6]\n",
      "CNNGS Architecture: aggregation_pooling (aggregation)\n",
      "  input: M_0 = N = 25\n",
      "  l_1: gsconv_1\n",
      "    input dimension : M_0 = F_0 N_0 =  1 * 25 = 25\n",
      "    output dimension: M_1 = F_1 N_1 = 16 * 12 = 192\n",
      "    parameters: K_1 F_1 F_0 = 16 * 16 * 1 = 256\n",
      "  l_2: gsconv_2\n",
      "    input dimension : M_1 = F_1 N_1 = 16 * 12 = 192\n",
      "    output dimension: M_2 = F_2 N_2 = 16 *  6 = 96\n",
      "    parameters: K_2 F_2 F_1 = 16 * 16 * 16 = 4096\n",
      "  l_3: softmax\n",
      "    input dimension : M_2 = 96\n",
      "    output dimension: M_3 = 2\n",
      "    parameters: M_3 M_2 = 2 * 96 = 192\n",
      "  Total parameters = 4544\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[aggregation_pooling] step 1000 / 10398 (epoch 19.23 / 200):\n",
      "  learning_rate = 9.81e-04, loss_average = 1.61e-01\n",
      "  validation accuracy: 95.50 (552 / 578), f1 (binary): 0.00, loss: 4.17e-01\n",
      "  time: 72s (wall 15s)\n",
      "[aggregation_pooling] step 2000 / 10398 (epoch 38.47 / 200):\n",
      "  learning_rate = 9.63e-04, loss_average = 1.16e-01\n",
      "  validation accuracy: 94.12 (544 / 578), f1 (binary): 37.04, loss: 4.02e-01\n",
      "  time: 143s (wall 29s)\n",
      "[aggregation_pooling] step 3000 / 10398 (epoch 57.70 / 200):\n",
      "  learning_rate = 9.45e-04, loss_average = 2.39e-01\n",
      "  validation accuracy: 95.50 (552 / 578), f1 (binary): 0.00, loss: 8.18e-01\n",
      "  time: 213s (wall 43s)\n",
      "[aggregation_pooling] step 4000 / 10398 (epoch 76.94 / 200):\n",
      "  learning_rate = 9.27e-04, loss_average = 1.24e-01\n",
      "  validation accuracy: 95.33 (551 / 578), f1 (binary): 30.77, loss: 3.36e-01\n",
      "  time: 284s (wall 58s)\n",
      "[aggregation_pooling] step 5000 / 10398 (epoch 96.17 / 200):\n",
      "  learning_rate = 9.08e-04, loss_average = 1.36e-01\n",
      "  validation accuracy: 95.85 (554 / 578), f1 (binary): 20.00, loss: 2.64e-01\n",
      "  time: 355s (wall 72s)\n",
      "[aggregation_pooling] step 6000 / 10398 (epoch 115.41 / 200):\n",
      "  learning_rate = 8.91e-04, loss_average = 1.23e-01\n",
      "  validation accuracy: 95.50 (552 / 578), f1 (binary): 7.14, loss: 2.50e-01\n",
      "  time: 426s (wall 86s)\n",
      "[aggregation_pooling] step 7000 / 10398 (epoch 134.64 / 200):\n",
      "  learning_rate = 8.75e-04, loss_average = 9.19e-02\n",
      "  validation accuracy: 94.81 (548 / 578), f1 (binary): 16.67, loss: 2.63e-01\n",
      "  time: 496s (wall 101s)\n",
      "[aggregation_pooling] step 8000 / 10398 (epoch 153.88 / 200):\n",
      "  learning_rate = 8.58e-04, loss_average = 9.18e-02\n",
      "  validation accuracy: 95.50 (552 / 578), f1 (binary): 18.75, loss: 1.97e-01\n",
      "  time: 567s (wall 115s)\n",
      "[aggregation_pooling] step 9000 / 10398 (epoch 173.11 / 200):\n",
      "  learning_rate = 8.41e-04, loss_average = 8.43e-02\n",
      "  validation accuracy: 95.85 (554 / 578), f1 (binary): 20.00, loss: 2.76e-01\n",
      "  time: 638s (wall 129s)\n",
      "[aggregation_pooling] step 10000 / 10398 (epoch 192.34 / 200):\n",
      "  learning_rate = 8.25e-04, loss_average = 6.27e-02\n",
      "  validation accuracy: 95.67 (553 / 578), f1 (binary): 13.79, loss: 3.92e-01\n",
      "  time: 708s (wall 144s)\n",
      "[aggregation_pooling] step 10398 / 10398 (epoch 200.00 / 200):\n",
      "  learning_rate = 8.19e-04, loss_average = 5.62e-02\n",
      "  validation accuracy: 95.85 (554 / 578), f1 (binary): 25.00, loss: 3.75e-01\n",
      "  time: 737s (wall 150s)\n",
      "validation accuracy: peak = 95.85, mean = 95.40\n",
      "INFO:tensorflow:Restoring parameters from /home/u8634/EarthNetworks/src/cnngs/../checkpoints/1_aggregation_pooling/model-10398\n",
      "train accuracy: 97.73 (5081 / 5199), f1 (binary): 59.03, loss: 6.25e-02\n",
      "time: 3s (wall 1s)\n",
      "INFO:tensorflow:Restoring parameters from /home/u8634/EarthNetworks/src/cnngs/../checkpoints/1_aggregation_pooling/model-10398\n",
      "test  accuracy: 95.85 (554 / 578), f1 (binary): 25.00, loss: 3.75e-01\n",
      "time: 0s (wall 0s)\n",
      " \n",
      "Training model: hybrid_pooling\n",
      " \n",
      "  architecture/L = 2\n",
      "  architecture/N = [25, 25, 10]\n",
      "CNNGS Architecture: hybrid_pooling (hybrid)\n",
      "  input: M_0 = N = 25\n",
      "  l_1: gsconv_1\n",
      "    input dimension : M_0 = F_0 N_0 =  1 * 25 = 25\n",
      "    output dimension: M_1 = F_1 N_1 = 16 * 25 = 400\n",
      "    parameters_1 detail:\n",
      "      parameters_(1,1): K_(1,1) F_(1,1) F_(1,0) = 8 * 8 * 1 = 64\n",
      "      parameters_(1,2): K_(1,2) F_(1,2) F_(1,1) = 8 * 16 * 8 = 1024\n",
      "    parameters = parameters_1 N_1 = 1088 * 25 = 27200\n",
      "  l_2: gsconv_2\n",
      "    input dimension : M_1 = F_1 N_1 = 16 * 25 = 400\n",
      "    output dimension: M_2 = F_2 N_2 = 16 * 10 = 160\n",
      "    parameters_2 detail:\n",
      "      parameters_(2,1): K_(2,1) F_(2,1) F_(2,0) = 8 * 8 * 16 = 1024\n",
      "      parameters_(2,2): K_(2,2) F_(2,2) F_(2,1) = 8 * 16 * 8 = 1024\n",
      "    parameters = parameters_2 N_2 = 2048 * 10 = 20480\n",
      "  l_3: softmax\n",
      "    input dimension : M_2 = 160\n",
      "    output dimension: M_3 = 2\n",
      "    parameters: M_3 M_2 = 2 * 160 = 320\n",
      "  Total parameters = 48000\n",
      " \n",
      "[hybrid_pooling] step 1000 / 10398 (epoch 19.23 / 200):\n",
      "  learning_rate = 9.81e-04, loss_average = 6.73e-02\n",
      "  validation accuracy: 95.67 (553 / 578), f1 (binary): 13.79, loss: 8.40e-01\n",
      "  time: 197s (wall 27s)\n",
      "[hybrid_pooling] step 2000 / 10398 (epoch 38.47 / 200):\n",
      "  learning_rate = 9.63e-04, loss_average = 5.66e-02\n",
      "  validation accuracy: 95.85 (554 / 578), f1 (binary): 29.41, loss: 1.97e+00\n",
      "  time: 394s (wall 54s)\n",
      "[hybrid_pooling] step 3000 / 10398 (epoch 57.70 / 200):\n",
      "  learning_rate = 9.45e-04, loss_average = 8.10e-02\n",
      "  validation accuracy: 95.50 (552 / 578), f1 (binary): 43.48, loss: 7.77e+00\n",
      "  time: 588s (wall 79s)\n",
      "[hybrid_pooling] step 4000 / 10398 (epoch 76.94 / 200):\n",
      "  learning_rate = 9.27e-04, loss_average = 4.57e-02\n",
      "  validation accuracy: 95.67 (553 / 578), f1 (binary): 44.44, loss: 8.69e+00\n",
      "  time: 783s (wall 104s)\n",
      "[hybrid_pooling] step 5000 / 10398 (epoch 96.17 / 200):\n",
      "  learning_rate = 9.08e-04, loss_average = 3.07e-02\n",
      "  validation accuracy: 95.67 (553 / 578), f1 (binary): 41.86, loss: 7.98e+00\n",
      "  time: 978s (wall 129s)\n",
      "[hybrid_pooling] step 6000 / 10398 (epoch 115.41 / 200):\n",
      "  learning_rate = 8.91e-04, loss_average = 3.53e-02\n",
      "  validation accuracy: 96.19 (556 / 578), f1 (binary): 42.11, loss: 1.24e+01\n",
      "  time: 1173s (wall 154s)\n",
      "[hybrid_pooling] step 7000 / 10398 (epoch 134.64 / 200):\n",
      "  learning_rate = 8.75e-04, loss_average = 1.95e-02\n",
      "  validation accuracy: 95.50 (552 / 578), f1 (binary): 35.00, loss: 1.11e+01\n",
      "  time: 1368s (wall 179s)\n",
      "[hybrid_pooling] step 8000 / 10398 (epoch 153.88 / 200):\n",
      "  learning_rate = 8.58e-04, loss_average = 1.20e-02\n",
      "  validation accuracy: 96.54 (558 / 578), f1 (binary): 44.44, loss: 8.92e+00\n",
      "  time: 1563s (wall 203s)\n",
      "[hybrid_pooling] step 9000 / 10398 (epoch 173.11 / 200):\n",
      "  learning_rate = 8.41e-04, loss_average = 8.59e-03\n",
      "  validation accuracy: 96.19 (556 / 578), f1 (binary): 52.17, loss: 1.30e+01\n",
      "  time: 1758s (wall 229s)\n",
      "[hybrid_pooling] step 10000 / 10398 (epoch 192.34 / 200):\n",
      "  learning_rate = 8.25e-04, loss_average = 6.64e-03\n",
      "  validation accuracy: 95.85 (554 / 578), f1 (binary): 36.84, loss: 1.27e+01\n",
      "  time: 1954s (wall 254s)\n",
      "[hybrid_pooling] step 10398 / 10398 (epoch 200.00 / 200):\n",
      "  learning_rate = 8.19e-04, loss_average = 5.64e-03\n",
      "  validation accuracy: 95.85 (554 / 578), f1 (binary): 36.84, loss: 1.24e+01\n",
      "  time: 2033s (wall 264s)\n",
      "validation accuracy: peak = 96.54, mean = 95.88\n",
      "INFO:tensorflow:Restoring parameters from /home/u8634/EarthNetworks/src/cnngs/../checkpoints/1_hybrid_pooling/model-10398\n",
      "train accuracy: 99.77 (5187 / 5199), f1 (binary): 96.79, loss: 6.60e-03\n",
      "time: 6s (wall 1s)\n",
      "INFO:tensorflow:Restoring parameters from /home/u8634/EarthNetworks/src/cnngs/../checkpoints/1_hybrid_pooling/model-10398\n",
      "test  accuracy: 95.85 (554 / 578), f1 (binary): 36.84, loss: 1.24e+01\n",
      "time: 1s (wall 1s)\n",
      " \n",
      "Showing results...\n",
      " \n",
      "    {n = 25, norm-Laplacian, num_epochs = 200, batch_size = 100, \n",
      "     reg = 0, dropout = 0, momentum = 0\n",
      "     ADAM, learning_rate = 0.001}\n",
      " \n",
      "Region: NYC\n",
      "    aggregation_pooling = {F = [16, 16], K = [16, 16], M = [2]}\n",
      "    c_cheb_a = {F = [14, 28], K = [7, 14], M = [2]}\n",
      "    hybrid_pooling = {F = [[8, 16], [8, 16]], K = [[8, 8], [8, 8]], M = [2]}\n",
      "    np_3 = {F = [14, 28], K = [7, 14], M = [2]}\n",
      "    selection_pooling = {F = [16, 16], K = [16, 16], M = [2]}\n",
      " \n",
      "    Results:\n",
      "      accuracy        F1        parameters    time [ms]  name\n",
      "    test  train   test  train   \n",
      "    95.85 97.73   25.00 59.03      4544         14       aggregation_pooling\n",
      "    95.16 95.98   51.72 55.25      6034         30       c_cheb_a\n",
      "    95.85 99.77   36.84 96.79     48000         25       hybrid_pooling\n",
      "    97.58 98.13   69.57 72.98      6986         12       np_3\n",
      "    95.33 96.23    0.00  0.00      4832         21       selection_pooling\n",
      " \n",
      " \n",
      "Clustering graph sizes:\n",
      "S_c[0]: 32\n",
      "S_c[1]: 16\n",
      "S_c[2]: 8\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pdb\n",
    "from IPython.core.debugger import set_trace\n",
    "from importlib import reload\n",
    "from cnngs import architecture, graphtools, manager\n",
    "import tensorflow as tf\n",
    "import scipy.sparse\n",
    "import numpy as np\n",
    "import time\n",
    "from EN_GNN.graph import get_distance_graph\n",
    "from EN_GNN.data import pick_greedy, import_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"\"\"\n",
    "Output\n",
    "\"\"\"\n",
    "log = False\n",
    "this_filename = \"../../Results/gnn_out\"\n",
    "\n",
    "\"\"\"\n",
    "SIMULATION SELECTION\n",
    "\"\"\"\n",
    "\n",
    "# Graph:\n",
    "\n",
    "GSO = 'norm-Laplacian'  # 'Adjacency', 'max2-Laplacian', 'norm-Laplacian'\n",
    "# Pre-training graph operations:\n",
    "do_clustering = True  # Obtain clustering GSOs\n",
    "do_degree = True  # Obtain nodes selected based on degree\n",
    "overlap_K = 0  # Number of shifts on where not to consider overlap\n",
    "# (If K=0 then it is simply degree based ordering)\n",
    "\n",
    "# Training:\n",
    "\n",
    "train_method = 'ADAM'  # 'SGD' or 'ADAM'\n",
    "do_validation = False\n",
    "\n",
    "# Presentation:\n",
    "\n",
    "print_data_summary = False\n",
    "\n",
    "# Methods:\n",
    "\n",
    "do_c_a = True  # Clustering with our code\n",
    "do_np = True  # No pooling\n",
    "do_sp = True  # Selection Pooling\n",
    "do_ap = True  # Aggregation Pooling\n",
    "do_hp = True  # Hybrid Pooling (multinode)\n",
    "do_nn = False  # Neural networks\n",
    "\n",
    "# Fields\n",
    "fields = [\"PressureSeaLevelMBar\"\n",
    "          ,\"TemperatureC\"\n",
    "          ,\"WindSpeedKph\"\n",
    "          ,\"PressureSeaLevelMBarRatePerHour\"\n",
    "          ,\"Humidity\"\n",
    "          ,\"HumidityRatePerHour\"\n",
    "          ,\"RainMillimetersRatePerHour\"\n",
    "]\n",
    "region = \"NYC\"\n",
    "\n",
    "scale = False # normalize the data such the for each field the maximum is 1\n",
    "\n",
    "#fields = \"DewPointC,DewPointCRatePerHour,Humidity,HumidityRatePerHour,Light,LightRatePerHour,PressureSeaLevelMBar,PressureSeaLevelMBarRatePerHour,RainMillimetersDaily,RainMillimetersRatePerHour,RainMillimetersMonthly,RainMillimetersYearly,TemperatureC,TemperatureCRatePerHour,FeelsLike,WindSpeedKph,WindDirectionDegrees,WindSpeedKphAvg,WindDirectionDegreesAvg,WindGustKphHourly,WindGustTimeUtcHourly,WindGustDirectionDegreesHourly,WindGustKphDaily,WindGustTimeUtcDaily,WindGustDirectionDegreesDaily,HumidityHigh,HumidityHighUtc,HumidityLow,HumidityLowUtc,LightHigh,LightHighUtc,LightLow,LightLowUtc,PressureSeaLevelHighMBar,PressureSeaLevelHighUtc,PressureSeaLevelLowMBar,PressureSeaLevelLowUtc,RainRateMaxMmPerHour,RainRateMaxUtc,TemperatureHighC,TemperatureHighUtc,TemperatureLowC,TemperatureLowUtc\".split(',')\n",
    "#fields = ['PressureSeaLevelMBar', \"TemperatureC\", \"WindSpeedKph\", \"PressureSeaLevelMBarRatePerHour\", \"Humidity\", \"HumidityRatePerHour\", \"RainMillimetersRatePerHour\", \"TemperatureCRatePerHour\"]\n",
    "#fields = ['PressureSeaLevelMBar', \"TemperatureC\", \"WindSpeedKph\", \"PressureSeaLevelMBarRatePerHour\"]\n",
    "\n",
    "\"\"\"\n",
    "PARAMETERS SELECTION\n",
    "\"\"\"\n",
    "\n",
    "print(\"Setting up problem parameters...\", end=\" \", flush=True)\n",
    "\n",
    "# Graph:\n",
    "N = 25  # Number of nodes\n",
    "if do_clustering:\n",
    "    coarsening_levels = 2  # Same as number of layers\n",
    "\n",
    "# Training:\n",
    "N_batch = 32  # Number of samples in the batch\n",
    "\n",
    "# Training split size:\n",
    "test_split = 0.1\n",
    "\n",
    "# Specific to the dataset:\n",
    "try: tf_flags_defined\n",
    "except NameError: tf_flags_defined = False\n",
    "if not tf_flags_defined:\n",
    "    flags = tf.app.flags\n",
    "    FLAGS = flags.FLAGS\n",
    "    tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "    flags.DEFINE_integer(\n",
    "        'number_edges', 16,\n",
    "        'Graph: minimum number of edges per vertex.')\n",
    "    flags.DEFINE_string(\n",
    "        'metric', 'cosine',\n",
    "        'Graph: similarity measure (between features).')\n",
    "    flags.DEFINE_bool(\n",
    "        'normalized_laplacian', True,\n",
    "        'Graph Laplacian: normalized.')\n",
    "    flags.DEFINE_integer(\n",
    "        'coarsening_levels', coarsening_levels,\n",
    "        'Number of coarsened graphs.')\n",
    "    flags.DEFINE_string(\n",
    "        'dir_data', os.path.join('..', 'data', '20news'),\n",
    "        'Directory to store data.')\n",
    "    flags.DEFINE_integer(\n",
    "        'val_size', 400,\n",
    "    'Size of the validation set.')\n",
    "    tf_flags_defined = True\n",
    "\n",
    "l_short_docs = 5  # 5 in the original 20news file, no mention in the paper.\n",
    "\n",
    "common = {}\n",
    "common['num_epochs'] = 500  # 20 in the paper (for ADAM), 80 in the source\n",
    "# file (for SGD)\n",
    "common['learning_rate'] = 0.001  # This was set in the paper. It was 0.1 in\n",
    "# the cgconv_softmax in the source file. The other methods will remain the\n",
    "# same as they are.\n",
    "common['decay_rate'] = 0.999  # Nothing says anywhere about this in the\n",
    "# paper. I believe this is for the SGD. I left it at 0.999 which is the one\n",
    "# that was set for the cgconv. The other ones that were also 0.999 have\n",
    "# been removed from params. The ones that were different were kept.\n",
    "common['momentum'] = 0  # Determines whether training is done following\n",
    "# train_method or with momentum training.\n",
    "\n",
    "common['regularization'] = 0  # Only for the graph CNN that has no FC layer\n",
    "# the rest were originally set to zero. The paper says there is\n",
    "# regularization and this is the value that was found in the source file.\n",
    "common['dropout'] = 0  # Nothing says about dropout in the paper. Only\n",
    "# in the MNIST. In the source file they were all set to 1 (no dropout).\n",
    "common['batch_size'] = N_batch\n",
    "common['eval_frequency'] = common['num_epochs']\n",
    "\n",
    "common['dir_name'] = \"1\"  # TODO\n",
    "\n",
    "common['GSO'] = GSO\n",
    "common['train_method'] = train_method\n",
    "\n",
    "print(\"DONE\")\n",
    "\n",
    "\"\"\"\n",
    "GRAPH CREATION & DATA HANDLING\n",
    "\"\"\"\n",
    "\n",
    "print(\"Gathering data...\", end=\" \", flush=True)\n",
    "\n",
    "stations = pick_greedy(region, n_stations=N)\n",
    "data, labels = import_data(fields, stations, region)  # type: (np.ndarray, np.ndarray)\n",
    "data = data.astype(dtype=np.float32)\n",
    "labels = labels.astype(dtype=np.int32).flatten()\n",
    "\n",
    "if scale:\n",
    "    minimum = data.min(0).min(0)\n",
    "    minimum = np.tile(minimum, (data.shape[0], N,1))\n",
    "    maximum = data.max(0).max(0)\n",
    "    maximum = np.tile(maximum, (data.shape[0], N,1))\n",
    "    data = (data - minimum) / maximum\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=test_split)\n",
    "\n",
    "print(\"Numbmer of datapoints: {}\".format(data.shape[0]))\n",
    "\n",
    "if print_data_summary:\n",
    "    print(\" \")\n",
    "    print(\"DATA Summary:\")\n",
    "    print(\"    train_data:\")\n",
    "    print(\"        type = {}\".format(type(train_data)))\n",
    "    print(\"        dtype = {}\".format(train_data.dtype))\n",
    "    print(\"        shape = {}\".format(train_data.shape))\n",
    "    print(\"        average = {:.4}\".format(train_data.mean()))\n",
    "    print(\"        min = {:.4}\".format(train_data.min()))\n",
    "    print(\"        max = {:.4}\".format(train_data.max()))\n",
    "    print(\"    train_labels:\")\n",
    "    print(\"        type = {}\".format(type(train_labels)))\n",
    "    print(\"        dtype = {}\".format(train_labels.dtype))\n",
    "    print(\"        shape = {}\".format(train_labels.shape))\n",
    "    print(\"        average = {:.4}\".format(train_labels.mean()))\n",
    "    print(\"        min = {}\".format(train_labels.min()))\n",
    "    print(\"        max = {}\".format(train_labels.max()))\n",
    "    print(\"    test_data:\")\n",
    "    print(\"        type = {}\".format(type(test_data)))\n",
    "    print(\"        dtype = {}\".format(test_data.dtype))\n",
    "    print(\"        shape = {}\".format(test_data.shape))\n",
    "    print(\"        average = {:.4}\".format(test_data.mean()))\n",
    "    print(\"        min = {:.4}\".format(test_data.min()))\n",
    "    print(\"        max = {:.4}\".format(test_data.max()))\n",
    "    print(\"    test_labels:\")\n",
    "    print(\"        type = {}\".format(type(test_labels)))\n",
    "    print(\"        dtype = {}\".format(test_labels.dtype))\n",
    "    print(\"        shape = {}\".format(test_labels.shape))\n",
    "    print(\"        average = {:.4}\".format(test_labels.mean()))\n",
    "    print(\"        min = {}\".format(test_labels.min()))\n",
    "    print(\"        max = {}\".format(test_labels.max()))\n",
    "    print(\" \")\n",
    "\n",
    "\"\"\"\n",
    "FEATURE GRAPH\n",
    "\"\"\"\n",
    "\n",
    "print(\"Building graph support...\", end=\" \", flush=True)\n",
    "A = get_distance_graph(region)\n",
    "A = A[stations.flatten(), :][: , stations.flatten()]  # type: np.ndarray\n",
    "A = A.astype(dtype=np.float32)\n",
    "A = scipy.sparse.csr_matrix(A)\n",
    "\n",
    "t_start = time.process_time()\n",
    "if GSO == 'Adjacency':\n",
    "    S = A\n",
    "else:\n",
    "    S = graphtools.laplacian(A, normalized=True)\n",
    "    if GSO == 'max2-Laplacian':\n",
    "        S = graphtools.rescale_L(S, lmax=2)\n",
    "\n",
    "if do_clustering:\n",
    "    graphs_c, perm_c = graphtools.coarsen(\n",
    "        A, levels=FLAGS.coarsening_levels, self_connections=False)\n",
    "    if GSO == 'Adjacency':\n",
    "        S_c = graphs_c\n",
    "    else:\n",
    "        L_c = [graphtools.laplacian(A, normalized=True) for A in graphs_c]\n",
    "        if GSO == 'norm-Laplacian':\n",
    "            S_c = L_c\n",
    "        if GSO == 'max2-Laplacian':\n",
    "            S_c = [graphtools.rescale_L(L, lmax=2) for L in L_c]\n",
    "\n",
    "if do_degree:\n",
    "    # REMEMBER: Input the true adjacency matrix!!\n",
    "    perm_d = graphtools.degree_order(A, overlap_K)\n",
    "    A_d = A[perm_d][:, perm_d]\n",
    "    if GSO == 'Adjacency':\n",
    "        S_d = A_d\n",
    "    else:\n",
    "        S_d = graphtools.laplacian(A_d, normalized=True)\n",
    "        if GSO == 'max2-Laplacian':\n",
    "            S_d = graphtools.rescale_L(S_d, lmax=2)\n",
    "\n",
    "if do_clustering:\n",
    "    L = train_data.shape[2]\n",
    "    train_data_c = graphtools.perm_data(train_data, perm_c)\n",
    "    test_data_c = graphtools.perm_data(test_data, perm_c)\n",
    "\n",
    "if do_degree:\n",
    "    train_data_d = train_data[:][:, perm_d, :]\n",
    "    test_data_d = test_data[:][:, perm_d, :]\n",
    "\n",
    "# Validation set.\n",
    "if do_validation:\n",
    "    # Not implemented yet:\n",
    "    val_data = train_data[:FLAGS.val_size, :, :]\n",
    "    val_labels = train_labels[:FLAGS.val_size, :]\n",
    "    train_data = train_data[FLAGS.val_size:, :, :]\n",
    "    train_labels = train_labels[FLAGS.val_size:, :]\n",
    "else:\n",
    "    val_data = test_data\n",
    "    val_labels = test_labels\n",
    "    if do_clustering:\n",
    "        val_data_c = test_data_c\n",
    "    if do_degree:\n",
    "        val_data_d = test_data_d\n",
    "\n",
    "print(\"DONE\")\n",
    "\n",
    "print(\"Running Neural Networks: BEGINNING\")\n",
    "\n",
    "common['decay_steps'] = len(train_labels) / N_batch\n",
    "common['decay_steps'] = len(train_labels) / N_batch\n",
    "C = max(train_labels) + 1  # number of classes\n",
    "\n",
    "if do_c_a or do_np or do_sp or do_ap or do_hp or do_nn:\n",
    "    model_manager = manager.model_manager()\n",
    "\n",
    "if do_c_a:\n",
    "    name = 'c_cheb_a'\n",
    "    print(\" \")\n",
    "    print(\"Training model: {}\".format(name))\n",
    "    print(\" \")\n",
    "    params = common.copy()\n",
    "    params['S'] = S_c\n",
    "    params['V'] = len(fields)\n",
    "\n",
    "    params['archit'] = 'clustering'\n",
    "    params['filter'] = 'chebyshev5'\n",
    "    params['pool'] = 'maxpool'\n",
    "    params['nonlin'] = 'b1relu'\n",
    "\n",
    "    params['K'] = [7, 14]\n",
    "    params['F'] = [14, 28]\n",
    "    params['a'] = [2, 2]\n",
    "    params['M'] = [C]\n",
    "\n",
    "    params['name'] = name\n",
    "    params['dir_name'] += '_' + name + '/'\n",
    "    \n",
    "    model_manager.test(architecture.cnngs(**params), name, params,\n",
    "                       train_data_c, train_labels, val_data_c, val_labels,\n",
    "                       test_data_c, test_labels)\n",
    "\n",
    "if do_np:\n",
    "    name = 'np_3'\n",
    "    print(\" \")\n",
    "    print(\"Training model: {}\".format(name))\n",
    "    print(\" \")\n",
    "    params = common.copy()\n",
    "    params['S'] = S\n",
    "    params['V'] = len(fields)\n",
    "\n",
    "    params['archit'] = 'no-pooling'\n",
    "    params['filter'] = 'lsigf'\n",
    "    params['pool'] = 'maxpool'\n",
    "    params['nonlin'] = 'b1relu'\n",
    "\n",
    "    params['K'] = [7, 14]\n",
    "    params['F'] = [14, 28]\n",
    "    params['a'] = [2, 2]\n",
    "    params['M'] = [C]\n",
    "    params['name'] = name\n",
    "    params['dir_name'] += '_' + name + '/'\n",
    "\n",
    "    model_manager.test(architecture.cnngs(**params), name, params,\n",
    "                       train_data, train_labels, val_data, val_labels,\n",
    "                       test_data, test_labels)\n",
    "\n",
    "if do_sp:\n",
    "    name = 'selection_pooling'\n",
    "    print(\" \")\n",
    "    print(\"Training model: {}\".format(name))\n",
    "    print(\" \")\n",
    "    params = common.copy()\n",
    "    params['S'] = [S_d, [25, 15]]  # the number of nodes at each layer, if fewer layers than elements in list, final parameter specifies downsampling at the end\n",
    "    params['V'] = len(fields)\n",
    "\n",
    "    params['archit'] = 'selection'\n",
    "    params['filter'] = 'lsigf'\n",
    "    params['pool'] = 'maxpool'\n",
    "    params['nonlin'] = 'b1relu'\n",
    "\n",
    "    params['K'] = [16, 16]  # number of filter taps\n",
    "    params['F'] = [16, 16]  # number of features\n",
    "    params['a'] = [2, 2]  # size of the pooling, n-hop pooling\n",
    "    params['M'] = [C]\n",
    "    params['name'] = name\n",
    "    params['dir_name'] += '_' + name + '/'\n",
    "\n",
    "    model_manager.test(architecture.cnngs(**params), name, params,\n",
    "                       train_data_d, train_labels, val_data_d, val_labels,\n",
    "                       test_data_d, test_labels)\n",
    "\n",
    "if do_ap:\n",
    "    name = 'aggregation_pooling'\n",
    "    print(\" \")\n",
    "    print(\"Training model: {}\".format(name))\n",
    "    print(\" \")\n",
    "    params = common.copy()\n",
    "    params['S'] = [S_d, [1]]  # the node to use for aggregation (nth highest degree)\n",
    "    params['V'] = len(fields)\n",
    "\n",
    "    params['archit'] = 'aggregation'\n",
    "    params['filter'] = 'lsigf'\n",
    "    params['pool'] = 'maxpool'\n",
    "    params['nonlin'] = 'b1relu'\n",
    "\n",
    "    params['K'] = [16, 16]\n",
    "    params['F'] = [16, 16]\n",
    "    params['a'] = [2, 2]\n",
    "    params['M'] = [C]\n",
    "\n",
    "    params['name'] = name\n",
    "    params['dir_name'] += '_' + name + '/'\n",
    "\n",
    "    model_manager.test(architecture.cnngs(**params), name, params,\n",
    "                       train_data_d, train_labels, val_data_d, val_labels,\n",
    "                       test_data_d, test_labels)\n",
    "\n",
    "if do_hp:\n",
    "    name = 'hybrid_pooling'\n",
    "    print(\" \")\n",
    "    print(\"Training model: {}\".format(name))\n",
    "    print(\" \")\n",
    "    params = common.copy()\n",
    "    params['S'] = [S_d, [25, 10], [15, 10]]  # subset of nodes to look at , number of exchanges you do\n",
    "    params['V'] = len(fields)\n",
    "\n",
    "    params['archit'] = 'hybrid'\n",
    "    params['filter'] = 'lsigf'\n",
    "    params['pool'] = 'maxpool'\n",
    "    params['nonlin'] = 'b1relu'\n",
    "\n",
    "    params['K'] = [[8, 8], [8, 8]]\n",
    "    params['F'] = [[8, 16], [8, 16]]\n",
    "    params['a'] = [[2, 2], [2, 2]]\n",
    "    params['M'] = [C]\n",
    "\n",
    "    params['name'] = name\n",
    "    params['dir_name'] += '_' + name + '/'\n",
    "\n",
    "    model_manager.test(architecture.cnngs(**params), name, params,\n",
    "                       train_data_d, train_labels, val_data_d, val_labels,\n",
    "                       test_data_d, test_labels)\n",
    "    \n",
    "#if do_nn:\n",
    "#    nn_train_data = train_data.reshape((train_data.shape[0], train_data.shape[1] * train_data.shape[2]))\n",
    "#    nn_val_data = val_data.reshape((val_data.shape[0], val_data.shape[1] * val_data.shape[2]))\n",
    "#    nn_test_data = test_data.reshape((test_data.shape[0], test_data.shape[1] * test_data.shape[2]))    \n",
    "#    name = 'fc_softmax'\n",
    "#    params = common.copy()\n",
    "#    del params['GSO']\n",
    "#    del params['train_method']\n",
    "#    params['dir_name'] += name\n",
    "#    params['regularization'] = 0\n",
    "#    params['dropout']        = 0.4\n",
    "#    params['learning_rate']  = 0.1\n",
    "#    params['decay_rate']     = 0.95\n",
    "#    params['momentum']       = 0.9\n",
    "#    params['F']              = []\n",
    "#    params['K']              = []\n",
    "#    params['p']              = []\n",
    "#    params['M']              = [2500, C]\n",
    "#    params['V']              = len(fields)\n",
    "#    model = models.cgcnn(S, **params)\n",
    "#    model.n_param = 0\n",
    "#    model_manager.test(model, name, params,\n",
    "#\t\t\tnn_train_data, train_labels, nn_val_data, val_labels, nn_test_data,\n",
    "#\t\t\ttest_labels)\n",
    "#    name = 'fc_fc_softmax'\n",
    "#    params = common.copy()\n",
    "#    del params['GSO']\n",
    "#    del params['train_method']\n",
    "#    params['dir_name'] += name\n",
    "#    params['regularization'] = 0\n",
    "#    params['dropout']        = 0.4\n",
    "#    params['learning_rate']  = 0.1\n",
    "#    params['decay_rate']     = 0.95\n",
    "#    params['momentum']       = 0.9\n",
    "#    params['F']              = []\n",
    "#    params['K']              = []\n",
    "#    params['p']              = []\n",
    "#    params['M']              = [2500, 500, C]\n",
    "#    params['V']              = len(fields)\n",
    "#    model = models.cgcnn(S, **params)\n",
    "#    model.n_param = 0\n",
    "#    model_manager.test(model, name, params,\n",
    "#\t\t\tnn_train_data, train_labels, nn_val_data, val_labels, nn_test_data,\n",
    "#\t\t\ttest_labels)\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"Showing results...\")\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "print(\"    {{n = {}, {}, num_epochs = {}, batch_size = {}, \".\n",
    "      format(train_data.shape[1], common['GSO'],\n",
    "             common['num_epochs'], common['batch_size']))\n",
    "print(\"     reg = {}, dropout = {}, momentum = {}\".\n",
    "      format(common['regularization'], common['dropout'],\n",
    "             common['momentum']))\n",
    "if train_method == \"SGD\":\n",
    "    print(\"     {}, learning_rate = {}, decay_rate = {}}}\".\n",
    "          format(common['train_method'], common['learning_rate'],\n",
    "                 common['decay_rate']))\n",
    "elif train_method == \"ADAM\":\n",
    "    print(\"     {}, learning_rate = {}}}\".\n",
    "          format(common['train_method'], common['learning_rate']))\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "if do_c_a or do_np or do_sp or do_ap or do_hp:\n",
    "    print(\"Region: \" + region)\n",
    "    model_manager.show()\n",
    "    print(\" \")\n",
    "\n",
    "print(\" \")\n",
    "print(\"Clustering graph sizes:\")\n",
    "for i in range(len(S_c)):\n",
    "    print('S_c[{}]: {}'.format(i, S_c[i].shape[0]))\n",
    "\n",
    "if log:\n",
    "    ff = open(this_filename + '-DONE', 'a');\n",
    "    ff.write('DONE.')\n",
    "    ff.write('\\r\\n')\n",
    "    ff.close()\n",
    "\n",
    "if False:\n",
    "    grid_params = {}\n",
    "    data = (train_data, train_labels, val_data, val_labels, test_data,\n",
    "            test_labels)\n",
    "    manager.grid_search(params, grid_params, *data,\n",
    "                        model=lambda x: architecture.cnngs(**x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel, 2018 update 2)",
   "language": "python",
   "name": "intel_distribution_of_python_3_2018u2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
